#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
LitSearchTestBedä¸“ç”¨æµ‹è¯•åºŠ

ç»§æ‰¿è‡ªTestBedï¼Œä¸“é—¨ç”¨äºLitSearchæ•°æ®é›†ä¸Šçš„æœç´¢ç­–ç•¥æµ‹è¯•ã€‚
ä½¿ç”¨çœŸå®æ•°æ®åº“ï¼ˆVectorDB, MetadataDBï¼‰è¿›è¡Œå®Œæ•´çš„é›†æˆæµ‹è¯•ã€‚
"""

from experiments.recommendation.testbed.base_testbed import TestBed
from experiments.recommendation.testbed.litsearch_eval.evaluator import SearchEvaluator
from AIgnite.data.docset import DocSet, TextChunk, ChunkType
from AIgnite.db.metadata_db import MetadataDB, Base
from AIgnite.db.vector_db import VectorDB
from AIgnite.index.paper_indexer import PaperIndexer
from AIgnite.index.search_strategy import VectorSearchStrategy, TFIDFSearchStrategy, HybridSearchStrategy

#from eval.litsearch_eval_new.evaluator import SearchEvaluator
from sqlalchemy import create_engine, text, inspect
from typing import Dict, Any, Tuple, List, Optional
import os
import shutil
import numpy as np
from pathlib import Path
from datasets import load_dataset
from tqdm import tqdm
import json
import logging
import sys

# æ·»åŠ è·¯å¾„ä»¥ä¾¿å¯¼å…¥è¯„ä¼°æ¨¡å—
#sys.path.append('/data3/guofang/AIgnite-Solutions/AIgnite/eval/litsearch_eval_new')

logger = logging.getLogger(__name__)

class LitSearchTestBed(TestBed):
    """LitSearchä¸“ç”¨æµ‹è¯•åºŠ
    
    æä¾›å®Œæ•´çš„LitSearchæ•°æ®é›†æœç´¢ç­–ç•¥æµ‹è¯•ï¼ŒåŒ…æ‹¬ï¼š
    - å‘é‡æœç´¢æµ‹è¯•
    - TF-IDFæœç´¢æµ‹è¯•
    - æ··åˆæœç´¢æµ‹è¯•
    - æœç´¢ç­–ç•¥æ€§èƒ½æ¯”è¾ƒ
    - è¯„ä¼°æŒ‡æ ‡è®¡ç®—
    """
    
    def __init__(self, config_path: str):
        """åˆå§‹åŒ–LitSearchæµ‹è¯•åºŠ
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        super().__init__(config_path)
        
        # åŠ è½½å®Œæ•´é…ç½®æ–‡ä»¶ä»¥è·å–LITSEARCH_TESTBEDé…ç½®
        self.full_config = self._load_full_config(config_path)
        self.litsearch_config = self.full_config.get('LITSEARCH_TESTBED', {})
        
        self.vector_db = None
        self.metadata_db = None
        self.engine = None
        self.corpus_data = None
        self.ground_truth = None
        self.search_strategies = {}
        self.evaluation_results = {}
        
        
        # éªŒè¯LitSearchç‰¹å®šé…ç½®
        self._validate_litsearch_config()
    
    def _load_full_config(self, config_path: str) -> Dict[str, Any]:
        """åŠ è½½å®Œæ•´é…ç½®æ–‡ä»¶
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
            
        Returns:
            å®Œæ•´çš„é…ç½®å­—å…¸
        """
        import yaml
        try:
            with open(config_path, 'r') as f:
                full_config = yaml.safe_load(f)
            return full_config
        except Exception as e:
            self.logger.error(f"Failed to load full config from {config_path}: {str(e)}")
            raise
    
    def _validate_litsearch_config(self) -> None:
        """éªŒè¯LitSearchç‰¹å®šé…ç½®"""
        required_sections = ['dataset', 'search_strategies', 'evaluation']
        for section in required_sections:
            if section not in self.litsearch_config:
                raise ValueError(f"Missing required section '{section}' in LITSEARCH_TESTBED configuration")
        
        # éªŒè¯æ•°æ®é›†é…ç½®
        dataset_config = self.litsearch_config.get('dataset', {})
        if 'name' not in dataset_config:
            raise ValueError("Missing 'name' in dataset configuration")
        
        # éªŒè¯æœç´¢ç­–ç•¥é…ç½®
        strategies_config = self.litsearch_config.get('search_strategies', {})
        if not any(strategies_config.get(strategy, {}).get('enabled', False) 
                   for strategy in ['vector', 'tfidf', 'hybrid']):
            raise ValueError("At least one search strategy must be enabled")
        
    
    
    def check_environment(self) -> Tuple[bool, str]:
        """æ£€æŸ¥æµ‹è¯•ç¯å¢ƒ
        
        Returns:
            Tuple[bool, str]: (æ˜¯å¦å°±ç»ª, é”™è¯¯ä¿¡æ¯)
        """
        try:
            # æ£€æŸ¥é…ç½®æ–‡ä»¶
            if not os.path.exists(self.config_path):
                return False, f"Config file not found: {self.config_path}"
            
            # æ£€æŸ¥æ•°æ®åº“è¿æ¥
            db_url = self.config['metadata_db']['db_url']
            if not db_url:
                return False, "Database URL not found in config"
            
            # å°è¯•è¿æ¥æ•°æ®åº“ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º
            try:
                engine = create_engine(db_url)
                with engine.connect() as conn:
                    conn.execute(text("SELECT 1"))
                engine.dispose()
            except Exception as db_error:
                # å¦‚æœæ•°æ®åº“ä¸å­˜åœ¨ï¼Œå°è¯•åˆ›å»º
                if "does not exist" in str(db_error):
                    self.logger.info("Database does not exist, attempting to create...")
                    try:
                        # è§£ææ•°æ®åº“URLä»¥è·å–æ•°æ®åº“åç§°
                        from urllib.parse import urlparse
                        parsed_url = urlparse(db_url)
                        db_name = parsed_url.path[1:]  # ç§»é™¤å¼€å¤´çš„ '/'
                        
                        # è¿æ¥åˆ°é»˜è®¤æ•°æ®åº“æ¥åˆ›å»ºæ–°æ•°æ®åº“
                        default_url = db_url.replace(f'/{db_name}', '/postgres')
                        # æ·»åŠ isolation_levelå‚æ•°æ¥é¿å…äº‹åŠ¡å—
                        default_engine = create_engine(default_url, isolation_level='AUTOCOMMIT')
                        
                        with default_engine.connect() as conn:
                            # æ£€æŸ¥æ•°æ®åº“æ˜¯å¦å­˜åœ¨
                            result = conn.execute(text(f"SELECT 1 FROM pg_database WHERE datname = '{db_name}'"))
                            if not result.fetchone():
                                # åˆ›å»ºæ•°æ®åº“
                                conn.execute(text(f"CREATE DATABASE {db_name}"))
                                self.logger.info(f"Created database: {db_name}")
                            else:
                                self.logger.info(f"Database {db_name} already exists")
                        
                        default_engine.dispose()
                        
                        # é‡æ–°æµ‹è¯•è¿æ¥
                        engine = create_engine(db_url)
                        with engine.connect() as conn:
                            conn.execute(text("SELECT 1"))
                        engine.dispose()
                        
                    except Exception as create_error:
                        return False, f"Failed to create database: {str(create_error)}"
                else:
                    return False, f"Database connection failed: {str(db_error)}"
            
            # æ£€æŸ¥å‘é‡æ•°æ®åº“è·¯å¾„æƒé™
            vector_db_path = self.config['vector_db']['db_path']
            if not vector_db_path:
                return False, "Vector database path not found in config"
            
            vector_db_dir = Path(vector_db_path).parent
            vector_db_dir.mkdir(parents=True, exist_ok=True)
            
            # æµ‹è¯•å†™æƒé™
            test_file = vector_db_dir / "test_write_permission"
            test_file.write_text("test")
            test_file.unlink()
            
            return True, "Environment check passed"
            
        except Exception as e:
            return False, f"Environment check failed: {str(e)}"
    
    def load_data(self) -> Tuple[List[Dict], Dict[str, List[str]]]:
        """åŠ è½½LitSearchæ•°æ®é›†
        
        Returns:
            Tuple[List[Dict], Dict[str, List[str]]]: (è¯­æ–™åº“æ•°æ®, æŸ¥è¯¢çœŸå€¼)
        """
        self.logger.info("Loading LitSearch dataset...")
        
        dataset_config = self.litsearch_config.get('dataset', {})
        dataset_name = dataset_config.get('name', 'princeton-nlp/LitSearch')
        corpus_config = dataset_config.get('corpus_config', 'corpus_clean')
        query_config = dataset_config.get('query_config', 'query')
        sample_size = dataset_config.get('sample_size')
        enable_sampling = dataset_config.get('enable_sampling', True)
        
        try:
            # åŠ è½½æŸ¥è¯¢æ•°æ®é›†
            query_dataset = load_dataset(dataset_name, query_config, split="full")
            # åŠ è½½è¯­æ–™åº“æ•°æ®é›†
            corpus_dataset = load_dataset(dataset_name, corpus_config, split="full")
            
            # å°†æ•°æ®é›†è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
            corpus = [doc for doc in corpus_dataset]
            queries = query_dataset
            
            self.logger.info(f"Successfully loaded dataset: {len(corpus)} documents, {len(queries)} queries")
            
            # æå–æŸ¥è¯¢å’ŒçœŸå€¼
            ground_truth = {}
            for query in queries:
                query_text = query["query"]
                relevant_docs = [str(doc_id) for doc_id in query["corpusids"]]
                ground_truth[query_text] = relevant_docs
            
            self.logger.info(f"Extracted {len(ground_truth)} queries with ground truth")
            
            # æ•°æ®é‡‡æ ·ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if enable_sampling and sample_size and sample_size < len(corpus):
                corpus, ground_truth = self._sample_data(corpus, ground_truth, sample_size)
                self.logger.info(f"Sampled to {len(corpus)} documents and {len(ground_truth)} queries")
            
            return corpus, ground_truth
            
        except Exception as e:
            self.logger.error(f"Failed to load dataset: {str(e)}")
            raise
    
    def _sample_data(self, corpus: List[Dict], ground_truth: Dict[str, List[str]], sample_size: int) -> Tuple[List[Dict], Dict[str, List[str]]]:
        """é‡‡æ ·æ•°æ®
        
        Args:
            corpus: è¯­æ–™åº“
            ground_truth: æŸ¥è¯¢çœŸå€¼
            sample_size: é‡‡æ ·å¤§å°
            
        Returns:
            Tuple[List[Dict], Dict[str, List[str]]]: é‡‡æ ·åçš„è¯­æ–™åº“å’ŒæŸ¥è¯¢çœŸå€¼
        """
        self.logger.info(f"Sampling {sample_size} documents and related queries...")
        
        try:
            # 1. é¦–å…ˆéšæœºé€‰æ‹©ä¸€äº›æŸ¥è¯¢
            query_texts = list(ground_truth.keys())
            query_count = min(sample_size // 10, len(query_texts))  # é€‰æ‹©æŸ¥è¯¢æ•°é‡
            query_indices = np.random.choice(len(query_texts), query_count, replace=False)
            sampled_queries = [query_texts[i] for i in query_indices]
            
            # 2. è·å–è¿™äº›æŸ¥è¯¢ç›¸å…³çš„æ‰€æœ‰æ–‡æ¡£ID
            relevant_doc_ids = set()
            for query in sampled_queries:
                relevant_doc_ids.update(ground_truth[query])
            
            self.logger.info(f"Selected {len(sampled_queries)} queries, involving {len(relevant_doc_ids)} relevant documents")
            
            # 3. å¦‚æœç›¸å…³æ–‡æ¡£æ•°é‡ä¸è¶³ï¼Œåˆ™éšæœºæ·»åŠ ä¸€äº›æ–‡æ¡£
            corpus_ids = [str(doc["corpusid"]) for doc in corpus]
            corpus_id_to_index = {doc_id: i for i, doc_id in enumerate(corpus_ids)}
            
            # æ‰¾å‡ºå·²æœ‰çš„ç›¸å…³æ–‡æ¡£åœ¨è¯­æ–™åº“ä¸­çš„ç´¢å¼•
            relevant_indices = [corpus_id_to_index[doc_id] for doc_id in relevant_doc_ids if doc_id in corpus_id_to_index]
            
            # å¦‚æœç›¸å…³æ–‡æ¡£æ•°é‡ä¸è¶³é‡‡æ ·å¤§å°ï¼Œåˆ™éšæœºæ·»åŠ ä¸€äº›æ–‡æ¡£
            additional_count = max(0, sample_size - len(relevant_indices))
            if additional_count > 0:
                # è·å–éç›¸å…³æ–‡æ¡£çš„ç´¢å¼•
                non_relevant_indices = [i for i in range(len(corpus)) if i not in relevant_indices]
                if non_relevant_indices:
                    # éšæœºé€‰æ‹©ä¸€äº›éç›¸å…³æ–‡æ¡£
                    additional_indices = np.random.choice(
                        non_relevant_indices, 
                        min(additional_count, len(non_relevant_indices)), 
                        replace=False
                    )
                    all_indices = list(relevant_indices) + list(additional_indices)
                else:
                    all_indices = relevant_indices
            else:
                # å¦‚æœç›¸å…³æ–‡æ¡£æ•°é‡å·²ç»è¶…è¿‡é‡‡æ ·å¤§å°ï¼Œåˆ™éšæœºé€‰æ‹©ä¸€éƒ¨åˆ†
                all_indices = np.random.choice(relevant_indices, sample_size, replace=False)
            
            # 4. é‡‡æ ·è¯­æ–™åº“
            corpus_sample = [corpus[i] for i in all_indices]
            
            # 5. æ›´æ–° ground_truthï¼Œåªä¿ç•™é‡‡æ ·æ–‡æ¡£ä¸­çš„ç›¸å…³æ–‡æ¡£
            sampled_corpus_ids = set(str(doc["corpusid"]) for doc in corpus_sample)
            sampled_ground_truth = {}
            for query in sampled_queries:
                relevant_docs = [doc_id for doc_id in ground_truth[query] if doc_id in sampled_corpus_ids]
                if relevant_docs:  # åªä¿ç•™æœ‰ç›¸å…³æ–‡æ¡£çš„æŸ¥è¯¢
                    sampled_ground_truth[query] = relevant_docs
            
            # å¦‚æœæ²¡æœ‰æŸ¥è¯¢æœ‰ç›¸å…³æ–‡æ¡£ï¼Œè®°å½•è­¦å‘Š
            if not sampled_ground_truth:
                self.logger.warning("After sampling, no queries have relevant documents! Using original queries and ground truth.")
                return corpus_sample, ground_truth
            
            self.logger.info(f"After sampling: {len(corpus_sample)} documents, {len(sampled_ground_truth)} queries")
            self.logger.info(f"Average relevant documents per query: {sum(len(docs) for docs in sampled_ground_truth.values()) / len(sampled_ground_truth):.2f}")
            
            return corpus_sample, sampled_ground_truth
        except Exception as e:
            self.logger.error(f"Data sampling failed: {str(e)}")
            return corpus, ground_truth
    
    def setup(self, clean_before_test: bool = True) -> None:
        """è®¾ç½®æµ‹è¯•ç¯å¢ƒ
        
        Args:
            clean_before_test: æ˜¯å¦åœ¨æµ‹è¯•å‰æ¸…ç†ç¯å¢ƒï¼Œé»˜è®¤ä¸ºTrue
        """
        self.logger.info(f"Setting up {self.__class__.__name__} test environment...")
        
        # æ£€æŸ¥ç¯å¢ƒ
        is_ready, error_msg = self.check_environment()
        if not is_ready:
            raise RuntimeError(f"Environment check failed: {error_msg}")
        
        self.logger.info("Environment check passed")
        
        # æµ‹è¯•å‰æ¸…ç†ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if clean_before_test:
            self._cleanup_before_test()
        
        # åˆ›å»ºä¸´æ—¶ç›®å½•
        import tempfile
        self.temp_dir = tempfile.mkdtemp()
        self.logger.info(f"Created temporary directory: {self.temp_dir}")
        
        # åŠ è½½æ•°æ®
        self.logger.info("Loading test data...")
        self.data = self.load_data()
        self.logger.info(f"Loaded {len(self.data) if hasattr(self.data, '__len__') else 'test'} data items")
        
        # ä»é…ç½®ä¸­è¯»å–load_previous_dbå‚æ•°
        load_previous_db = self.litsearch_config.get('load_previous_db', False)
        self.logger.info(f"Load previous database setting: {load_previous_db}")
        
        # åˆå§‹åŒ–æ•°æ®åº“
        self.logger.info("Initializing databases...")
        self.initialize_databases(self.data, load_previous_db=load_previous_db)
        self.logger.info("Database initialization completed")
    
    def initialize_databases(self, data: Tuple[List[Dict], Dict[str, List[str]]], load_previous_db: bool = False) -> None:
        """åˆå§‹åŒ–çœŸå®æ•°æ®åº“
        
        Args:
            data: (è¯­æ–™åº“æ•°æ®, æŸ¥è¯¢çœŸå€¼) å…ƒç»„
            load_previous_db: æ˜¯å¦åŠ è½½å·²å­˜åœ¨çš„æ•°æ®åº“ï¼ŒTrue=è·³è¿‡æ•°æ®ç´¢å¼•ï¼ŒFalse=é‡æ–°ç´¢å¼•æ•°æ®
        """
        self.logger.info("Initializing real databases...")
        
        corpus_data, ground_truth = data
        self.corpus_data = corpus_data
        self.ground_truth = ground_truth
        
        
        # åˆå§‹åŒ–å‘é‡æ•°æ®åº“
        vector_db_path = self.config['vector_db']['db_path']
        model_name = self.config['vector_db'].get('model_name', 'BAAI/bge-base-en-v1.5')
        print('MODEL NAME: ', model_name)
        vector_dim = self.config['vector_db'].get('vector_dim', 768)
        print('VECTOR DIM: ', vector_dim)
        self.vector_db = VectorDB(
            db_path=vector_db_path,
            model_name=model_name,
            vector_dim=vector_dim
        )
        self.logger.info(f"Vector database initialized: {vector_db_path}")
        
        # åˆå§‹åŒ–å…ƒæ•°æ®æ•°æ®åº“
        db_url = self.config['metadata_db']['db_url']
        self.engine = create_engine(db_url)
        
        '''
        # é‡æ–°åˆ›å»ºè¡¨
        Base.metadata.drop_all(self.engine)
        Base.metadata.create_all(self.engine)
        self.logger.info("Metadata database tables recreated")
        '''
        
        self.metadata_db = MetadataDB(db_path=db_url)
        self.logger.info("Metadata database initialized")
        

        # æ˜¾ç¤ºå­˜å‚¨ä¿¡æ¯
        self._display_storage_info()

        # åˆå§‹åŒ–PaperIndexer
        self.paper_indexer = PaperIndexer(self.vector_db, self.metadata_db, None)
        self.logger.info("PaperIndexer initialized with real databases")

        # æ ¹æ®load_previous_dbå‚æ•°å†³å®šæ˜¯å¦ç´¢å¼•è¯­æ–™åº“æ•°æ®
        if load_previous_db:
            self.logger.info("Loading previous database - skipping corpus data indexing")
        else:
            self.logger.info("Indexing corpus data...")
            self._index_corpus_data(corpus_data)
        
        # åˆå§‹åŒ–æœç´¢ç­–ç•¥
        self._initialize_search_strategies()
        
        self.logger.info("Database initialization completed")
        
        # æ˜¾ç¤ºå­˜å‚¨ä¿¡æ¯
        self._display_storage_info()
    
    def _index_corpus_data(self, corpus_data: List[Dict]) -> None:
        """ç´¢å¼•è¯­æ–™åº“æ•°æ®åˆ°æ•°æ®åº“
        
        Args:
            corpus_data: è¯­æ–™åº“æ•°æ®åˆ—è¡¨
        """
        self.logger.info(f"Converting {len(corpus_data)} documents to DocSet format...")
        
        # æ­¥éª¤1: è½¬æ¢æ•°æ®æ ¼å¼ Dict -> DocSet
        docsets = []
        for doc in tqdm(corpus_data, desc="Converting to DocSet format"):
            try:
                docset = DocSet(
                    doc_id=str(doc["corpusid"]),
                    title=doc.get("title", ""),
                    abstract=doc.get("abstract", ""),
                    authors=[],  # LitSearch æ•°æ®é›†æ²¡æœ‰ä½œè€…ä¿¡æ¯
                    categories=[],  # LitSearch æ•°æ®é›†æ²¡æœ‰åˆ†ç±»ä¿¡æ¯
                    published_date="",  # LitSearch æ•°æ®é›†æ²¡æœ‰å‘å¸ƒæ—¥æœŸ
                    text_chunks=[],  # ç©ºåˆ—è¡¨
                    figure_chunks=[],  # ç©ºåˆ—è¡¨
                    table_chunks=[],  # ç©ºåˆ—è¡¨
                    metadata={},  # ç©ºå­—å…¸
                    pdf_path="",  # ç©ºå­—ç¬¦ä¸²
                    HTML_path=None,
                    comments=None
                )
                docsets.append(docset)
            except Exception as e:
                self.logger.error(f"Failed to convert document {doc.get('corpusid', 'unknown')}: {str(e)}")
        
        self.logger.info(f"Successfully converted {len(docsets)} documents to DocSet format")
        
        # æ­¥éª¤2: ä½¿ç”¨ PaperIndexer ç´¢å¼•æ–‡æ¡£
        self.logger.info(f"Indexing {len(docsets)} documents using PaperIndexer...")
        indexing_status = self.paper_indexer.index_papers(
            papers=docsets,
            store_images=False,  # LitSearch æ•°æ®é›†ä¸åŒ…å«å›¾ç‰‡
            keep_temp_image=False
        )
        
        # æ­¥éª¤3: ç»Ÿè®¡ç´¢å¼•ç»“æœ
        total_docs = len(indexing_status)
        metadata_success = sum(1 for status in indexing_status.values() if status.get("metadata", False))
        vector_success = sum(1 for status in indexing_status.values() if status.get("vectors", False))
        
        self.logger.info(f"Indexing completed:")
        self.logger.info(f"  Total documents: {total_docs}")
        self.logger.info(f"  Metadata indexed: {metadata_success}/{total_docs}")
        self.logger.info(f"  Vectors indexed: {vector_success}/{total_docs}")
        
        # è®°å½•å¤±è´¥çš„æ–‡æ¡£
        failed_docs = [doc_id for doc_id, status in indexing_status.items() 
                       if not status.get("metadata", False) or not status.get("vectors", False)]
        if failed_docs:
            self.logger.warning(f"Failed to fully index {len(failed_docs)} documents: {failed_docs[:10]}...")
    
    def _initialize_search_strategies(self) -> None:
        """æ ¹æ®å·²åˆå§‹åŒ–çš„æ•°æ®åº“åˆå§‹åŒ–æœç´¢ç­–ç•¥"""
        self.logger.info("Initializing search strategies...")
        
        strategies_config = self.litsearch_config.get('search_strategies', {})
        
        # åˆå§‹åŒ–å‘é‡æœç´¢ç­–ç•¥ï¼ˆå¦‚æœå‘é‡æ•°æ®åº“å·²åˆå§‹åŒ–ä¸”å¯ç”¨ï¼‰
        if self.vector_db is not None and strategies_config.get('vector', {}).get('enabled', False):
            vector_config = strategies_config.get('vector', {})
            # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦æ ¹æ®å®é™…çš„VectorSearchStrategyæ„é€ å‡½æ•°è°ƒæ•´
            # æš‚æ—¶ä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬
            self.search_strategies['vector'] = {
                'strategy': None,  # å°†åœ¨å®é™…æœç´¢æ—¶åˆ›å»º
                'config': vector_config
            }
            self.logger.info("Vector search strategy configured")
        
        # åˆå§‹åŒ–TF-IDFæœç´¢ç­–ç•¥ï¼ˆå¦‚æœå…ƒæ•°æ®æ•°æ®åº“å·²åˆå§‹åŒ–ä¸”å¯ç”¨ï¼‰
        if self.metadata_db is not None and strategies_config.get('tfidf', {}).get('enabled', False):
            tfidf_config = strategies_config.get('tfidf', {})
            self.search_strategies['tfidf'] = {
                'strategy': None,  # å°†åœ¨å®é™…æœç´¢æ—¶åˆ›å»º
                'config': tfidf_config
            }
            self.logger.info("TF-IDF search strategy configured")
        
        # åˆå§‹åŒ–æ··åˆæœç´¢ç­–ç•¥ï¼ˆå¦‚æœä¸¤ä¸ªæ•°æ®åº“éƒ½åˆå§‹åŒ–ä¸”å¯ç”¨ï¼‰
        if (self.vector_db is not None and self.metadata_db is not None and 
            strategies_config.get('hybrid', {}).get('enabled', False)):
            hybrid_config = strategies_config.get('hybrid', {})
            self.search_strategies['hybrid'] = {
                'strategy': None,  # å°†åœ¨å®é™…æœç´¢æ—¶åˆ›å»º
                'config': hybrid_config
            }
            self.logger.info("Hybrid search strategy configured")
        
        self.logger.info(f"Initialized {len(self.search_strategies)} search strategies")
    
    '''
    def execute(self) -> Dict[str, Any]:
        """æ‰§è¡Œå®Œæ•´çš„æµ‹è¯•æµç¨‹ï¼ˆé›†æˆæ¸…ç†æ§åˆ¶ï¼‰
        
        Returns:
            æµ‹è¯•ç»“æœå­—å…¸
        """
        # ä½¿ç”¨åŸºç±»çš„ execute æ–¹æ³•ï¼Œå®ƒä¼šè‡ªåŠ¨è°ƒç”¨ setup() -> run_tests() -> teardown()
        return super().execute()
    '''
    
    def run_tests(self) -> Dict[str, Any]:
        """è¿è¡ŒLitSearchæµ‹è¯•
        
        Returns:
            æµ‹è¯•ç»“æœå­—å…¸
        """
        self.logger.info("Running LitSearch tests with PaperIndexer...")
        
        results = {}
        '''
        # æµ‹è¯•å‘é‡æœç´¢ç­–ç•¥
        self.logger.info("Testing vector search strategy...")
        vector_result = self._test_paper_indexer_vector_search()
        results['vector_search'] = vector_result
        
        # æµ‹è¯•TF-IDFæœç´¢ç­–ç•¥
        self.logger.info("Testing TF-IDF search strategy...")
        tfidf_result = self._test_paper_indexer_tfidf_search()
        results['tfidf_search'] = tfidf_result
        
        
        # æµ‹è¯•æ··åˆæœç´¢ç­–ç•¥
        self.logger.info("Testing hybrid search strategy...")
        hybrid_result = self._test_paper_indexer_hybrid_search()
        results['hybrid_search'] = hybrid_result
        '''
        # è¿è¡Œå®Œæ•´è¯„ä¼°
        self.logger.info("Running comprehensive evaluation...")
        evaluation_result = self._run_comprehensive_evaluation()
        results['comprehensive_evaluation'] = evaluation_result
        
        # ç»Ÿè®¡æµ‹è¯•ç»“æœ
        total_tests = len(results)
        passed_tests = sum(1 for result in results.values() if result.get('success', False))
        
        self.logger.info(f"Test Results: {passed_tests}/{total_tests} tests passed")
        
        return results
    
    def _test_paper_indexer_vector_search(self) -> Dict[str, Any]:
        """ä½¿ç”¨PaperIndexeræµ‹è¯•å‘é‡æœç´¢ç­–ç•¥
        
        Returns:
            æµ‹è¯•ç»“æœå­—å…¸
        """
        try:
            # é€‰æ‹©ä¸€äº›æµ‹è¯•æŸ¥è¯¢
            test_queries = list(self.ground_truth.keys())[:10]  # ä½¿ç”¨å‰10ä¸ªæŸ¥è¯¢è¿›è¡Œæµ‹è¯•
            
            # æ‰§è¡Œæœç´¢
            search_results = {}
            for query in test_queries:
                results = self.paper_indexer.find_similar_papers(
                    query=query,
                    top_k=5,
                    search_strategies=[('vector', 0.8)],
                    result_include_types=['metadata', 'search_parameters']
                )
                search_results[query] = [result.get('doc_id') for result in results if result.get('doc_id')]
            
            # è®¡ç®—åŸºæœ¬æŒ‡æ ‡
            total_queries = len(test_queries)
            successful_queries = len([q for q in test_queries if search_results.get(q)])
            
            success = successful_queries > 0
            details = f"Vector search: {successful_queries}/{total_queries} queries returned results"
            
            self.log_test_result("Vector Search", success, details)
            return {
                'success': success, 
                'successful_queries': successful_queries,
                'total_queries': total_queries,
                'details': details,
                'search_results': search_results
            }
            
        except Exception as e:
            self.log_test_result("Vector Search", False, f"Error: {str(e)}")
            return {'success': False, 'error': str(e)}
    
    def _test_paper_indexer_tfidf_search(self) -> Dict[str, Any]:
        """ä½¿ç”¨PaperIndexeræµ‹è¯•TF-IDFæœç´¢ç­–ç•¥
        
        Returns:
            æµ‹è¯•ç»“æœå­—å…¸
        """
        try:
            # é€‰æ‹©ä¸€äº›æµ‹è¯•æŸ¥è¯¢
            test_queries = list(self.ground_truth.keys())[:10]  # ä½¿ç”¨å‰10ä¸ªæŸ¥è¯¢è¿›è¡Œæµ‹è¯•
            
            # æ‰§è¡Œæœç´¢
            search_results = {}
            for query in test_queries:
                results = self.paper_indexer.find_similar_papers(
                    query=query,
                    top_k=5,
                    search_strategies=[('tf-idf', 0.5)],
                    result_include_types=['metadata', 'search_parameters']
                )
                search_results[query] = [result.get('doc_id') for result in results if result.get('doc_id')]
            
            # è®¡ç®—åŸºæœ¬æŒ‡æ ‡
            total_queries = len(test_queries)
            successful_queries = len([q for q in test_queries if search_results.get(q)])
            
            success = successful_queries > 0
            details = f"TF-IDF search: {successful_queries}/{total_queries} queries returned results"
            
            self.log_test_result("TF-IDF Search", success, details)
            return {
                'success': success, 
                'successful_queries': successful_queries,
                'total_queries': total_queries,
                'details': details,
                'search_results': search_results
            }
            
        except Exception as e:
            self.log_test_result("TF-IDF Search", False, f"Error: {str(e)}")
            return {'success': False, 'error': str(e)}
    
    def _test_paper_indexer_hybrid_search(self) -> Dict[str, Any]:
        """ä½¿ç”¨PaperIndexeræµ‹è¯•æ··åˆæœç´¢ç­–ç•¥
        
        Returns:
            æµ‹è¯•ç»“æœå­—å…¸
        """
        try:
            # é€‰æ‹©ä¸€äº›æµ‹è¯•æŸ¥è¯¢
            test_queries = list(self.ground_truth.keys())[:10]  # ä½¿ç”¨å‰10ä¸ªæŸ¥è¯¢è¿›è¡Œæµ‹è¯•
            
            # æ‰§è¡Œæœç´¢
            search_results = {}
            for query in test_queries:
                results = self.paper_indexer.find_similar_papers(
                    query=query,
                    top_k=5,
                    search_strategies=[('vector', 0.8), ('tf-idf', 0.5)],
                    result_include_types=['metadata', 'search_parameters']
                )
                search_results[query] = [result.get('doc_id') for result in results if result.get('doc_id')]
            
            # è®¡ç®—åŸºæœ¬æŒ‡æ ‡
            total_queries = len(test_queries)
            successful_queries = len([q for q in test_queries if search_results.get(q)])
            
            success = successful_queries > 0
            details = f"Hybrid search: {successful_queries}/{total_queries} queries returned results"
            
            self.log_test_result("Hybrid Search", success, details)
            return {
                'success': success, 
                'successful_queries': successful_queries,
                'total_queries': total_queries,
                'details': details,
                'search_results': search_results
            }
            
        except Exception as e:
            self.log_test_result("Hybrid Search", False, f"Error: {str(e)}")
            return {'success': False, 'error': str(e)}
    
    def _test_search_strategy(self, strategy_name: str, strategy_info: Dict) -> Dict[str, Any]:
        """æµ‹è¯•å•ä¸ªæœç´¢ç­–ç•¥
        
        Args:
            strategy_name: ç­–ç•¥åç§°
            strategy_info: ç­–ç•¥é…ç½®ä¿¡æ¯
            
        Returns:
            æµ‹è¯•ç»“æœå­—å…¸
        """
        try:
            # é€‰æ‹©ä¸€äº›æµ‹è¯•æŸ¥è¯¢
            test_queries = list(self.ground_truth.keys())[:10]  # ä½¿ç”¨å‰10ä¸ªæŸ¥è¯¢è¿›è¡Œæµ‹è¯•
            
            # æ‰§è¡Œæœç´¢
            search_results = {}
            for query in test_queries:
                if strategy_name == 'vector':
                    results = self.vector_db.search(query, top_k=5)
                    search_results[query] = [result[0].doc_id for result in results]
                elif strategy_name == 'tfidf':
                    results = self.metadata_db.search(query, top_k=5)
                    search_results[query] = [result['doc_id'] for result in results]
                elif strategy_name == 'hybrid':
                    # æ··åˆæœç´¢éœ€è¦ç»“åˆä¸¤ç§ç­–ç•¥
                    vector_results = self.vector_db.search(query, top_k=10)
                    tfidf_results = self.metadata_db.search(query, top_k=10)
                    
                    # ç®€å•çš„æ··åˆç­–ç•¥ï¼šåˆå¹¶ç»“æœå¹¶å»é‡
                    vector_docs = [result[0].doc_id for result in vector_results]
                    tfidf_docs = [result['doc_id'] for result in tfidf_results]
                    
                    # åˆå¹¶å¹¶å»é‡ï¼Œä¿æŒé¡ºåº
                    combined_docs = []
                    seen = set()
                    for doc_id in vector_docs + tfidf_docs:
                        if doc_id not in seen:
                            combined_docs.append(doc_id)
                            seen.add(doc_id)
                    
                    search_results[query] = combined_docs[:5]  # å–å‰5ä¸ª
            
            # è®¡ç®—åŸºæœ¬æŒ‡æ ‡
            total_queries = len(test_queries)
            successful_queries = len([q for q in test_queries if search_results.get(q)])
            
            success = successful_queries > 0
            details = f"Strategy {strategy_name}: {successful_queries}/{total_queries} queries returned results"
            
            self.log_test_result(f"{strategy_name.title()} Search", success, details)
            return {
                'success': success, 
                'successful_queries': successful_queries,
                'total_queries': total_queries,
                'details': details,
                'search_results': search_results
            }
            
        except Exception as e:
            self.log_test_result(f"{strategy_name.title()} Search", False, f"Error: {str(e)}")
            return {'success': False, 'error': str(e)}
    
    def _run_comprehensive_evaluation(self) -> Dict[str, Any]:
        """è¿è¡Œç»¼åˆè¯„ä¼°
        
        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        try:
            # ä½¿ç”¨SearchEvaluatorè¿›è¡Œç»¼åˆè¯„ä¼°
            evaluator = SearchEvaluator()
            
            # å‡†å¤‡è¯„ä¼°æ•°æ®
            retrieval_results = {}
            
            # ä½¿ç”¨PaperIndexerä¸ºæ¯ä¸ªç­–ç•¥å‡†å¤‡æ£€ç´¢ç»“æœ
            strategies = {
                'vector': [('vector', self.litsearch_config.get('search_strategies', {}).get('vector', {}).get('similarity_cutoff', 0.5))],
               #'tfidf': [('tf-idf', 0.5)],
                #'hybrid': [('vector', 0.8), ('tf-idf', 0.5)]
            }
            
            for strategy_name, search_strategies in strategies.items():
                strategy_results = {}
                
                for query in list(self.ground_truth.keys()):
                    try:
                        results = self.paper_indexer.find_similar_papers(
                            query=query,
                            top_k=10,
                            search_strategies=search_strategies,
                            result_include_types=['metadata', 'search_parameters']
                        )
                        strategy_results[query] = [result.get('doc_id') for result in results if result.get('doc_id')]
                        
                    except Exception as e:
                        self.logger.warning(f"Failed to search query '{query}' with {strategy_name} strategy: {str(e)}")
                        strategy_results[query] = []
                
                retrieval_results[strategy_name] = strategy_results
            
            # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
            evaluation_results = SearchEvaluator.compare_strategies(retrieval_results, self.ground_truth)
            
            # ä¿å­˜è¯„ä¼°ç»“æœ
            self.evaluation_results = evaluation_results
            
            # ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
            self._generate_evaluation_report(evaluation_results)
            
            success = len(evaluation_results) > 0
            details = f"Comprehensive evaluation completed for {len(evaluation_results)} strategies"
            
            self.log_test_result("Comprehensive Evaluation", success, details)
            return {
                'success': success,
                'evaluation_results': evaluation_results,
                'details': details
            }
            
        except Exception as e:
            self.log_test_result("Comprehensive Evaluation", False, f"Error: {str(e)}")
            return {'success': False, 'error': str(e)}
    
    def _generate_evaluation_report(self, evaluation_results: Dict[str, Dict[str, float]]) -> None:
        """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
        
        Args:
            evaluation_results: è¯„ä¼°ç»“æœå­—å…¸
        """
        try:
            # åˆ›å»ºç»“æœç›®å½•
            results_path = self.litsearch_config.get('test', {}).get('results_path', 'litsearch_test_results')
            os.makedirs(results_path, exist_ok=True)
            
            # ç”ŸæˆMarkdownæŠ¥å‘Š
            report_path = os.path.join(results_path, 'evaluation_report.md')
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write("# LitSearch æœç´¢ç­–ç•¥è¯„ä¼°æŠ¥å‘Š\n\n")
                f.write("## æ¦‚è¿°\n\n")
                
                # æ‰¾å‡ºæœ€ä½³ç­–ç•¥
                if evaluation_results:
                    best_strategy = max(evaluation_results.items(), key=lambda x: x[1].get("map", 0))[0]
                    f.write(f"**æœ€ä½³ç­–ç•¥**: {best_strategy}\n\n")
                
                f.write("## è¯¦ç»†ç»“æœ\n\n")
                f.write("| ç­–ç•¥ | MAP | Precision@1 | Precision@5 | Precision@10 | Recall@10 | æŸ¥è¯¢æ•°é‡ |\n")
                f.write("| ---- | --- | ----------- | ----------- | ------------ | --------- | -------- |\n")
                
                for strategy_name, metrics in evaluation_results.items():
                    f.write(f"| {strategy_name} | {metrics.get('map', 0):.4f} | "
                           f"{metrics.get('precision@1', 0):.4f} | {metrics.get('precision@5', 0):.4f} | "
                           f"{metrics.get('precision@10', 0):.4f} | {metrics.get('recall@10', 0):.4f} | "
                           f"{metrics.get('query_count', 0)} |\n")
                
                f.write("\n## ç­–ç•¥æ¯”è¾ƒ\n\n")
                
                # MAPæ¯”è¾ƒ
                f.write("### MAP (å¹³å‡ç²¾åº¦å‡å€¼)\n\n")
                map_sorted = sorted(evaluation_results.items(), key=lambda x: x[1].get("map", 0), reverse=True)
                for i, (strategy, metrics) in enumerate(map_sorted, 1):
                    f.write(f"{i}. **{strategy}**: {metrics.get('map', 0):.4f}\n")
                
                f.write("\n### Precision@10\n\n")
                p10_sorted = sorted(evaluation_results.items(), key=lambda x: x[1].get("precision@10", 0), reverse=True)
                for i, (strategy, metrics) in enumerate(p10_sorted, 1):
                    f.write(f"{i}. **{strategy}**: {metrics.get('precision@10', 0):.4f}\n")
                
                f.write("\n### Recall@10\n\n")
                r10_sorted = sorted(evaluation_results.items(), key=lambda x: x[1].get("recall@10", 0), reverse=True)
                for i, (strategy, metrics) in enumerate(r10_sorted, 1):
                    f.write(f"{i}. **{strategy}**: {metrics.get('recall@10', 0):.4f}\n")
                
                f.write("\n## ç»“è®º\n\n")
                if evaluation_results:
                    best_strategy = max(evaluation_results.items(), key=lambda x: x[1].get("map", 0))[0]
                    best_map = evaluation_results[best_strategy].get('map', 0)
                    f.write(f"åŸºäºè¯„ä¼°ç»“æœï¼Œ**{best_strategy}** ç­–ç•¥åœ¨æ•´ä½“æ€§èƒ½ä¸Šè¡¨ç°æœ€ä½³ï¼Œå…¶ MAP åˆ†æ•°ä¸º {best_map:.4f}ã€‚\n")
            
            # ä¿å­˜JSONæ ¼å¼çš„è¯¦ç»†ç»“æœ
            json_path = os.path.join(results_path, 'evaluation_results.json')
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(evaluation_results, f, indent=2, ensure_ascii=False)
            
            self.logger.info(f"Evaluation report saved to: {report_path}")
            self.logger.info(f"Detailed results saved to: {json_path}")
            
        except Exception as e:
            self.logger.error(f"Failed to generate evaluation report: {str(e)}")
    
    def _format_bytes(self, bytes_size: int) -> str:
        """Format bytes into human readable format.
        
        Args:
            bytes_size: Size in bytes
            
        Returns:
            Formatted string with appropriate unit
        """
        for unit in ['B', 'KB', 'MB', 'GB', 'TB']:
            if bytes_size < 1024.0:
                return f"{bytes_size:.1f} {unit}"
            bytes_size /= 1024.0
        return f"{bytes_size:.1f} PB"
    
    def _display_storage_info(self) -> None:
        """Display current storage information for metadata_db and vector_db.
        
        Shows detailed information about database storage including file sizes,
        record counts, and database connectivity status.
        """
        try:
            self.logger.info("=" * 60)
            self.logger.info("ğŸ“Š CURRENT STORAGE INFORMATION")
            self.logger.info("=" * 60)
            
            # Vector DB Storage Information
            vector_db_path = self.config['vector_db']['db_path']
            self.logger.info(f"ğŸ—‚ï¸  Vector Database Path: {vector_db_path}")
            
            # Check if vector database exists and get info
            if os.path.exists(vector_db_path):
                index_file = os.path.join(vector_db_path, "index.pkl")
                faiss_file = os.path.join(vector_db_path, "index.faiss")
                
                if os.path.exists(index_file) and os.path.exists(faiss_file):
                    # Get file sizes
                    index_size = os.path.getsize(index_file)
                    faiss_size = os.path.getsize(faiss_file)
                    total_size = index_size + faiss_size
                    
                    self.logger.info(f"   âœ… Vector DB exists")
                    self.logger.info(f"   ğŸ“ Index file size: {self._format_bytes(index_size)}")
                    self.logger.info(f"   ğŸ“ FAISS file size: {self._format_bytes(faiss_size)}")
                    self.logger.info(f"   ğŸ“Š Total size: {self._format_bytes(total_size)}")
                    
                    # Try to get vector count
                    try:
                        # First try to use initialized VectorDB object if available
                        if hasattr(self, 'vector_db') and self.vector_db and hasattr(self.vector_db, 'faiss_store'):
                            vector_count = len(self.vector_db.faiss_store.docstore._dict)
                            self.logger.info(f"   ğŸ”¢ Vector count: {vector_count}")
                        else:
                            # Fallback: Quick check without full model loading
                            import pickle
                            with open(index_file, 'rb') as f:
                                index_data = pickle.load(f)
                                if hasattr(index_data, 'docstore') and hasattr(index_data.docstore, '_dict'):
                                    vector_count = len(index_data.docstore._dict)
                                    self.logger.info(f"   ğŸ”¢ Vector count: {vector_count}")
                                else:
                                    self.logger.info(f"   ğŸ”¢ Vector count: Unable to determine")
                    except Exception as e:
                        self.logger.info(f"   ğŸ”¢ Vector count: Unable to determine ({str(e)})")
                else:
                    self.logger.info(f"   âŒ Vector DB files incomplete or missing")
            else:
                self.logger.info(f"   âŒ Vector DB directory does not exist")
            
            self.logger.info("")
            
            # Metadata DB Storage Information
            db_url = self.config['metadata_db']['db_url']
            self.logger.info(f"ğŸ—„ï¸  Metadata Database URL: {db_url}")
            
            # Try to connect and get metadata info
            try:
                engine = create_engine(db_url)
                with engine.connect() as conn:
                    # Check if tables exist
                    inspector = inspect(engine)
                    tables = inspector.get_table_names()
                    
                    if 'papers' in tables:
                        # Get paper count
                        result = conn.execute(text("SELECT COUNT(*) FROM papers"))
                        paper_count = result.scalar()
                        self.logger.info(f"   âœ… Metadata DB connected")
                        self.logger.info(f"   ğŸ“„ Papers table exists")
                        self.logger.info(f"   ğŸ”¢ Paper count: {paper_count}")
                        
                        # Get text chunks count if table exists
                        if 'text_chunks' in tables:
                            result = conn.execute(text("SELECT COUNT(*) FROM text_chunks"))
                            chunk_count = result.scalar()
                            self.logger.info(f"   ğŸ“ Text chunks count: {chunk_count}")
                        
                        # Get database size (PostgreSQL specific)
                        if 'postgresql' in db_url:
                            try:
                                result = conn.execute(text("""
                                    SELECT pg_size_pretty(pg_database_size(current_database())) as db_size
                                """))
                                db_size = result.scalar()
                                self.logger.info(f"   ğŸ’¾ Database size: {db_size}")
                            except Exception:
                                self.logger.info(f"   ğŸ’¾ Database size: Unable to determine")
                    else:
                        self.logger.info(f"   âŒ Papers table does not exist")
                        self.logger.info(f"   ğŸ“‹ Available tables: {', '.join(tables) if tables else 'None'}")
                        
            except Exception as e:
                self.logger.info(f"   âŒ Cannot connect to metadata DB: {str(e)}")
            
            self.logger.info("=" * 60)
            
        except Exception as e:
            self.logger.error(f"Failed to display storage information: {str(e)}")


#python3 -m test.index.litsearch_testbed

if __name__ == '__main__':
    config_path = Path("/data3/guofang/AIgnite-Solutions/AIgnite/experiments/recommendation/configs/litsearch_testbed_config_gritlm.yaml")
    
    if not config_path.exists():
        print(f"Error: Configuration file not found: {config_path}")
        print(f"Please create the configuration file or specify a different path with -c")
        sys.exit(1)
    
    # åˆ›å»ºå¹¶è¿è¡Œæµ‹è¯•åºŠ
    logger.info("Initializing LitSearchTestBed...")
    testbed = LitSearchTestBed(str(config_path))
    #testbed.setup()
    #testbed.execute()  # è¿™ä¼šè°ƒç”¨ check_environment(), load_data(), initialize_databases(), run_tests() å¹¶åŒ…å«æ¸…ç†æ§åˆ¶

    cleanup_before_test = testbed.litsearch_config.get('cleanup_before_test', True)
    cleanup_after_test = testbed.litsearch_config.get('cleanup_after_test', True)

    print(f"æ¸…ç†è®¾ç½®: æ¸…ç†å‰={cleanup_before_test}, æ¸…ç†å={cleanup_after_test}")
    
    # æ‰§è¡Œæµ‹è¯•ï¼Œä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„æ¸…ç†è®¾ç½®
    #testbed.setup(clean_before_test=cleanup_before_test)
    testbed.execute(clean_before_test=cleanup_before_test, clean_after_test=cleanup_after_test)
