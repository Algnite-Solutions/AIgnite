from bs4 import BeautifulSoup
from .docset import DocSet, TextChunk, FigureChunk, TableChunk, ChunkType
from pathlib import Path
#new
import fitz  # PyMuPDF
import os
import io
from datetime import datetime, timezone, timedelta
import arxiv
import os
import requests
from urllib.parse import urljoin
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import json
import time
import re
from typing import List, Tuple
import base64
from volcengine.visual.VisualService import VisualService
from spire.pdf.common import *
from spire.pdf import *
from abc import ABC, abstractmethod

class BasePDFExtractor(ABC):
    """Abstract base classes of the PDF extractor"""

    def __init__(self, docs, pdf_folder_path, image_folder_path, arxiv_pool, json_path, volcengine_ak, volcengine_sk, start_time, end_time):
        """
        Args:
        html_text_folder: the folder path used to store the .html file.
        pdf_folder_path: the folder path used to store the .pdf (and .md) file.
        arxiv_pool: path of a .txt file to store the arxiv_id which is serialized successfully
        image_folder_path:  the folder path used to store the .png file.
        json_path: the folder path used to store the .json file.
        volcengine_ak: get from https://console.volcengine.com/ai/ability/info/72
        volcengine_sk: get from https://console.volcengine.com/ai/ability/info/72
        start_time: the earliest paper you want
        end_time: the last paper you want
        """
        if docs is None:
            self.docs = []
        else:
            self.docs = docs
        self.pdf_folder_path = pdf_folder_path
        self.image_folder_path = image_folder_path
        self.pdf_paths = []
        self.arxiv_pool = arxiv_pool
        self.json_path = json_path
        self.date = datetime.now(timezone.utc).date()
        self.ak = volcengine_ak
        self.sk = volcengine_sk
        self.start_time = start_time
        self.end_time = end_time

    @abstractmethod
    def extract_all(self):
        """Carry out the complete extraction process"""
        pass

    @abstractmethod
    def remain_docparser(self):
        """provide help for HTMLparser"""
        pass

    @abstractmethod
    def init_docset(self):
        """Initialize the document metadata"""
        pass

    @abstractmethod
    def serialize_docs(self):
        """Serialize the extraction results into JSON"""
        pass

    @abstractmethod
    def pdf_text_chunk(self, markdown_path):
        """Extract the text chunk from Markdown produced by PDF"""
        pass

    @abstractmethod
    def pdf_images_chunk(self, markdown_path, image_folder_path, doc_id):
        """Extract the img chunk from Markdown produced by PDF"""
        pass

    @abstractmethod
    def pdf_tables_chunk(self, markdown_path):
        """Extract the table chunk from Markdown produced by PDF"""
        pass

class ArxivPDFExtractor(BasePDFExtractor):
    """
    A class used to extract information from daily arXiv PDFs and serialize it into JSON files.
    """
    def init_docset(self):
        """
        Initialize the docset with papers' some metadata:
        doc_id, title, authors, categories, published_date, abstract, pdf_path, HTML_path
        The 3 types of chunk remain to add
        """
        client = arxiv.Client()
        query = "cat:cs.* AND submittedDate:[" + self.start_time + " TO " + self.end_time + "]"
        search = arxiv.Search(
            query=query,
            max_results=None,  # You can set max papers you want here
            sort_by=arxiv.SortCriterion.SubmittedDate
        )
        print(f"grabbing arXiv papers in cs.* submitted from {self.start_time} to {self.end_time}......")

        tem = client.results(search)
        tem = list(tem)
        print("successful search!")
        for result in tem:
            print(1)
            arxiv_id = result.pdf_url.split('/')[-1]
            print(2)
            with open(self.arxiv_pool, "r", encoding="utf-8") as f:
                if arxiv_id in f.read():
                    print(f"{arxiv_id} is already extracted before!")
                    continue
            #add basic info
            print(3)
            add_doc = DocSet(
            doc_id=arxiv_id,
            title=result.title,
            authors=[author.name for author in result.authors],
            categories=result.categories,
            published_date=str(result.published),
            abstract=result.summary,
            pdf_path=str(os.path.join(self.pdf_folder_path, f'{arxiv_id}.pdf')),
            #Set htmlpath to None first and update it later
            HTML_path=None )
            print(4)

            print(arxiv_id)
            #download_arxiv_pdf(arxiv_id, self.pdf_folder_path)
            #result.download_pdf(dirpath = self.pdf_folder_path, filename=f"{arxiv_id}.pdf")
            success = download_paper(
                result=result,
                save_path=self.pdf_folder_path,
                filename=f"{arxiv_id}.pdf"
            )
            if not success:
                print(f"âŒ è®ºæ–‡ {result.title} ä¸‹è½½æœ€ç»ˆå¤±è´¥")

            print(5)

            self.docs.append(add_doc)

        #self.serialize_docs_init()

    def extract_all(self):
        """"All in one function"""
        self.init_docset()
        for doc in self.docs:
            path = doc.pdf_path
            print("getting markdown...")
            markdown_path = get_pdf_md(path,self.pdf_folder_path,doc.doc_id,self.ak,self.sk)
            print("done, begin chunking")
            if markdown_path:
                doc.figure_chunks = self.pdf_images_chunk(markdown_path,self.image_folder_path,doc.doc_id)
                doc.table_chunks = self.pdf_tables_chunk(markdown_path)
                doc.text_chunks = self.pdf_text_chunk(markdown_path)#ä¸€å®šåœ¨æœ€å
        self.serialize_docs()

    def remain_docparser(self):
        """
        Help HTMLExtractor. We don't use it in the process of PDF's extractor
        """
        for doc in self.docs:
            if doc.HTML_path == None and doc.pdf_path is not None:
                path = doc.pdf_path
                print("getting markdown...")
                markdown_path = get_pdf_md(path,self.pdf_folder_path,doc.doc_id,self.ak,self.sk)
                print("done, begin chunking")
                if markdown_path:
                    doc.figure_chunks = self.pdf_images_chunk(markdown_path,self.image_folder_path,doc.doc_id)
                    doc.table_chunks = self.pdf_tables_chunk(markdown_path)
                    doc.text_chunks = self.pdf_text_chunk(markdown_path)#ä¸€å®šåœ¨æœ€å
            elif doc.pdf_path == None:
                print("Neither PDF or HTML is avaliable.")
                   
    def pdf_images_chunk(self, markdown_path, image_folder_path, doc_id):
        figures = []

        try:
            # read Markdown
            with open(markdown_path, 'r', encoding='utf-8') as f:
                md_content = f.read()
                
            image_list = _parse_image_urls(md_content, doc_id)
            if not image_list:
                print("Warning: No image link was found in Markdown")
                return figures
                
            os.makedirs(image_folder_path, exist_ok=True)
            
            # download
            success_count = 0
            for name, url, caption in image_list:
                if _download_single_image(name, url, image_folder_path):
                    success_count += 1
                    figures.append(FigureChunk(
                        id = None,
                        title = name,
                        type = ChunkType.FIGURE,
                        image_path = str(os.path.join(image_folder_path, name)),
                        alt_text = "Refer to caption",
                        caption = caption
                    ))
            
            print(f"\nğŸ“Œ Download completed: process{len(image_list)} figures totally, {success_count} Successfully")
            
        except FileNotFoundError:
            print(f"Error: Markdown Not Found - {markdown_path}")
            
        except Exception as e:
            print(f"program exception:{str(e)}")
        
        #print(figures)
        return figures
        
    def pdf_tables_chunk(self, markdown_path):
        tables = []
        try:
            with open(markdown_path, 'r', encoding='utf-8') as f:
                md_content = f.read()

            soup = BeautifulSoup(md_content, 'html.parser')
            all_tables = soup.find_all('table')

            for idx, table in enumerate(all_tables):
                table_html = str(table)

                # Find the text position of this table in the Markdown content
                table_pos = md_content.find(table_html)
                context_before = md_content[max(0, table_pos - 500):table_pos]

                # Look for the Table title from the previous text
                caption_match = re.search(r'(Table\s*\d+[.:]?\s*)([^\n<]+)', context_before, re.IGNORECASE)
                if caption_match:
                    table_name = caption_match.group(1).strip().replace(':', '').replace('.', '')
                    caption_text = caption_match.group(2).strip()
                else:
                    table_name = f'table_{idx+1}'
                    caption_text = ''

                tables.append(TableChunk(
                    id=None,
                    title=table_name,
                    type=ChunkType.TABLE,
                    table_html=table_html,
                    caption=caption_text
                ))
                

        except FileNotFoundError:
            print(f"Error: Markdown Not Found - {markdown_path}")
        except Exception as e:
            print(f"program exception:{str(e)}")
        #print(tables)
        return tables
    
    def pdf_text_chunk(self, markdown_path) -> List[TextChunk]:
        all_text = []

        try:
            with open(markdown_path, 'r', encoding='utf-8') as f:
                md_content = f.read()
            md_content = re.sub(r'^!\[fig_[^\n]*\n?', '', md_content, flags=re.MULTILINE)

            # æŸ¥æ‰¾æ‰€æœ‰ä¸€çº§æ ‡é¢˜ï¼ˆ## å¼€å¤´ï¼Œæ’é™¤å¦‚ 2.1 å¼€å¤´çš„å­æ ‡é¢˜ï¼‰
            pattern = r'(?:^|\n)(##\s+(?!(?:[A-Za-z]+\.)?\d+\.\d+)[^\n]+)'
            matches = list(re.finditer(pattern, md_content))

            # ä¸ºæ–¹ä¾¿å¤„ç†ï¼Œè®°å½•æ‰€æœ‰æ®µè½èµ·å§‹ä½ç½®
            section_boundaries = [m.start() for m in matches]
            section_boundaries.append(len(md_content))  # åŠ å…¥æœ€åçš„ç»ˆç‚¹

            for i in range(len(matches)):
                start = section_boundaries[i]
                end = section_boundaries[i + 1]
                section_text = md_content[start:end].strip()

                header_line = matches[i].group(1).strip()
                title = header_line.lstrip('#').strip()

                section_id = f"text_{i+1}"

                all_text.append(TextChunk(
                    id=section_id,
                    type=ChunkType.TEXT,
                    title=title,
                    caption=title,
                    text=section_text
                ))

        except FileNotFoundError:
            print(f"é”™è¯¯ï¼šMarkdownæ–‡ä»¶æœªæ‰¾åˆ° - {markdown_path}")
        except Exception as e:
            print(f"ç¨‹åºå¼‚å¸¸ï¼š{str(e)}")
        return all_text
    
    def serialize_docs(self):
        """
        Serialize the extracted documents into JSON files.
        """
        output_dir = self.json_path
        for doc in self.docs:
            with open(self.arxiv_pool, "a", encoding="utf-8") as f:
                f.write(doc.doc_id+'\n')
            output_path = Path(output_dir) / f"{doc.doc_id}.json"
            with open(output_path, "w", encoding="utf-8") as f:
                doc_dict = doc.model_dump()
                json_str = json.dumps(doc_dict, indent=4)
                f.write(json_str)

############################################################### Some Tools ####################################################################
def compress_pdf(input_path: str, output_path: str = None, max_size_mb: float = 7.5) -> str:
    """
    å‹ç¼© PDF æ–‡ä»¶ï¼Œä½¿è¾“å‡ºæ–‡ä»¶å¤§å°å°äºæŒ‡å®šçš„æœ€å¤§ MBã€‚
    è‹¥å›¾ç‰‡é‡ç¼–ç ä¸ä¸åŒè´¨é‡å°è¯•ä»æœªè¾¾åˆ°ç›®æ ‡ï¼Œä¼šä»æœ€åä¸€é¡µå¼€å§‹é€é¡µè£å‰ªç›´åˆ°ç¬¦åˆæˆ–è£å®Œã€‚
    
    å‚æ•°ï¼š
        input_path (str): è¾“å…¥ PDF è·¯å¾„
        output_path (str): è¾“å‡º PDF è·¯å¾„ï¼ˆå¦‚æœä¸º None åˆ™è¦†ç›–åŸæ–‡ä»¶ï¼‰
        max_size_mb (float): æœ€å¤§æ–‡ä»¶å¤§å°ï¼ˆMBï¼‰
    
    è¿”å›ï¼š
        str: å‹ç¼©åæ–‡ä»¶è·¯å¾„ï¼ˆå¦‚æœè¦†ç›–åŸæ–‡ä»¶ï¼Œè¿”å›åŸæ–‡ä»¶è·¯å¾„ï¼‰
    """
    # å†³å®šå†™å…¥è·¯å¾„ï¼šå¦‚æœ output_path ä¸º Noneï¼Œåˆ™å†™å…¥ä¸´æ—¶æ–‡ä»¶ï¼Œæœ€åæ›¿æ¢åŸæ–‡ä»¶
    overwrite_original = output_path is None
    if overwrite_original:
        tmp_output = f"{input_path}.compress_tmp"
        write_path = tmp_output
        final_output_path = input_path
    else:
        write_path = output_path
        final_output_path = output_path

    if output_path is None:
        # ä¿æŒå…¼å®¹ï¼šä¸ç›´æ¥è¦†ç›–åŸæ–‡ä»¶ï¼Œä½¿ç”¨ä¸´æ—¶æ–‡ä»¶æœ€ååŸå­æ›¿æ¢
        pass
    else:
        # å¦‚æœä¼ å…¥çš„æ˜¯ç›®å½•ï¼Œåˆ™åœ¨è¯¥ç›®å½•ä¸‹æ„é€ æ–‡ä»¶å
        if os.path.isdir(output_path):
            base = os.path.splitext(os.path.basename(input_path))[0]
            write_path = os.path.join(output_path, f"{base}_compressed.pdf")
            final_output_path = write_path

    # è¾“å…¥æ–‡ä»¶å¤§å°ï¼ˆMBï¼‰
    try:
        original_size = os.path.getsize(input_path)
    except Exception:
        original_size = None

    if original_size is not None:
        print(f"è¾“å…¥: {input_path} -> {original_size / 1024 / 1024:.2f} MB")
    else:
        print(f"è¾“å…¥: {input_path} -> æ— æ³•è·å–å¤§å°")

    max_bytes = max_size_mb * 1024 * 1024
    quality = 90
    step = 10

    # å°è¯•å¯¼å…¥ Pillowï¼Œç”¨äºå›¾ç‰‡é‡ç¼–ç 
    try:
        from PIL import Image
        pillow_available = True
        print("Pillow: å¯ç”¨ï¼Œå¯ç”¨å›¾ç‰‡é‡ç¼–ç ")
    except Exception:
        pillow_available = False
        print("Pillow: ä¸å¯ç”¨ï¼Œä»…ä½¿ç”¨ PyMuPDF ä¿å­˜å‚æ•°")

    out_buf = None
    size = None

    # æ¯æ¬¡ä»åŸæ–‡ä»¶é‡æ–°æ‰“å¼€ï¼Œé¿å…é‡å¤å‹ç¼©åŒä¸€å›¾åƒ
    try:
        while quality >= 10:
            doc = fitz.open(input_path)
            # è‹¥å®‰è£…äº† Pillowï¼Œåˆ™å¯¹æ–‡æ¡£ä¸­æ‰€æœ‰å›¾ç‰‡ä»¥å½“å‰è´¨é‡é‡ç¼–ç 
            if pillow_available:
                for page_index in range(len(doc)):
                    page = doc[page_index]
                    images = page.get_images(full=True)
                    if not images:
                        continue
                    for img_info in images:
                        xref = img_info[0]
                        try:
                            img_dict = doc.extract_image(xref)
                            img_bytes = img_dict.get("image")
                            if not img_bytes:
                                continue
                            img = Image.open(io.BytesIO(img_bytes)).convert("RGB")
                            buf = io.BytesIO()
                            img.save(buf, format="JPEG", quality=quality)
                            new_bytes = buf.getvalue()
                            # ç”¨é‡ç¼–ç åçš„å­—èŠ‚æ›¿æ¢åŸå›¾
                            doc.update_image(xref, stream=new_bytes)
                        except Exception:
                            # é‡åˆ°ä»»ä½•å›¾ç‰‡å¤„ç†é—®é¢˜ï¼Œè·³è¿‡è¯¥å›¾ç‰‡
                            continue

            # ä½¿ç”¨ä»…å—æ”¯æŒçš„å‚æ•°ä¿å­˜åˆ°å†…å­˜å¹¶æ£€æŸ¥å¤§å°
            out_buf = io.BytesIO()
            doc.save(out_buf, garbage=4, deflate=True, clean=True, incremental=False)
            size = len(out_buf.getvalue())
            doc.close()

            print(f"å°è¯•: è´¨é‡={quality} -> {size / 1024 / 1024:.2f} MB")

            if size <= max_bytes:
                # å†™å…¥ä¸´æ—¶æˆ–ç›®æ ‡æ–‡ä»¶
                with open(write_path, "wb") as f:
                    f.write(out_buf.getvalue())
                # å¦‚æœéœ€è¦è¦†ç›–åŸæ–‡ä»¶ï¼ŒåŸå­æ›¿æ¢
                if overwrite_original:
                    try:
                        os.replace(write_path, final_output_path)
                    except Exception as e:
                        # è‹¥æ›¿æ¢å¤±è´¥ï¼Œä¿ç•™ä¸´æ—¶æ–‡ä»¶å¹¶æŠ›å‡º
                        print(f"âš ï¸ æ— æ³•æ›¿æ¢åŸæ–‡ä»¶: {e}ï¼Œä¸´æ—¶æ–‡ä»¶ä¿ç•™åœ¨ {write_path}")
                        raise
                if original_size is not None:
                    print(f"âœ… å‹ç¼©å®Œæˆ: {final_output_path} ({original_size / 1024 / 1024:.2f} MB -> {size / 1024 / 1024:.2f} MB, è´¨é‡={quality})")
                else:
                    print(f"âœ… å‹ç¼©å®Œæˆ: {final_output_path} ({size / 1024 / 1024:.2f} MB, è´¨é‡={quality})")
                return final_output_path

            quality -= step

        # è‹¥åˆ°è¿™é‡Œä»æœªè¾¾æ ‡ï¼Œå°è¯•è£å‰ªæœ€åå‡ é¡µï¼ˆä»æœ€åä¸€é¡µå¼€å§‹è£ï¼‰ï¼Œæ¯æ¬¡å°‘ä¸€é¡µç›´åˆ°ç¬¦åˆæˆ–è£å®Œ
        print("å¼€å§‹è£å‰ªé¡µé¢ï¼ˆä»æœ€åä¸€é¡µå¼€å§‹é€é¡µè£å‰ªï¼‰ä»¥å°è¯•è¾¾æ ‡...")
        try:
            orig_doc = fitz.open(input_path)
        except Exception as e:
            print(f"æ— æ³•æ‰“å¼€åŸæ–‡ä»¶è¿›è¡Œè£å‰ª: {e}")
            # è‹¥æ— æ³•æ‰“å¼€åŸæ–‡ä»¶ï¼Œç›´æ¥ä¿å­˜ä¸Šæ¬¡ç”Ÿæˆçš„ out_bufï¼ˆè‹¥æœ‰ï¼‰åˆ°å†™å…¥è·¯å¾„
            if out_buf is not None:
                with open(write_path, "wb") as f:
                    f.write(out_buf.getvalue())
                if overwrite_original:
                    try:
                        os.replace(write_path, final_output_path)
                    except Exception:
                        pass
            raise

        total_pages = len(orig_doc)
        # å¦‚æœæ–‡ä»¶æœ¬æ¥å°±åªæœ‰1é¡µä¸”æœªè¾¾æ ‡ï¼Œç›´æ¥ä¿å­˜å½“å‰ out_bufï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if total_pages <= 1:
            print("PDF åªæœ‰ 1 é¡µï¼Œæ— æ³•è£å‰ªæ›´å¤šé¡µã€‚")
            if out_buf is not None:
                with open(write_path, "wb") as f:
                    f.write(out_buf.getvalue())
                if overwrite_original:
                    try:
                        os.replace(write_path, final_output_path)
                    except Exception:
                        pass
            orig_doc.close()
            if original_size is not None:
                print(f"âš ï¸ æ— æ³•å®Œå…¨å‹ç¼©åˆ° {max_size_mb} MB ä»¥ä¸‹ï¼Œå·²å°½å¯èƒ½å‹ç¼© ({original_size / 1024 / 1024:.2f} MB -> {size / 1024 / 1024:.2f} MB)")
            else:
                print(f"âš ï¸ æ— æ³•å®Œå…¨å‹ç¼©åˆ° {max_size_mb} MB ä»¥ä¸‹ï¼Œå·²å°½å¯èƒ½å‹ç¼© ({size / 1024 / 1024:.2f} MB)")
            return final_output_path

        # è£å‰ªå¾ªç¯ï¼šä¿ç•™é¡µæ•°ä» total_pages-1 åˆ° 1
        success = False
        for keep in range(total_pages - 1, 0, -1):
            new_doc = fitz.open()  # ç©ºæ–‡æ¡£
            try:
                new_doc.insert_pdf(orig_doc, from_page=0, to_page=keep - 1)
                # å¯¹æ–°æ–‡æ¡£è¿›è¡Œå›¾ç‰‡é‡ç¼–ç ï¼ˆå¦‚æœ Pillow å¯ç”¨ï¼‰ï¼Œä½¿ç”¨è¾ƒä½è´¨é‡ä»¥æ›´æ˜“è¾¾æ ‡
                if pillow_available:
                    trim_quality = max(10, quality) if quality >= 10 else 30
                    for page_index in range(len(new_doc)):
                        page = new_doc[page_index]
                        images = page.get_images(full=True)
                        if not images:
                            continue
                        for img_info in images:
                            xref = img_info[0]
                            try:
                                img_dict = new_doc.extract_image(xref)
                                img_bytes = img_dict.get("image")
                                if not img_bytes:
                                    continue
                                img = Image.open(io.BytesIO(img_bytes)).convert("RGB")
                                buf = io.BytesIO()
                                img.save(buf, format="JPEG", quality=trim_quality)
                                new_bytes = buf.getvalue()
                                new_doc.update_image(xref, stream=new_bytes)
                            except Exception:
                                continue

                buf = io.BytesIO()
                new_doc.save(buf, garbage=4, deflate=True, clean=True, incremental=False)
                this_size = len(buf.getvalue())
                print(f"è£å‰ª: ä¿ç•™ {keep} é¡µ -> {this_size / 1024 / 1024:.2f} MB")

                if this_size <= max_bytes:
                    with open(write_path, "wb") as f:
                        f.write(buf.getvalue())
                    if overwrite_original:
                        try:
                            os.replace(write_path, final_output_path)
                        except Exception as e:
                            print(f"âš ï¸ æ— æ³•æ›¿æ¢åŸæ–‡ä»¶: {e}ï¼Œä¸´æ—¶æ–‡ä»¶ä¿ç•™åœ¨ {write_path}")
                            raise
                    if original_size is not None:
                        print(f"âœ… è£å‰ªå¹¶å‹ç¼©å®Œæˆ: {final_output_path} ({original_size / 1024 / 1024:.2f} MB -> {this_size / 1024 / 1024:.2f} MB, ä¿ç•™ {keep} é¡µ)")
                    else:
                        print(f"âœ… è£å‰ªå¹¶å‹ç¼©å®Œæˆ: {final_output_path} ({this_size / 1024 / 1024:.2f} MB, ä¿ç•™ {keep} é¡µ)")
                    success = True
                    new_doc.close()
                    break
            finally:
                try:
                    new_doc.close()
                except Exception:
                    pass

        orig_doc.close()

        if not success:
            # æ— æ³•é€šè¿‡è£å‰ªè¾¾åˆ°ç›®æ ‡ï¼Œä¿å­˜æœ€åä¸€æ¬¡å°è¯•çš„ç»“æœï¼ˆå°½å¯èƒ½å‹ç¼©ï¼‰
            if out_buf is not None:
                with open(write_path, "wb") as f:
                    f.write(out_buf.getvalue())
                if overwrite_original:
                    try:
                        os.replace(write_path, final_output_path)
                    except Exception:
                        pass
            if original_size is not None and size is not None:
                print(f"âš ï¸ è£å‰ªåä»æ— æ³•å®Œå…¨å‹ç¼©åˆ° {max_size_mb} MB ä»¥ä¸‹ï¼Œå·²å°½å¯èƒ½å‹ç¼© ({original_size / 1024 / 1024:.2f} MB -> {size / 1024 / 1024:.2f} MB)")
            elif size is not None:
                print(f"âš ï¸ è£å‰ªåä»æ— æ³•å®Œå…¨å‹ç¼©åˆ° {max_size_mb} MB ä»¥ä¸‹ï¼Œå·²å°½å¯èƒ½å‹ç¼© ({size / 1024 / 1024:.2f} MB)")
        return final_output_path
    finally:
        # æ¸…ç†å¯èƒ½æ®‹ç•™çš„ä¸´æ—¶æ–‡ä»¶ï¼ˆä»…å½“å†™å…¥ä¸´æ—¶è¦†ç›–åŸæ–‡ä»¶ä¸”æœ€ç»ˆæ–‡ä»¶å·²è¢«æ›¿æ¢æ—¶ï¼‰
        if overwrite_original:
            tmp = f"{input_path}.compress_tmp"
            if os.path.exists(tmp):
                # å¦‚æœåŸå­æ›¿æ¢å·²ç»å‘ç”Ÿï¼Œä¸´æ—¶æ–‡ä»¶å¯èƒ½å·²è¢«ç§»åŠ¨ï¼›ä»…åœ¨è¿˜å­˜åœ¨æ—¶å°è¯•åˆ é™¤
                try:
                    os.remove(tmp)
                except Exception:
                    pass

def verify_pdf(file_path: str) -> bool:
    """
    éªŒè¯PDFæ–‡ä»¶æ˜¯å¦æœ‰æ•ˆ
    
    Args:
        file_path: PDFæ–‡ä»¶è·¯å¾„
    
    Returns:
        bool: æ–‡ä»¶æ˜¯å¦æœ‰æ•ˆ
    """
    try:
        with open(file_path, 'rb') as f:
            header = f.read(5)
            if not header.startswith(b'%PDF-'):
                print(f"âŒ {file_path} ä¸æ˜¯æœ‰æ•ˆçš„PDFæ–‡ä»¶")
                return False
        return True
    except Exception as e:
        print(f"âŒ éªŒè¯PDFæ–‡ä»¶å¤±è´¥ {file_path}: {str(e)}")
        return False

def download_pdf_with_retry(url: str, save_path: str, filename: str, max_retries: int = 3) -> bool:
    """
    ä½¿ç”¨é‡è¯•æœºåˆ¶ä¸‹è½½PDFæ–‡ä»¶
    
    Args:
        url: PDFæ–‡ä»¶çš„URL
        save_path: ä¿å­˜è·¯å¾„
        filename: æ–‡ä»¶å
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
    
    Returns:
        bool: ä¸‹è½½æ˜¯å¦æˆåŠŸ
    """
    session = requests.Session()
    retries = Retry(
        total=max_retries,
        backoff_factor=1,
        status_forcelist=[500, 502, 503, 504]
    )
    session.mount('http://', HTTPAdapter(max_retries=retries))
    session.mount('https://', HTTPAdapter(max_retries=retries))
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (compatible; AIgniteBot/1.0; +https://github.com/Algnite-Solutions/AIgnite)',
        'Accept': 'application/pdf'
    }
    
    temp_path = os.path.join(save_path, f"{filename}.tmp")
    final_path = os.path.join(save_path, filename)
    
    try:
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(save_path, exist_ok=True)
        
        # ä¸‹è½½æ–‡ä»¶
        response = session.get(url, headers=headers, stream=True, timeout=30)
        response.raise_for_status()
        
        # è·å–æ–‡ä»¶å¤§å°
        total_size = int(response.headers.get('content-length', 0))
        
        # ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶ä¸‹è½½
        with open(temp_path, 'wb') as f:
            downloaded_size = 0
            for chunk in response.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)
                    downloaded_size += len(chunk)
        
        # éªŒè¯æ–‡ä»¶å¤§å°
        if total_size > 0 and downloaded_size != total_size:
            raise ValueError(f"æ–‡ä»¶å¤§å°ä¸åŒ¹é…: é¢„æœŸ {total_size} å­—èŠ‚ï¼Œå®é™…ä¸‹è½½ {downloaded_size} å­—èŠ‚")
        
        # éªŒè¯PDFæ–‡ä»¶
        with open(temp_path, 'rb') as f:
            header = f.read(5)
            if not header.startswith(b'%PDF-'):
                raise ValueError("æ–‡ä»¶ä¸æ˜¯æœ‰æ•ˆçš„PDFæ ¼å¼")
        
        # å¦‚æœéªŒè¯é€šè¿‡ï¼Œé‡å‘½åä¸´æ—¶æ–‡ä»¶
        if os.path.exists(final_path):
            os.remove(final_path)
        os.rename(temp_path, final_path)
        
        print(f"âœ… æˆåŠŸä¸‹è½½: {filename}")
        return True
        
    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥ {filename}: {str(e)}")
        if os.path.exists(temp_path):
            os.remove(temp_path)
        return False

def download_paper(result, save_path: str, filename: str) -> bool:
    """
    ä¸‹è½½è®ºæ–‡ï¼Œå…ˆå°è¯•ä½¿ç”¨arxiv APIï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨å¯é ä¸‹è½½æ–¹æ³•
    
    Args:
        result: arxivæœç´¢ç»“æœ
        save_path: ä¿å­˜è·¯å¾„
        filename: æ–‡ä»¶å
    
    Returns:
        bool: ä¸‹è½½æ˜¯å¦æˆåŠŸ
    """
    file_path = os.path.join(save_path, filename)
    
    # ç¬¬ä¸€æ­¥ï¼šå°è¯•ä½¿ç”¨arxiv APIä¸‹è½½
    try:
        print(f"å°è¯•ä½¿ç”¨arxiv APIä¸‹è½½: {filename}")
        result.download_pdf(dirpath=save_path, filename=filename)
        
        # éªŒè¯ä¸‹è½½çš„æ–‡ä»¶
        if verify_pdf(file_path):
            print(f"âœ… arxiv APIä¸‹è½½æˆåŠŸ: {filename}")
            return True
        else:
            print(f"âš ï¸ arxiv APIä¸‹è½½çš„æ–‡ä»¶æ— æ•ˆï¼Œå°è¯•ä½¿ç”¨å¯é ä¸‹è½½æ–¹æ³•...è¿™å¯èƒ½éœ€è¦ç¨å¾®é•¿ä¸€ç‚¹çš„æ—¶é—´ã€‚")
            if os.path.exists(file_path):
                os.remove(file_path)
    except Exception as e:
        print(f"âŒ arxiv APIä¸‹è½½å¤±è´¥: {str(e)}")
        if os.path.exists(file_path):
            os.remove(file_path)
    
    # ç¬¬äºŒæ­¥ï¼šä½¿ç”¨å¯é ä¸‹è½½æ–¹æ³•
    print(f"ä½¿ç”¨å¯é ä¸‹è½½æ–¹æ³•ä¸‹è½½: {filename}")
    return download_pdf_with_retry(
        url=result.entry_id.replace("abs", "pdf"),
        save_path=save_path,
        filename=filename
    )

def get_img_from_url(arxivid,img_src):
    time.sleep(1)
    session = requests.Session()
    retries = Retry(total=3, backoff_factor=1, status_forcelist=[502, 503, 504])
    adapter = HTTPAdapter(max_retries=retries)
    session.mount("http://", adapter)
    session.mount("https://", adapter)

    headers = {
        "User-Agent": "Mozilla/5.0 (compatible; AIgniteBot/1.0; +https://github.com/Algnite-Solutions/AIgnite)"
    }

    img_url = urljoin(f"https://arxiv.org/html/{str(arxivid)}/", img_src)
    #print(f"[INFO] Fetching image from: {img_url}")

    try:
        response = session.get(img_url, headers=headers, timeout=10)
        response.raise_for_status()
        img_data = response.content
        return img_data
    except requests.exceptions.RequestException as e:
        print(f"[ERROR] Failed to fetch image {img_url}: {e}")
        return None

def get_pdf_md(path,store_path,name,ak,sk):
    visual_service = VisualService()
    # call below method if you dont set ak and sk in $HOME/.volc/config
    visual_service.set_ak(ak)
    visual_service.set_sk(sk)

    params = dict()
    pdf_content = None

    # ä½¿ç”¨ with è¯­å¥ç¡®ä¿æ–‡ä»¶æ­£ç¡®å…³é—­
    with open(str(path), 'rb') as f:
        pdf_content = f.read()
        
    if os.path.getsize(path) > 7.5*1024*1024:
        print(f"ğŸ“¦ PDF è¶…è¿‡ 7.5MBï¼Œéœ€è¦å‹ç¼©ã€‚")
        try:
            compressed_path = compress_pdf(path)
            # compress_pdfåœ¨å¤±è´¥æ—¶ä¼šè¿”å›åŸæ–‡ä»¶è·¯å¾„ï¼Œæ£€æŸ¥æ˜¯å¦çœŸçš„å‹ç¼©äº†
            original_size = os.path.getsize(path)
            compressed_size = os.path.getsize(compressed_path) if os.path.exists(compressed_path) else original_size
            
            if compressed_size < original_size:
                print(f"âœ… PDFå‹ç¼©å®Œæˆï¼Œä½¿ç”¨å‹ç¼©åçš„æ–‡ä»¶ ({compressed_size/(1024*1024):.2f}MB)")
                path = compressed_path
            else:
                print(f"âš ï¸ PDFå‹ç¼©æœªç”Ÿæ•ˆï¼Œä½¿ç”¨åŸæ–‡ä»¶ ({original_size/(1024*1024):.2f}MB)")
            
            with open(str(path), 'rb') as f:
                pdf_content = f.read()
        except Exception as e:
            print(f"âš ï¸ å‹ç¼©è¿‡ç¨‹å‡ºé”™ï¼š{e}ï¼Œå°è¯•ä½¿ç”¨åŸæ–‡ä»¶")
            # å³ä½¿å‹ç¼©å¤±è´¥ï¼Œä¹Ÿå°è¯•ä½¿ç”¨åŸæ–‡ä»¶ç»§ç»­å¤„ç†
            try:
                with open(str(path), 'rb') as f:
                    pdf_content = f.read()
            except Exception as e2:
                print(f"âŒ æ— æ³•è¯»å–PDFæ–‡ä»¶ï¼š{e2}")
                return None
    
    form = {
        "image_base64": base64.b64encode(pdf_content).decode(),   # æ–‡ä»¶binary å›¾ç‰‡/PDF 
        "image_url": "",                  # url
        "version": "v3",                  # ç‰ˆæœ¬
        "page_start": 0,                  # èµ·å§‹é¡µæ•°
        "page_num": 16,                   # è§£æé¡µæ•°
        "table_mode": "html",             # è¡¨æ ¼è§£ææ¨¡å¼
        "filter_header": "true"           # è¿‡æ»¤é¡µçœ‰é¡µè„šæ°´å°
    }

    # è¯·æ±‚
    try:
        resp = visual_service.ocr_pdf(form)
        
        # å¢å¼ºé”™è¯¯å¤„ç†
        if resp is None:
            print("âŒ OCRè¯·æ±‚å¤±è´¥ï¼šæœåŠ¡è¿”å›ç©ºå“åº”")
            return None
            
        if not isinstance(resp, dict):
            print(f"âŒ OCRè¯·æ±‚å¤±è´¥ï¼šå“åº”ç±»å‹é”™è¯¯ï¼ŒæœŸæœ›dictï¼Œå®é™…{type(resp)}")
            return None
            
        if "data" not in resp:
            print(f"âŒ OCRè¯·æ±‚å¤±è´¥ï¼šå“åº”ä¸­ç¼ºå°‘'data'å­—æ®µï¼Œå®é™…å­—æ®µ: {list(resp.keys())}")
            return None
            
        if resp["data"] is None:
            print("âŒ OCRè¯·æ±‚å¤±è´¥ï¼šdataå­—æ®µä¸ºNone")
            return None
            
        markdown = resp["data"].get("markdown")
        if not markdown:
            print("âŒ OCRè¯·æ±‚å¤±è´¥ï¼šæœªè·å–åˆ°markdownå†…å®¹")
            print(f"å“åº”dataå†…å®¹: {resp['data']}")
            return None

        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(store_path, exist_ok=True)
        # å®Œæ•´æ–‡ä»¶è·¯å¾„
        file_path = os.path.join(store_path, f"{name}.md")

        # å†™å…¥æ–‡ä»¶
        with open(file_path, "w", encoding="utf-8") as f:
            f.write(markdown)

        return file_path

    except Exception as e:
        print(f"âŒ OCRè¯·æ±‚å¤±è´¥ï¼š{str(e)}")
        return None

def _parse_image_urls(content: str,arxiv_id) -> List[Tuple[str, str, str]]:
    """å†…éƒ¨å‡½æ•°ï¼šè§£æå›¾ç‰‡URLå¹¶æå–ç¬¦åˆè§„åˆ™çš„åç§°"""
    # æ­£åˆ™è§„åˆ™è¯´æ˜ï¼š
    # 1. åŒ¹é…![ä»»æ„å†…å®¹](URL) æ ¼å¼
    # 2. æå–åç§°ï¼šåœ¨`)`ä¹‹åï¼Œå¯»æ‰¾ä»¥F/få¼€å¤´ã€ç¬¬ä¸€ä¸ªæ•°å­—ç»“å°¾çš„å­—ç¬¦ä¸²
    pattern = r'!\[.*?\]\((https?://[^\)]+)\)'  # å…ˆæå–æ‰€æœ‰å›¾ç‰‡é“¾æ¥
    all_matches = re.findall(pattern, content)
    
    image_info = []
    for url in all_matches:
        # ä»URLå‰åçš„ä¸Šä¸‹æ–‡ä¸­æå–åç§°ï¼ˆå‡è®¾åç§°åœ¨`)`ä¹‹åï¼Œæ ¼å¼ä¸ºF/få¼€å¤´+æ•°å­—ç»“å°¾ï¼‰
        # ç¤ºä¾‹ï¼š`) Fig_1"` æˆ– `) fig3. `
        name = _extract_name_from_context(content, url)
        name = arxiv_id+'_'+name
        caption = _extract_caption_from_context(content, url)
        if name:
            image_info.append((name, url, caption))
    return image_info

def _extract_name_from_context(content: str, url: str) -> str:
    """ä»URLåçš„æ–‡æœ¬ä¸­æå–ä»¥Få¼€å¤´ã€æ•°å­—ç»“å°¾çš„åç§°ï¼Œå»é™¤ç‚¹å’Œç©ºæ ¼"""
    # å®šä½URLåœ¨æ–‡æœ¬ä¸­çš„ä½ç½®ï¼ˆæŸ¥æ‰¾URLåçš„å†…å®¹ï¼‰
    url_end = content.find(url) + len(url)
    post_url_content = content[url_end:].strip()  # è·å–URLä¹‹åçš„æ–‡æœ¬
    
    # æ­£åˆ™åŒ¹é…ï¼šä»¥Få¼€å¤´ï¼Œä»»æ„å­—ç¬¦ï¼ˆæ’é™¤ç‚¹å’Œç©ºæ ¼ï¼‰ï¼Œä»¥æ•°å­—ç»“å°¾
    pattern = r'(fig(?:ure)?\.?\s*\d+)'
    match = re.search(pattern, post_url_content, re.IGNORECASE)  # ä¸åŒºåˆ†å¤§å°å†™
    
    if match:
        # æå–åŒ¹é…å†…å®¹å¹¶å»é™¤ç‚¹å’Œç©ºæ ¼
        raw_name = match.group(0)
        cleaned_name = raw_name.replace('.', '').replace(' ', '')  # å»é™¤ç‚¹å’Œç©ºæ ¼
        
        # é¦–å­—æ¯å¤§å†™
        cleaned_name = cleaned_name[0].upper() + cleaned_name[1:]
        
        # å°†Figå¼€å¤´è½¬æ¢ä¸ºFigureå¼€å¤´
        if cleaned_name.startswith('Fig') and not cleaned_name.startswith('Figure'):
            cleaned_name = 'Figure' + cleaned_name[3:]
            
        return cleaned_name
    return ""

def _extract_caption_from_context(content: str, url: str) -> str:
    """ä» URL åæå–å›¾æ³¨æ–‡æœ¬ï¼Œç›´åˆ°ä¸‹ä¸€ä¸ªæ¢è¡Œç¬¦"""
    url_end = content.find(url) + len(url)
    post_url_content = content[url_end:].lstrip()

    # æå–ä» 'Fig' å¼€å¤´åˆ°æ¢è¡Œç¬¦ç»“æŸçš„ä¸€æ•´è¡Œä½œä¸º caption
    match = re.search(r'(fig(?:ure)?\.?\s*\d+[a-zA-Z]?\.*.*?)\n', post_url_content, re.IGNORECASE | re.DOTALL)
    if match:
        return match.group(1).strip()
    return ""

def _download_single_image(name: str, url: str, save_dir: str) -> bool:
    """å†…éƒ¨å‡½æ•°ï¼šä¸‹è½½å•å¼ å›¾ç‰‡"""
    try:
        response = requests.get(url, stream=True, timeout=15)
        response.raise_for_status()
        
        # å¤„ç†æ–‡ä»¶æ‰©å±•åï¼ˆæ”¯æŒURLä¸­å¸¦æŸ¥è¯¢å‚æ•°çš„æƒ…å†µï¼‰
        '''ext = url.split('.')[-1].split('?')[0].lower()
        if ext not in ['jpg', 'jpeg', 'png', 'gif', 'bmp', 'tiff']:
            ext = 'png'  # é»˜è®¤æ‰©å±•å'''
        
        ext = 'png'
        
        file_name = f"{name}.{ext}"
        save_path = os.path.join(save_dir, file_name)
        
        with open(save_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)
        
        print(f"âœ… æˆåŠŸä¸‹è½½ï¼š{name} -> {save_path}")
        return True
        
    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥ {url}ï¼š{str(e)}")
        return False