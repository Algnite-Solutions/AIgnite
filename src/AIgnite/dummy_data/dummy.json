{
    "doc_id": "2507.11233v1",
    "title": "Improving Neural Pitch Estimation with SWIPE Kernels",
    "authors": [
        "David Marttila",
        "Joshua D. Reiss"
    ],
    "categories": [
        "cs.SD",
        "eess.AS"
    ],
    "published_date": "2025-07-15 12:04:07+00:00",
    "abstract": "Neural networks have become the dominant technique for accurate pitch and\nperiodicity estimation. Although a lot of research has gone into improving\nnetwork architectures and training paradigms, most approaches operate directly\non the raw audio waveform or on general-purpose time-frequency representations.\nWe investigate the use of Sawtooth-Inspired Pitch Estimation (SWIPE) kernels as\nan audio frontend and find that these hand-crafted, task-specific features can\nmake neural pitch estimators more accurate, robust to noise, and more\nparameter-efficient. We evaluate supervised and self-supervised\nstate-of-the-art architectures on common datasets and show that the SWIPE audio\nfrontend allows for reducing the network size by an order of magnitude without\nperformance degradation. Additionally, we show that the SWIPE algorithm on its\nown is much more accurate than commonly reported, outperforming\nstate-of-the-art self-supervised neural pitch estimators.",
    "text_chunks": [
        {
            "id": "S1",
            "type": "text",
            "title": "1Introduction",
            "caption": "1Introduction",
            "metadata": {},
            "text": "\n1 Introduction\nPitch plays a central role in how humans perceive sound. Consequently, pitch estimation is a fundamental task in many music, speech and audio processing pipelines. While pitch is a psychoacoustic phenomenon, it closely correlates to the signal processing concept of the fundamental frequency f0subscript\ud835\udc530f_{0}italic_f start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT. Recent literature commonly uses the term \u201cpitch estimation\u201d to refer to the task of estimating an audio signal\u2019s f0subscript\ud835\udc530f_{0}italic_f start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT.\n\nGiven the importance of accurate pitch estimation, the topic has received a considerable amount of research attention over the past decades. Numerous digital signal processing (DSP) techniques estimate pitch based on the cepstrum [1], the power spectrum [2, 3, 4], or the autocorrelation function [5, 6].\n\nMore recently, deep neural networks have been applied to the task of pitch estimation [7, 8, 9, 10]. In a typical architecture, a convolutional neural network (CNN) is given overlapping frames of raw audio as input and trained to predict a probability distribution over a discrete set of f0subscript\ud835\udc530f_{0}italic_f start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT candidates in a supervised fashion. While these models can reach very high accuracy, they require a large amount of training data annotated with reliable ground truth pitch values and can struggle with out-of-domain generalization and robustness to noise and reverberation. Additionally, the CNNs usually consist of millions of parameters, making them less suitable for use in low-resource environments.\n\nThese drawbacks have been addressed in two different ways. Self-supervised training paradigms [11, 12, 13, 14] do not require labeled training data, and incorporating traditional DSP approaches into neural networks has been shown to increase efficiency and robustness [15, 16].\n\nIn this paper, we combine these two approaches and merge task-specific DSP-based features with both supervised and self-supervised training paradigms. Specifically, we substitute the audio frontend in the Pitch Estimation with Self-Supervised Transposition-Equivariant Objective (PESTO) architecture [14] for a representation obtained from the Sawtooth Waveform Inspired Pitch Estimator (SWIPE) [3], which estimates pitch by measuring the similarity of the input spectrum to that of sawtooth waves at various pitch candidates. We also investigate the use of SWIPE as a frontend for supervised neural pitch estimators, which usually operate directly on the audio waveform.\n\nThe core insights of our work are these:\n\n\n\u2022\nAlthough SWIPE is commonly used as a baseline for neural pitch estimation, we find that its performance has been significantly underreported. We show that SWIPE in its original form surpasses the accuracy of the state of the art in self-supervised neural pitch detection (PESTO).\n\n\u2022\nSWIPE is a well-suited audio frontend for neural pitch estimators in both supervised and self-supervised settings, and can improve the state-of-the-art in terms of accuracy, robustness, efficiency, and latency.\n\n\nTrained models alongside a SWIPE implementation in PyTorch [17] are available online.111https://github.com/dsuedholt/neural-pitch-swipe The remainder of this paper is structured as follows. In Section\u00a02, we give an overview over SWIPE and neural pitch estimation methods. Section\u00a03 covers some aspects of our SWIPE implementation choices and details how we embed SWIPE into neural pitch estimation architectures. Section\u00a04 describes how we evaluate our approach, and Section\u00a05 presents the results of our evaluation.\n"
        },
        {
            "id": "S2",
            "type": "text",
            "title": "2Background",
            "caption": "2Background",
            "metadata": {},
            "text": "\n2 Background\n\n2.1 SWIPE\n\nThe Sawtooth Waveform Inspired Pitch Estimator (SWIPE) [3] estimates pitch by identifying the fundamental frequency of a sawtooth waveform whose spectrum best matches that of the input signal. To achieve this, it constructs spectral kernels for a number of discrete pitch candidates, and assigns a score to each pitch candidate by measuring the similarity between its associated kernel and the spectrum of the input signal.\n\n2.1.1 Score Calculation\nMore formally, consider a (windowed) audio signal x\u2062[n]\ud835\udc65delimited-[]\ud835\udc5bx[n]italic_x [ italic_n ] of length N\ud835\udc41Nitalic_N and its discrete Fourier Transform (DFT) X\u2062[k]\ud835\udc4bdelimited-[]\ud835\udc58X[k]italic_X [ italic_k ], which may be truncated to the K=\u230aN/2\u230b+1\ud835\udc3e\ud835\udc4121K=\\lfloor N/2\\rfloor+1italic_K = \u230a italic_N / 2 \u230b + 1 bins corresponding to non-negative frequencies if x\ud835\udc65xitalic_x is real-valued. Let now C={f1,f2,\u2026\u2062f|C|}\ud835\udc36subscript\ud835\udc531subscript\ud835\udc532\u2026subscript\ud835\udc53\ud835\udc36C=\\{f_{1},f_{2},\\ldots f_{|C|}\\}italic_C = { italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_f start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , \u2026 italic_f start_POSTSUBSCRIPT | italic_C | end_POSTSUBSCRIPT } be a set of |C|\ud835\udc36|C|| italic_C | pitch candidates. Then Sc\u2062[k]subscript\ud835\udc46\ud835\udc50delimited-[]\ud835\udc58S_{c}[k]italic_S start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT [ italic_k ] is the spectral kernel associated with the pitch candidate fcsubscript\ud835\udc53\ud835\udc50f_{c}italic_f start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT, and we can compute its SWIPE score Z\u2062(fc):C\u2192[\u22121,1]:\ud835\udc4dsubscript\ud835\udc53\ud835\udc50\u2192\ud835\udc3611Z(f_{c}):C\\rightarrow\\mathbb{[}-1,1]italic_Z ( italic_f start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ) : italic_C \u2192 [ - 1 , 1 ] as the normalized inner product between Scsubscript\ud835\udc46\ud835\udc50S_{c}italic_S start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT and X\ud835\udc4bXitalic_X:\n\n\nZ\u2062(fc)=\u2211k=0K\u22121Sc\u2062[k]\u22c5|X\u2062[k]|1/2(\u2211k=0K\u22121|X\u2062[k]|)1/2\ud835\udc4dsubscript\ud835\udc53\ud835\udc50superscriptsubscript\ud835\udc580\ud835\udc3e1\u22c5subscript\ud835\udc46\ud835\udc50delimited-[]\ud835\udc58superscript\ud835\udc4bdelimited-[]\ud835\udc5812superscriptsuperscriptsubscript\ud835\udc580\ud835\udc3e1\ud835\udc4bdelimited-[]\ud835\udc5812Z(f_{c})=\\frac{\\sum_{k=0}^{K-1}S_{c}[k]\\cdot\\left|X[k]\\right|^{1/2}}{\\left(%\n\\sum_{k=0}^{K-1}|X[k]|\\right)^{1/2}}italic_Z ( italic_f start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ) = divide start_ARG \u2211 start_POSTSUBSCRIPT italic_k = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K - 1 end_POSTSUPERSCRIPT italic_S start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT [ italic_k ] \u22c5 | italic_X [ italic_k ] | start_POSTSUPERSCRIPT 1 / 2 end_POSTSUPERSCRIPT end_ARG start_ARG ( \u2211 start_POSTSUBSCRIPT italic_k = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K - 1 end_POSTSUPERSCRIPT | italic_X [ italic_k ] | ) start_POSTSUPERSCRIPT 1 / 2 end_POSTSUPERSCRIPT end_ARG\n(1)\n\n\nThe pitch estimate is then given by the fcsubscript\ud835\udc53\ud835\udc50f_{c}italic_f start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT that maximizes Z\u2062(fc)\ud835\udc4dsubscript\ud835\udc53\ud835\udc50Z(f_{c})italic_Z ( italic_f start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ) and may optionally be further refined by e.g. parabolic interpolation of the local maximum.\n\nWhile Eqn\u00a0(1) computes the inner product over all bins of the DFT for notational simplicity, the original SWIPE paper suggests resampling the spectrum to the Equivalent Rectangular Bandwidth (ERB) [18] scale for speech data, or to the mel scale for musical instruments.\n\n\n2.1.2 Kernel Design\nThe kernel Scsubscript\ud835\udc46\ud835\udc50S_{c}italic_S start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT is designed to maximize the inner product with X\ud835\udc4bXitalic_X if x\ud835\udc65xitalic_x is a signal with fundamental frequency f0=fcsubscript\ud835\udc530subscript\ud835\udc53\ud835\udc50f_{0}=f_{c}italic_f start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = italic_f start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT. To achieve that, it contains cosine lobes of width fc/2subscript\ud835\udc53\ud835\udc502f_{c}/2italic_f start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT / 2 at all integer harmonics of fcsubscript\ud835\udc53\ud835\udc50f_{c}italic_f start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT, decaying in magnitude to mimic the spectrum of a sawtooth wave. As the authors of SWIPE lay out, this corresponds precisely to the square root of the main lobes of a Hann-windowed sawtooth wave if the size of the analysis window is exactly T=8/fc\ud835\udc478subscript\ud835\udc53\ud835\udc50T=8/f_{c}italic_T = 8 / italic_f start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT. The kernel further contains negative-valued valleys at 12\u2062fc,32\u2062fc,\u202612subscript\ud835\udc53\ud835\udc5032subscript\ud835\udc53\ud835\udc50\u2026\\frac{1}{2}f_{c},\\frac{3}{2}f_{c},\\ldotsdivide start_ARG 1 end_ARG start_ARG 2 end_ARG italic_f start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT , divide start_ARG 3 end_ARG start_ARG 2 end_ARG italic_f start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT , \u2026, i.e.\u00a0at the midpoint between each harmonic peak.\n\nSince all harmonics of a signal with a true fundamental frequency of f\ud835\udc53fitalic_f also contribute to the scores of the pitch candidates at f/2,f/3,\u2026\ud835\udc532\ud835\udc533\u2026f/2,f/3,\\ldotsitalic_f / 2 , italic_f / 3 , \u2026, a common variant of the SWIPE algorithm removes the non-prime harmonics (except for the first one) of all kernels to reduce the problem of octave errors. This is known as SWIPE\u2019, but effective and widespread enough that it is often simply referred to as SWIPE, for example in the Speech Processing Toolkit (SPTK) [19] implementation, which is based directly on the MATLAB code published along with SWIPE. We take the same approach in this paper and will generally assume that the scores Z\ud835\udc4dZitalic_Z are calculated using SWIPE\u2019 kernels. An example of such a kernel is illustrated in Figure\u00a01.\n\n\n2.2 Supervised Neural Pitch Estimation\nThe established way of using neural networks for pitch estimation is to interpret an audio signal x\u2062[n]\ud835\udc65delimited-[]\ud835\udc5bx[n]italic_x [ italic_n ] as a vector \ud835\udc31\ud835\udc31\\mathbf{x}bold_x. A network f\u03b8subscript\ud835\udc53\ud835\udf03f_{\\theta}italic_f start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT then maps \ud835\udc31\ud835\udc31\\mathbf{x}bold_x to a vector \ud835\udc32\u2208[0,1]|C|\ud835\udc32superscript01\ud835\udc36\\mathbf{y}\\in[0,1]^{|C|}bold_y \u2208 [ 0 , 1 ] start_POSTSUPERSCRIPT | italic_C | end_POSTSUPERSCRIPT, where each entry ycsubscript\ud835\udc66\ud835\udc50y_{c}italic_y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT represents the probability that a corresponding pitch candidate fcsubscript\ud835\udc53\ud835\udc50f_{c}italic_f start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT is the pitch of \ud835\udc31\ud835\udc31\\mathbf{x}bold_x. In supervised training, this is treated as a multi-class classification problem, calculating the loss using the cross-entropy to the ground truth pitch. The ground truth distribution may be smoothed using Gaussian blurring to aid training [7]. Voicing confidence can be deduced from the entropy of the predicted probability distribution [9].\n\n\n2.3 Self-Supervised Neural Pitch Estimation\nPitch Estimation with Self-Supervised Transposition-Equivariant Objective (PESTO) [14] is a state-of-the-art architecture for self-supervised training of neural network pitch estimators, where natural symmetries of the input are exploited to learn a translation-equivariant representation, instead of providing ground truth pitches to the model.\n\n2.3.1 Training Setup\nDuring training, the model learns to optimize a combination of three losses:\n\nAn equivariance loss enforces that pitch-shifted versions of an input should result in output distributions that are transpositions of the original input. Given an input \ud835\udc31\ud835\udc31\\mathbf{x}bold_x and its pitch-shifted version \ud835\udc31(k)superscript\ud835\udc31\ud835\udc58\\mathbf{x}^{(k)}bold_x start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT (shifted by k\ud835\udc58kitalic_k semitones), their respective outputs \ud835\udc32\ud835\udc32\\mathbf{y}bold_y and \ud835\udc32(k)superscript\ud835\udc32\ud835\udc58\\mathbf{y}^{(k)}bold_y start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT should satisfy \u03d5\u2062(\ud835\udc32(k))=\u03b1k\u2062\u03d5\u2062(\ud835\udc32)italic-\u03d5superscript\ud835\udc32\ud835\udc58superscript\ud835\udefc\ud835\udc58italic-\u03d5\ud835\udc32\\phi(\\mathbf{y}^{(k)})=\\alpha^{k}\\phi(\\mathbf{y})italic_\u03d5 ( bold_y start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ) = italic_\u03b1 start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT italic_\u03d5 ( bold_y ), where \u03d5italic-\u03d5\\phiitalic_\u03d5 is a deterministic linear mapping:\n\n\u03d5:\u211d|C|\u2192\u211d\ud835\udc32\u21a6(\u03b1,\u03b12,\u2026,\u03b1|C|)\u2062\ud835\udc32italic-\u03d5:superscript\u211d\ud835\udc36\u2192\u211dmissing-subexpressionmissing-subexpression\ud835\udc32maps-to\ud835\udefcsuperscript\ud835\udefc2\u2026superscript\ud835\udefc\ud835\udc36\ud835\udc32\\begin{array}[]{ccccc}\\phi&:&\\mathbb{R}^{|C|}&\\rightarrow&\\mathbb{R}\\\\\n&&\\mathbf{y}&\\mapsto&(\\alpha,\\alpha^{2},\\ldots,\\alpha^{|C|})\\mathbf{y}\\end{array}start_ARRAY start_ROW start_CELL italic_\u03d5 end_CELL start_CELL : end_CELL start_CELL blackboard_R start_POSTSUPERSCRIPT | italic_C | end_POSTSUPERSCRIPT end_CELL start_CELL \u2192 end_CELL start_CELL blackboard_R end_CELL end_ROW start_ROW start_CELL end_CELL start_CELL end_CELL start_CELL bold_y end_CELL start_CELL \u21a6 end_CELL start_CELL ( italic_\u03b1 , italic_\u03b1 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT , \u2026 , italic_\u03b1 start_POSTSUPERSCRIPT | italic_C | end_POSTSUPERSCRIPT ) bold_y end_CELL end_ROW end_ARRAY\n(2)\n\nand \u03b1\ud835\udefc\\alphaitalic_\u03b1 is a hyperparameter.\n\nA regularization loss further ensures that the network\u2019s outputs for pitch-shifted inputs maintain the expected transposition relationship. For a pair of outputs \ud835\udc32\ud835\udc32\\mathbf{y}bold_y and \ud835\udc32(k)superscript\ud835\udc32\ud835\udc58\\mathbf{y}^{(k)}bold_y start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT, the shifted cross-entropy loss\n\n\u2112SCE\u2062(\ud835\udc32,\ud835\udc32(k),k)=\u2211i=0|C|\u22121yi\u2062log\u2061(yi+k(k))subscript\u2112SCE\ud835\udc32superscript\ud835\udc32\ud835\udc58\ud835\udc58superscriptsubscript\ud835\udc560\ud835\udc361subscript\ud835\udc66\ud835\udc56superscriptsubscript\ud835\udc66\ud835\udc56\ud835\udc58\ud835\udc58\\mathcal{L}_{\\text{SCE}}(\\mathbf{y},\\mathbf{y}^{(k)},k)=\\sum_{i=0}^{|C|-1}y_{i%\n}\\log\\left(y_{i+k}^{(k)}\\right)caligraphic_L start_POSTSUBSCRIPT SCE end_POSTSUBSCRIPT ( bold_y , bold_y start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT , italic_k ) = \u2211 start_POSTSUBSCRIPT italic_i = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT | italic_C | - 1 end_POSTSUPERSCRIPT italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT roman_log ( italic_y start_POSTSUBSCRIPT italic_i + italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT )\n(3)\n\nmeasures how well \ud835\udc32(k)superscript\ud835\udc32\ud835\udc58\\mathbf{y}^{(k)}bold_y start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT matches the k\ud835\udc58kitalic_k-semitone shift of \ud835\udc32\ud835\udc32\\mathbf{y}bold_y.\n\nFinally, an invariance loss encourages the mapping f\u03b8subscript\ud835\udc53\ud835\udf03f_{\\theta}italic_f start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT to be invariant to the timbre of the signal. During training, PESTO draws random transforms t\ud835\udc61titalic_t from a set of pitch-preserving data augmentations \ud835\udcaf\ud835\udcaf\\mathcal{T}caligraphic_T. Given \ud835\udc31~=t\u2062(\ud835\udc31)~\ud835\udc31\ud835\udc61\ud835\udc31\\tilde{\\mathbf{x}}=t(\\mathbf{x})over~ start_ARG bold_x end_ARG = italic_t ( bold_x ), the invariance loss is then expressed as the cross-entropy between \ud835\udc32=f\u03b8\u2062(\ud835\udc31)\ud835\udc32subscript\ud835\udc53\ud835\udf03\ud835\udc31\\mathbf{y}=f_{\\theta}(\\mathbf{x})bold_y = italic_f start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( bold_x ) and \ud835\udc32~=f\u03b8\u2062(\ud835\udc31~)~\ud835\udc32subscript\ud835\udc53\ud835\udf03~\ud835\udc31\\tilde{\\mathbf{y}}=f_{\\theta}({\\mathbf{\\tilde{x}}})over~ start_ARG bold_y end_ARG = italic_f start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( over~ start_ARG bold_x end_ARG ).\n\n\n2.3.2 Model Architecture\nThe PESTO architecture uses the constant-Q transform (CQT) of an audio frame as its input, where the bins of the CQT exactly correspond to the pitch candidates. The transforms \ud835\udcaf\ud835\udcaf\\mathcal{T}caligraphic_T take the form of adding random noise and gain to the CQT frames. A CNN processes the frame, and its flattened output is fed to a final linear layer followed by a softmax layer which produces a probability distribution. Importantly, the final linear layer uses a Toeplitz matrix as its weight matrix to preserve the transposition equivariance of the CNN.\n\n"
        },
        {
            "id": "S3",
            "type": "text",
            "title": "3Methods",
            "caption": "3Methods",
            "metadata": {},
            "text": "\n3 Methods\nThe core insight of this work is that SWIPE scores encode rich pitch information and are thus well suited as an audio frontend for neural pitch estimation in both supervised and self-supervised settings. This section first covers our implementation of SWIPE in detail, and then describes how we adapt supervised and self-supervised neural pitch estimators to work with SWIPE scores.\n\n3.1 SWIPE Implementation\n\nWe calculate the SWIPE scores by sampling the spectrum at 1024 frequencies, which are linearly spaced on the mel scale over a range from 0.25\u22c5fmin\u22c50.25subscript\ud835\udc530.25\\cdot f_{\\min}0.25 \u22c5 italic_f start_POSTSUBSCRIPT roman_min end_POSTSUBSCRIPT to 1.25\u22c5fmax\u22c51.25subscript\ud835\udc531.25\\cdot f_{\\max}1.25 \u22c5 italic_f start_POSTSUBSCRIPT roman_max end_POSTSUBSCRIPT. We used the Slaney-style mel scale, which is linear up to 1 kHz and logarithmic above, as implemented in the librosa toolkit [20]. For each of the various window sizes, the spectrum is calculated with the same FFT resolution (zero-padding the input as needed) and evaluated at the sampling frequencies using linear interpolation. We arrange the pitch candidates to match the CQT resolution used in PESTO: logarithmically spaced over a range of fmin=27.5\u2062\u00a0Hzsubscript\ud835\udc5327.5\u00a0Hzf_{\\min}=27.5\\text{ Hz}italic_f start_POSTSUBSCRIPT roman_min end_POSTSUBSCRIPT = 27.5 Hz to fmax=8055\u2062\u00a0Hzsubscript\ud835\udc538055\u00a0Hzf_{\\max}=8055\\text{ Hz}italic_f start_POSTSUBSCRIPT roman_max end_POSTSUBSCRIPT = 8055 Hz, using a resolution of 3 bins per semitone, for a total of 295 bins.\n\nAlthough many papers on neural pitch estimation compare their work to SWIPE as a baseline, they generally do not cite the implementation they used or report the parameters they chose. To make sure that our implementation does not significantly underperform, we compare it to the most popular open-source implementation of SWIPE, which is contained in the Speech Processing Toolkit (SPTK) [19]. It uses 8 pitch bins per semitone by default, samples the input frequency spectrum according to the ERB [18] scale, and refines the estimate using parabolic interpolation.\n\nTable\u00a01 contains the Raw Pitch Accuracy (RPA) achieved by the SPTK and our implementation on the MDB-stem-synth and MIR-1K datasets and compares it to previously reported baseline values. The metrics and datasets are described in more detail in Section\u00a04. The accuracy of the SPTK implementation seems to significantly deteriorate for large search ranges. We report the values for upper limits of 2\u2062\u00a0kHz2\u00a0kHz2\\text{ kHz}2 kHz and 8\u2062\u00a0kHz8\u00a0kHz8\\text{ kHz}8 kHz, where the lower limit is 30\u2062\u00a0Hz30\u00a0Hz30\\text{ Hz}30 Hz for both. We set the score threshold which pitch candidates need to exceed to be considered to 0.\n\nOur implementation appears to be a lot more robust to its large search range (27.5\u20138055 Hz). Switching from ERB to mel sampling results in a notable accuracy gain on MDB-stem-synth, which contains more varied timbres. This matches the results of the original SWIPE paper.\n\nBoth the SPTK and our own implementation can perform much more accurately than the values that were previously reported as baselines in the neural pitch estimation literature suggest, to the extent that SWIPE outperforms even state-of-the-art self-supervised pitch detection models (see Section\u00a05.2).\n\n\n3.2 Supervised Neural Pitch Estimation\nWe experiment with using both SWIPE scores and the CQT as an audio frontend in a supervised training context.\n\nWe feed the input into a CNN with 6 1D-convolutional layers, applying layer normalization and a leaky ReLU non-linearity with slope 0.3 between each layer. Zero-padding is applied to the input in each layer to preserve the input dimension. After flattening, the output of the final layer is reduced to the dimensionality of the pitch bins and fed into a Softmax layer to obtain a probability distribution. We find that using a dense linear layer to perform the dimensionality reduction strongly degraded generalization in this setup, and instead also employ a Toeplitz layer in the supervised model.\n\n\n3.3 Self-Supervised Neural Pitch Estimation\nThe original PESTO architecture is already well suited to work with SWIPE scores, which can be directly substituted for the CQT bins without violating the assumptions on translation equivariance. Since SWIPE scores encode periodicity information much more explicitly than CQT frames, we expect the encoder network to achieve similar performance with fewer parameters. We test this hypothesis by training a PESTO-style encoder with a drastically reduced parameter count, consisting only of the final Toeplitz fully-connected layer \u2013 a convolutional layer with a single filter of size 647 \u2013 and softmax normalization. In this very simple architecture, the Toeplitz layer can be seen as essentially learning a reweighting of the SWIPE scores, and to refine the location of the peak score if the output resolution is larger than the input resolution.\n\nInitial experiments indicated that applying random data augmentation to the SWIPE scores only resulted in degraded performance compared to the baseline DSP algorithm. We additionally augment the audio frame in the time domain by adding random noise and applying a finite impulse response (FIR) filter with a randomized amplitude response. This results in an increased computational cost for training, since the SWIPE scores need to be recalculated at every training step, but does not affect the computational cost of inference once training has finished.\n\n"
        },
        {
            "id": "S4",
            "type": "text",
            "title": "4Experimental Setup",
            "caption": "4Experimental Setup",
            "metadata": {},
            "text": "\n4 Experimental Setup\n\n4.1 Datasets\nOur experiments use three f0subscript\ud835\udc530f_{0}italic_f start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT-annotated datasets that are commonly used for training and benchmarking pitch detectors:\n\nMDB-stem-synth [21] contains 230 solo tracks (418 minutes total) of instrument sounds and vocals. The audio is re-synthesized from its f0subscript\ud835\udc530f_{0}italic_f start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT annotations, which means that the f0subscript\ud835\udc530f_{0}italic_f start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT annotations are perfect. It is annotated with a hop size of 2.9 ms.\n\nPTDB-TUG [22] contains 4720 audio and laryngograph recordings (576 minutes total) of 20 English speakers reading sentences. It is annotated with a hop size of 10 ms.\n\nMIR-1K [23] contains 1000 short recordings (133 minutes total) of Chinese karaoke performances. It is annotated with a hop size of 20 ms.\n\n\n4.2 Baselines\nWe compare all results to two DSP-based pitch detection baselines: PYIN [24] and SWIPE. We do not perform Viterbi decoding or any sort of peak refinement, simply selecting the pitch candidate with the highest score. PYIN scores are based on autocorrelation and so their natural resolution is expressed in integer samples. We resample the scores to the same pitch candidate resolution as used for SWIPE using linear interpolation.\n\nAs a baseline for supervised training, we choose FCNF0++ [9], which to the best of our knowledge is the currently best-performing supervised monophonic neural pitch detector that operates on a frame-by-frame basis, rather than processing the entire audio signal at once.\n\nIn the self-supervised setting, we compare our results against the original PESTO architecture, which is the current state of the art in self-supervised monophonic neural pitch estimation.\n\n\n4.3 Evaluation Metrics\nWe use the mir_eval package [25] to report the following metrics:\n\nRaw Pitch Accuracy (RPA), the percentage of voiced frames for which the model predicted a pitch within 50 cents of the ground truth.\n\nF-Score, measuring the accuracy of the binary voiced/unvoiced decision.\n\nOverall Accuracy (OA), the percentage of all frames (voiced and unvoiced) for which a correct voicing decision was made, and for which the model predicted a pitch within 50 cents of the ground truth if the frame is voiced.\n\n"
        },
        {
            "id": "S5",
            "type": "text",
            "title": "5Results and Discussion",
            "caption": "5Results and Discussion",
            "metadata": {},
            "text": "\n5 Results and Discussion\nWe report separate experimental results for the supervised and self-supervised approaches, in each case closely replicating the training setup of the baselines (FCNF0++ and PESTO, respectively) to assess the impact of using SWIPE scores as an audio frontend.\n\n5.1 Supervised Models\nWe refer to the two proposed supervised models (see Section\u00a03.2) as CQT-sup and SWIPE-sup. We train our models as well as the FCNF0++ baseline on MDB-stem-synth and PTDB-TUG at the same time. While networks that take CQT or SWIPE scores as input are sample-rate agnostic, FCFN0++ operates on the raw audio waveform and was designed to work with a sampling rate of 8\u2062\u00a0kHz8\u00a0kHz8\\text{ kHz}8 kHz, so we resample its input accordingly.\n\nIn the interest of a direct comparison, we use the 70-15-15 split into training, validation and testing partitions that was published in [9]. The performance of the trained models is measured by calculating RPA, F-Score and OA on the testing set. To better measure generalization performance on unseen data, we additionally evaluate the trained models on the full MIR-1K dataset, which is not used in training.\n\nWe train the models for 500,000 steps, using a batch size of 256 and the Adam optimizer [26] with an initial learning rate of 0.0002. Table\u00a02 shows the results of the evaluation. Overall, both CQT-sup and SWIPE-sup seem competitive with FCNF0++, but not clearly superior. They are able to almost match the in-domain RPA of FCNF0++, and outperform it in terms of voiced/unvoiced accuracy and generalization. The two proposed models use fewer trainable parameters than FCNF0++, but require a larger context window.\n\nThe CQT input features seem to be particularly well suited for making voiced/unvoiced decisions, with the CQT-sup model attaining the highest F-Score on both the test set and on MIR-1K.\n\nAll three models struggle with generalization, staying well behind the DSP baselines on MIR-1K. The best generalization behavior is shown by SWIPE-sup, even though it was the least accurate model on the test set.\n\n\n5.2 Self-Supervised Models\nWe refer to the three modified PESTO models (see Section\u00a03.3) as CQT-tiny, SWIPE-full, and SWIPE-tiny, where \u201ctiny\u201d refers to the Toeplitz-only encoder and \u201cfull\u201d to the original PESTO encoder architecture with a multi-layer CNN. We train the models on the whole of MIR-1K and measure their performance on MDB-stem-synth, and vice versa. The models are trained for 50 epochs using a batch size of 256 and the Adam optimizer with an initial learning rate of 0.0001.\n\nThe results of the evaluation are given in Table\u00a03. The baseline SWIPE implementation outperforms PESTO on both datasets, regardless of which dataset the model was trained on. This means that the original SWIPE algorithm outperforms all work on self-supervised monophonic pitch detection published to date.\n\nWhen using CQT frames as input, reducing the encoder network to just the final Toeplitz layer noticeably degrades performance, especially in the across-dataset evaluation. However, it is worth noting that CQT-tiny still achieves relatively good same-dataset accuracy. Since no explicit pitch information is given to the model during training, this is a strong indicator for the usefulness of the transposition-equivariant training structure that PESTO introduced.\n\nThe highest accuracy on the same-dataset evaluation is achieved by the two models that use SWIPE scores as input. Like CQT-tiny however, their performance plummets when trained on MIR-1K and evaluated on MDB-stem-synth. MDB-stem-synth covers a larger pitch range than MIR-1K and contains more varied timbres, making generalization challenging. The original PESTO is the only model that is able to make this jump reasonably well.\n\nIn the reverse direction however, SWIPE-tiny achieves the highest RPA out of the four models when trained on MDB-stem-synth and evaluated on MIR-1K, as well as the best same-dataset RPA for MDB-stem-synth. Adding the Toeplitz layer on top of the SWIPE scores improves their performance, but the additional network layers in SWIPE-full do not bring further accuracy gains, seemingly hindering performance instead.\n\nThe Toeplitz-only encoder in SWIPE-tiny seems to learn to refine the peaks of the SWIPE scores, mitigating errors caused by the quantization of the search space or by input spectra that deviate too far from the harmonic ideal. Figure\u00a02 illustrates a frame where an estimation error of 90 cents is reduced to 30 cents after feeding the SWIPE scores through the SWIPE-tiny encoder.\n\n\n5.3 Latency-Accuracy Tradeoff for SWIPE\nThe frame-based structure of the evaluated models, and especially the lightweight architecture of the self-supervised estimators, lend themselves naturally to use in real-time, streaming applications. In this context, small window sizes are desirable to reduce latency.\n\nAs described in Section\u00a02.1, the theoretical ideal window size for each pitch candidate fcsubscript\ud835\udc53\ud835\udc50f_{c}italic_f start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT is exactly 8/fc8subscript\ud835\udc53\ud835\udc508/f_{c}8 / italic_f start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT. In practice however, it is sufficient to only consider window sizes whose length in samples is a power of two. For a given pitch candidate with an ideal window length W\ud835\udc4aWitalic_W, the score is then calculated twice at window lengths 2\u230alog2\u2061(W)\u230bsuperscript2subscript2\ud835\udc4a2^{\\lfloor\\log_{2}(W)\\rfloor}2 start_POSTSUPERSCRIPT \u230a roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_W ) \u230b end_POSTSUPERSCRIPT and 2\u2308log2\u2061(W)\u2309superscript2subscript2\ud835\udc4a2^{\\lceil\\log_{2}(W)\\rceil}2 start_POSTSUPERSCRIPT \u2308 roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_W ) \u2309 end_POSTSUPERSCRIPT, and linearly interpolated to obtain an approximation to the score at the ideal size. Given a sampling rate of fs=44.1\u2062\u00a0kHzsubscript\ud835\udc53\ud835\udc6044.1\u00a0kHzf_{s}=44.1\\text{ kHz}italic_f start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT = 44.1 kHz and a minimum pitch candidate of fmin=27.5\u2062\u00a0Hzsubscript\ud835\udc5327.5\u00a0Hzf_{\\min}=27.5\\text{ Hz}italic_f start_POSTSUBSCRIPT roman_min end_POSTSUBSCRIPT = 27.5 Hz, the next-longest window with a power-of-two length in samples corresponds to 327 ms, which is already a significant improvement compared to the 1871 ms required by the CQT.\n\nHowever, this can be reduced further. The SWIPE-based models offer a straightforward way to reduce both latency and computational cost at the expense of accuracy by simply calculating the scores for lower pitch candidates at shorter window sizes (without interpolation). Crucially, this adjustment can be made flexibly at inference time without retraining the model. Table\u00a04 shows the effect that reducing the window size has on the RPA for two selected models. Note that reducing the window size to a length that is not a power of two is also possible if finer control over the tradeoff is desired.\n\n\n5.4 Robustness to Noise\nIn the final experiment, we investigate how robust various trained models are to noisy conditions by adding white noise to the input audio at decreasing signal-to-noise ratios. The results are shown in Table\u00a05. SWIPE-tiny appears to be fairly robust to background noise, especially compared to the base SWIPE algorithm. The performance of the supervised models degrades somewhat quicker than that of the self-supervised ones. This is not too surprising, since invariance to added noise is an explicit training objective for the self-supervised models (see Section\u00a03.3).\n\n"
        },
        {
            "id": "S6",
            "type": "text",
            "title": "6Conclusion",
            "caption": "6Conclusion",
            "metadata": {},
            "text": "\n6 Conclusion\nWe investigated the potential of combining the SWIPE algorithm with neural pitch estimation. We adapted established supervised and self-supervised training techniques to use SWIPE scores as an audio frontend and obtained accurate, efficient, robust and flexible pitch estimators.\n\nWe demonstrated that the potential of SWIPE has been significantly underestimated in the literature despite being commonly used as a baseline. The algorithm in its original form outperforms state-of-the-art self-supervised neural pitch estimators.\n\nIn the future, we plan to explore whether the performance of the pitch estimators can be further improved by hybrid training schemes that make simultaneous use of labeled data and self-supervised training objectives. We would also like to investigate whether certain parameters of SWIPE, such as the frequency envelope or the weights of individual harmonics, can be directly learned from data.\n\n"
        },
        {
            "id": "S7",
            "type": "text",
            "title": "7Acknowledgments",
            "caption": "7Acknowledgments",
            "metadata": {},
            "text": "\n7 Acknowledgments\nThis work was supported by UK Research and Innovation [grant number EP/S022694/1]. The authors would like to thank the anonymous reviewers for their valuable feedback which significantly improved this paper.\n"
        }
    ],
    "figure_chunks": [
        {
            "id": "S2.F1",
            "type": "figure",
            "title": "2507.11233v1_Figure1",
            "caption": "Figure 1:SWIPE and SWIPE\u2019 kernels corresponding to a pitch candidate at 330 Hz. The SWIPE kernel contains peaks at all integer harmonics of the candidate frequency. The SWIPE\u2019 kernel is obtained by removing the peaks at non-prime harmonics.",
            "metadata": {},
            "image_path": "/data3/guofang/peirongcan/PaperIgnition/orchestrator/imgs/2507.11233v1_Figure1.png",
            "alt_text": "SWIPE kernel illustration"
        },
        {
            "id": "S5.F2",
            "type": "figure",
            "title": "2507.11233v1_Figure2",
            "caption": "Figure 2:Top:The spectrum of a frame of audio from MIR-1K. The solid vertical line marks the ground truth pitch.Bottom:The SWIPE scores for the frame, before (dotted) and after (dashed) they were transformed by the SWIPE-tiny encoder trained on MDB-stem-synth. The vertical lines indicate the pitch estimate obtained from the basic SWIPE algorithm (dotted), the estimate given by SWIPE-tiny (dashed), and the ground truth (solid).",
            "metadata": {},
            "image_path": "/data3/guofang/peirongcan/PaperIgnition/orchestrator/imgs/2507.11233v1_Figure2.png",
            "alt_text": "Refer to caption"
        }
    ],
    "table_chunks": [
        {
            "id": "S3.T1",
            "type": "table",
            "title": "2507.11233v1_Table1",
            "caption": "Table 1:Raw Pitch Accuracy obtained by SWIPE implementations on the MIR-1K and MDB-stem-synth datasets, compared to baseline values previously reported in self-supervised pitch estimation papers. For SPTK, we report different upper limits for the search range. For our implementation, the search range is constant, but the frequency sampling scale changes.",
            "metadata": {},
            "table_html": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S3.T1.2\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S3.T1.2.2\">\n<th class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r\" id=\"S3.T1.2.2.3\"></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S3.T1.2.2.4\">Reported in <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2507.11233v1#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2507.11233v1#bib.bib13\" title=\"\">13</a>]</cite>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S3.T1.1.1.1\">SPTK (<math alttext=\"2\\text{ kHz}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.1.1.1.m1.1\"><semantics id=\"S3.T1.1.1.1.m1.1a\"><mrow id=\"S3.T1.1.1.1.m1.1.1\" xref=\"S3.T1.1.1.1.m1.1.1.cmml\"><mn id=\"S3.T1.1.1.1.m1.1.1.2\" xref=\"S3.T1.1.1.1.m1.1.1.2.cmml\">2</mn><mo id=\"S3.T1.1.1.1.m1.1.1.1\" xref=\"S3.T1.1.1.1.m1.1.1.1.cmml\">\u2062</mo><mtext id=\"S3.T1.1.1.1.m1.1.1.3\" xref=\"S3.T1.1.1.1.m1.1.1.3a.cmml\">\u00a0kHz</mtext></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.T1.1.1.1.m1.1b\"><apply id=\"S3.T1.1.1.1.m1.1.1.cmml\" xref=\"S3.T1.1.1.1.m1.1.1\"><times id=\"S3.T1.1.1.1.m1.1.1.1.cmml\" xref=\"S3.T1.1.1.1.m1.1.1.1\"></times><cn id=\"S3.T1.1.1.1.m1.1.1.2.cmml\" type=\"integer\" xref=\"S3.T1.1.1.1.m1.1.1.2\">2</cn><ci id=\"S3.T1.1.1.1.m1.1.1.3a.cmml\" xref=\"S3.T1.1.1.1.m1.1.1.3\"><mtext id=\"S3.T1.1.1.1.m1.1.1.3.cmml\" xref=\"S3.T1.1.1.1.m1.1.1.3\">\u00a0kHz</mtext></ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T1.1.1.1.m1.1c\">2\\text{ kHz}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.T1.1.1.1.m1.1d\">2 kHz</annotation></semantics></math>)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S3.T1.2.2.2\">SPTK (<math alttext=\"8\\text{ kHz}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.2.2.2.m1.1\"><semantics id=\"S3.T1.2.2.2.m1.1a\"><mrow id=\"S3.T1.2.2.2.m1.1.1\" xref=\"S3.T1.2.2.2.m1.1.1.cmml\"><mn id=\"S3.T1.2.2.2.m1.1.1.2\" xref=\"S3.T1.2.2.2.m1.1.1.2.cmml\">8</mn><mo id=\"S3.T1.2.2.2.m1.1.1.1\" xref=\"S3.T1.2.2.2.m1.1.1.1.cmml\">\u2062</mo><mtext id=\"S3.T1.2.2.2.m1.1.1.3\" xref=\"S3.T1.2.2.2.m1.1.1.3a.cmml\">\u00a0kHz</mtext></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.T1.2.2.2.m1.1b\"><apply id=\"S3.T1.2.2.2.m1.1.1.cmml\" xref=\"S3.T1.2.2.2.m1.1.1\"><times id=\"S3.T1.2.2.2.m1.1.1.1.cmml\" xref=\"S3.T1.2.2.2.m1.1.1.1\"></times><cn id=\"S3.T1.2.2.2.m1.1.1.2.cmml\" type=\"integer\" xref=\"S3.T1.2.2.2.m1.1.1.2\">8</cn><ci id=\"S3.T1.2.2.2.m1.1.1.3a.cmml\" xref=\"S3.T1.2.2.2.m1.1.1.3\"><mtext id=\"S3.T1.2.2.2.m1.1.1.3.cmml\" xref=\"S3.T1.2.2.2.m1.1.1.3\">\u00a0kHz</mtext></ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T1.2.2.2.m1.1c\">8\\text{ kHz}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.T1.2.2.2.m1.1d\">8 kHz</annotation></semantics></math>)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S3.T1.2.2.5\">Ours (ERB)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S3.T1.2.2.6\">Ours (mel)</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S3.T1.2.3.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S3.T1.2.3.1.1\">MIR-1K</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.2.3.1.2\">86.6%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.2.3.1.3\">96.5%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.2.3.1.4\">68.2%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.2.3.1.5\">95.7%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.2.3.1.6\">96.2%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.2.4.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S3.T1.2.4.2.1\">MDB</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.2.4.2.2\">90.7%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.2.4.2.3\">94.1%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.2.4.2.4\">61.4%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.2.4.2.5\">94.0%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.2.4.2.6\">96.1%</td>\n</tr>\n</tbody>\n</table>"
        },
        {
            "id": "S5.T2",
            "type": "table",
            "title": "2507.11233v1_Table2",
            "caption": "Table 2:Evaluation results of the supervised models as measured by Raw Pitch Accuracy (RPA), the F-Score for the voiced/unvoiced decision, and Overall Accuracy (OA). We also report the sizes of the neural networks and the maximum window size required by the estimator. Models are evaluated using the combined test partition of MDB-stem-synth and PTDB-TUG published in[9], as well as on the entire MIR-1K dataset, which was not used in training.",
            "metadata": {},
            "table_html": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S5.T2.2\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T2.2.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T2.2.1.1.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S5.T2.2.1.1.1.1\">Method</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T2.2.1.1.2\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S5.T2.2.1.1.2.1\"># Params</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T2.2.1.1.3\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S5.T2.2.1.1.3.1\">Window Size [ms]</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" colspan=\"3\" id=\"S5.T2.2.1.1.4\">Test Partition</td>\n<td class=\"ltx_td ltx_align_center\" colspan=\"3\" id=\"S5.T2.2.1.1.5\">MIR-1K</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.2.2.2\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.2.2.2.1\">RPA</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.2.2.2.2\">F-Score</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T2.2.2.2.3\">OA</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.2.2.2.4\">RPA</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.2.2.2.5\">F-Score</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.2.2.2.6\">OA</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.2.3.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S5.T2.2.3.3.1\">PYIN</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" id=\"S5.T2.2.3.3.2\">-</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S5.T2.2.3.3.3\">145</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.2.3.3.4\">89.4%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.2.3.3.5\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T2.2.3.3.6\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.2.3.3.7\">95.4%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.2.3.3.8\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.2.3.3.9\">-</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.2.4.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T2.2.4.4.1\">SWIPE</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" id=\"S5.T2.2.4.4.2\">-</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S5.T2.2.4.4.3\">327</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.2.4.4.4\">93.2%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.2.4.4.5\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T2.2.4.4.6\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.2.4.4.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T2.2.4.4.7.1\">96.2%</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.2.4.4.8\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.2.4.4.9\">-</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.2.5.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S5.T2.2.5.5.1\">CQT-sup (ours)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" id=\"S5.T2.2.5.5.2\">0.9M</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S5.T2.2.5.5.3\">1871</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.2.5.5.4\">98.1%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.2.5.5.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T2.2.5.5.5.1\">98.7%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T2.2.5.5.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T2.2.5.5.6.1\">99.2%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.2.5.5.7\">92.2%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.2.5.5.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T2.2.5.5.8.1\">91.2%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.2.5.5.9\">87.1%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.2.6.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T2.2.6.6.1\">SWIPE-sup (ours)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" id=\"S5.T2.2.6.6.2\">0.9M</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S5.T2.2.6.6.3\">327</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.2.6.6.4\">97.9%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.2.6.6.5\">98.1%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T2.2.6.6.6\">98.6%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.2.6.6.7\">93.5%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.2.6.6.8\">90.0%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.2.6.6.9\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T2.2.6.6.9.1\">87.8%</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.2.7.7\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S5.T2.2.7.7.1\">FCNF0++ <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2507.11233v1#bib.bib9\" title=\"\">9</a>]</cite>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" id=\"S5.T2.2.7.7.2\">6.6M</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S5.T2.2.7.7.3\">128</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.2.7.7.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T2.2.7.7.4.1\">98.3%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.2.7.7.5\">98.2%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T2.2.7.7.6\">98.7%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.2.7.7.7\">91.0%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.2.7.7.8\">87.8%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.2.7.7.9\">86.0%</td>\n</tr>\n</tbody>\n</table>"
        },
        {
            "id": "S5.T3",
            "type": "table",
            "title": "2507.11233v1_Table3",
            "caption": "Table 3:Evaluation results of the self-supervised models. For both datasets, we highlight the best result achieved when training and evaluating on the same dataset (no explicit pitch information is provided to the model during training), and when training on one dataset and evaluating on the other. The performance of PYIN and SWIPE is given for comparison.",
            "metadata": {},
            "table_html": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S5.T3.2\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T3.2.1.1\">\n<th class=\"ltx_td ltx_th ltx_th_row\" id=\"S5.T3.2.1.1.1\"></th>\n<td class=\"ltx_td\" id=\"S5.T3.2.1.1.2\"></td>\n<td class=\"ltx_td\" id=\"S5.T3.2.1.1.3\"></td>\n<td class=\"ltx_td ltx_align_center\" colspan=\"2\" id=\"S5.T3.2.1.1.4\">Raw Pitch Accuracy</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.2.2.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T3.2.2.2.1\">Method</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.2.2.2.2\"># params</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.2.2.2.3\">Trained on</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.2.2.2.4\">MIR-1K</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.2.2.2.5\">MDB-stem-synth</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.2.3.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S5.T3.2.3.3.1\">PYIN</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.2.3.3.2\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.2.3.3.3\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.2.3.3.4\">95.4%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.2.3.3.5\">91.6%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.2.4.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T3.2.4.4.1\">SWIPE</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.2.4.4.2\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.2.4.4.3\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.2.4.4.4\">96.2%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.2.4.4.5\">96.1%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.2.5.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S5.T3.2.5.5.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S5.T3.2.5.5.1.1\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S5.T3.2.5.5.1.1.1\">\n<span class=\"ltx_tr\" id=\"S5.T3.2.5.5.1.1.1.1\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_left\" id=\"S5.T3.2.5.5.1.1.1.1.1\">PESTO</span></span>\n<span class=\"ltx_tr\" id=\"S5.T3.2.5.5.1.1.1.2\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_left\" id=\"S5.T3.2.5.5.1.1.1.2.1\">(baseline from <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2507.11233v1#bib.bib14\" title=\"\">14</a>]</cite>)</span></span>\n</span></span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.2.5.5.2\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S5.T3.2.5.5.2.1\">28.9k</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.2.5.5.3\">MIR-1K</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.2.5.5.4\">96.1%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.2.5.5.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.2.5.5.5.1\">94.6%</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.2.6.6\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.2.6.6.1\">MDB-stem-synth</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.2.6.6.2\">93.5%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.2.6.6.3\">95.5%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.2.7.7\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S5.T3.2.7.7.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S5.T3.2.7.7.1.1\">CQT-tiny</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.2.7.7.2\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S5.T3.2.7.7.2.1\">647</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.2.7.7.3\">MIR-1K</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.2.7.7.4\">95.6%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.2.7.7.5\">78.8%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.2.8.8\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.2.8.8.1\">MDB-stem-synth</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.2.8.8.2\">91.7%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.2.8.8.3\">95.5%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.2.9.9\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S5.T3.2.9.9.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S5.T3.2.9.9.1.1\">SWIPE-full</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.2.9.9.2\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S5.T3.2.9.9.2.1\">28.2k</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.2.9.9.3\">MIR-1K</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.2.9.9.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.2.9.9.4.1\">97.0%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.2.9.9.5\">89.7%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.2.10.10\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.2.10.10.1\">MDB-stem-synth</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.2.10.10.2\">96.1%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.2.10.10.3\">96.4%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.2.11.11\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S5.T3.2.11.11.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S5.T3.2.11.11.1.1\">SWIPE-tiny</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.2.11.11.2\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S5.T3.2.11.11.2.1\">647</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.2.11.11.3\">MIR-1K</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.2.11.11.4\">96.6%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.2.11.11.5\">90.1%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.2.12.12\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.2.12.12.1\">MDB-stem-synth</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.2.12.12.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.2.12.12.2.1\">96.4%</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.2.12.12.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.2.12.12.3.1\">96.5%</span></td>\n</tr>\n</tbody>\n</table>"
        },
        {
            "id": "S5.T4",
            "type": "table",
            "title": "2507.11233v1_Table4",
            "caption": "Table 4:The effect of reducing the maximum window size (for a sampling rate of44.1\u2062kHz44.1kHz44.1\\text{ kHz}44.1 kHz) at which SWIPE scores are calculated. RPA on MIR-1K is reported for SWIPE-tiny (trained on MDB-stem-synth) and SWIPE-sup.",
            "metadata": {},
            "table_html": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S5.T4.4\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S5.T4.4.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r\" colspan=\"2\" id=\"S5.T4.4.1.1.1\">Window Size</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" colspan=\"2\" id=\"S5.T4.4.1.1.2\">Raw Pitch Accuracy</th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.4.2.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row\" id=\"S5.T4.4.2.2.1\">[Samples]</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r\" id=\"S5.T4.4.2.2.2\">[ms]</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S5.T4.4.2.2.3\">SWIPE-tiny</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S5.T4.4.2.2.4\">SWIPE-sup</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T4.4.3.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" id=\"S5.T4.4.3.1.1\">16384</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S5.T4.4.3.1.2\">372</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.4.3.1.3\">96.4%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.4.3.1.4\">97.2%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.4.4.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" id=\"S5.T4.4.4.2.1\">8192</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S5.T4.4.4.2.2\">186</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.4.4.2.3\">96.4%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.4.4.2.4\">97.1%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.4.5.3\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" id=\"S5.T4.4.5.3.1\">4096</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S5.T4.4.5.3.2\">93</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.4.5.3.3\">96.2%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.4.5.3.4\">96.7%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.4.6.4\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" id=\"S5.T4.4.6.4.1\">2048</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S5.T4.4.6.4.2\">46</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.4.6.4.3\">85.0%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.4.6.4.4\">86.9%</td>\n</tr>\n</tbody>\n</table>"
        },
        {
            "id": "S5.T5",
            "type": "table",
            "title": "2507.11233v1_Table5",
            "caption": "Table 5:The effect that adding white noise at various signal-to-noise ratios to the input audio has on the raw pitch accuracy on the MIR-1K dataset for various models.",
            "metadata": {},
            "table_html": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S5.T5.2\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T5.2.1.1\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_r\" id=\"S5.T5.2.1.1.1\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"></th>\n<td class=\"ltx_td ltx_align_center\" colspan=\"5\" id=\"S5.T5.2.1.1.2\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">Raw Pitch Accuracy (MIR-1K)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.2.2.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T5.2.2.2.1\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">Model</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.2.2.2.2\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">clean</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.2.2.2.3\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">5 dB</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.2.2.2.4\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">0 dB</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.2.2.2.5\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">-5 dB</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.2.2.2.6\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">-10 dB</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.2.3.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S5.T5.2.3.3.1\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">PYIN</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.2.3.3.2\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">95.4%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.2.3.3.3\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">95.3%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.2.3.3.4\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">95.1%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.2.3.3.5\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.2.3.3.5.1\">93.7%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.2.3.3.6\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">85.8%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.2.4.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T5.2.4.4.1\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">SWIPE</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.2.4.4.2\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">96.2%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.2.4.4.3\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">93.9%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.2.4.4.4\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">91.2%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.2.4.4.5\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">85.6%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.2.4.4.6\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">75.2%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.2.5.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S5.T5.2.5.5.1\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">CQT-sup</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.2.5.5.2\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">92.2%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.2.5.5.3\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">91.5%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.2.5.5.4\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">89.3%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.2.5.5.5\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">87.3%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.2.5.5.6\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">82.3%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.2.6.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T5.2.6.6.1\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">SWIPE-sup</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.2.6.6.2\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">93.5%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.2.6.6.3\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">91.6%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.2.6.6.4\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">90.0%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.2.6.6.5\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">87.1%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.2.6.6.6\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">72.2%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.2.7.7\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T5.2.7.7.1\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">SWIPE-tiny</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.2.7.7.2\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.2.7.7.2.1\">96.6</span>%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.2.7.7.3\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.2.7.7.3.1\">96.0</span>%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.2.7.7.4\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.2.7.7.4.1\">95.3%</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.2.7.7.5\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">93.4%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.2.7.7.6\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.2.7.7.6.1\">88.5%</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.2.8.8\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S5.T5.2.8.8.1\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">PESTO</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.2.8.8.2\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">94.6%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.2.8.8.3\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">93.3%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.2.8.8.4\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">92.9%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.2.8.8.5\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">90.1%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.2.8.8.6\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">81.7%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.2.9.9\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S5.T5.2.9.9.1\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">FCNF0++</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.2.9.9.2\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">91.0%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.2.9.9.3\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">90.3%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.2.9.9.4\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">89.0%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.2.9.9.5\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">83.5%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.2.9.9.6\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">81.0%</td>\n</tr>\n</tbody>\n</table>"
        }
    ],
    "metadata": {},
    "pdf_path": "/data3/guofang/peirongcan/PaperIgnition/orchestrator/pdfs/2507.11233v1.pdf",
    "HTML_path": "/data3/guofang/peirongcan/PaperIgnition/orchestrator/htmls/2507.11233v1.html"
}