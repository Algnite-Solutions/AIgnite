from abc import ABC, abstractmethod
import asyncio
import aiohttp
from typing import List
import os
import yaml
from google import genai
from google.genai import types
from AIgnite.data.docset import DocSet
from tqdm import tqdm
import time

# ç¼“å­˜é…ç½®
_PROMPT_CONFIGS = {}

def _load_prompt_config(input_format: str = "pdf"):
    """
    æ ¹æ®è¾“å…¥æ ¼å¼åŠ è½½prompté…ç½®
    
    Args:
        input_format: è¾“å…¥æ ¼å¼ï¼Œ'pdf' æˆ– 'text'
    
    Returns:
        prompté…ç½®å­—å…¸
    """
    if input_format in _PROMPT_CONFIGS:
        return _PROMPT_CONFIGS[input_format]
    
    if input_format == "pdf":
        config_file = "pdf_prompts.yaml"
    elif input_format == "text":
        config_file = "text_prompts.yaml"
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„è¾“å…¥æ ¼å¼: {input_format}ï¼Œåº”ä¸º 'pdf' æˆ– 'text'")
    
    config_path = os.path.join(os.path.dirname(__file__), config_file)
    with open(config_path, 'r', encoding='utf-8') as f:
        _PROMPT_CONFIGS[input_format] = yaml.safe_load(f)
    
    return _PROMPT_CONFIGS[input_format]

def format_blog_prompt(data_path: str, arxiv_id: str, text_chunks: str, table_chunks: str, figure_chunks: str, title: str, input_format: str = "pdf") -> str:
    """
    æ ¼å¼åŒ–åšå®¢ç”Ÿæˆprompt
    
    Args:
        data_path: æ•°æ®è·¯å¾„
        arxiv_id: è®ºæ–‡ID
        text_chunks: æ–‡æœ¬å—
        table_chunks: è¡¨æ ¼å—
        figure_chunks: å›¾è¡¨å—
        title: è®ºæ–‡æ ‡é¢˜
        input_format: è¾“å…¥æ ¼å¼ï¼Œ'pdf' æˆ– 'text'ï¼Œé»˜è®¤ä¸º 'pdf'
    
    Returns:
        æ ¼å¼åŒ–åçš„prompt
    """
    config = _load_prompt_config(input_format)
    
    # PDFæ¨¡å¼åªéœ€è¦åŸºæœ¬ä¿¡æ¯ï¼Œæ–‡æœ¬æ¨¡å¼éœ€è¦æ‰€æœ‰ä¿¡æ¯
    if input_format == "pdf":
        return config['blog_generation_prompt'].format(
            data_path=data_path,
            arxiv_id=arxiv_id,
            figure_chunks=figure_chunks,
            title=title
        )
    else:
        return config['blog_generation_prompt'].format(
            data_path=data_path,
            arxiv_id=arxiv_id,
            text_chunks=text_chunks,
            table_chunks=table_chunks,
            figure_chunks=figure_chunks,
            title=title
        )

class BaseGenerator(ABC):
    """
    Abstract base class for blog generators.
    """
    @abstractmethod
    def generate_digest(self):
        pass


class GeminiBlogGenerator_default(BaseGenerator):
    """
    A class to generate blog posts using the Gemini model.
    This class uses the Google Gemini model to generate blog posts based on the provided PDF documents.
    TODO: @Qi, replace data_path and output_path with the actual DB_query and DB_write functions.
    """
    def __init__(self, model_name="gemini-2.5-flash-lite-preview-09-2025", data_path="./output", output_path="./experiments/output", input_format="pdf"):
        api_key = "AIzaSyDbZun4zIIwgCWpmccj_wAczR4fXHVIZ0M"
        if not api_key:
            raise ValueError("GEMINI_API_KEY environment variable is not set")
        self.client = genai.Client(api_key=api_key)
        self.model_name = model_name
        self.data_path = data_path
        self.output_path = output_path
        self.input_format = input_format

    def generate_digest(self, papers: List[DocSet], input_format="pdf"):
        import concurrent.futures
        import threading
        
        def generate_with_delay(paper):
            self._generate_single_blog(paper, input_format)
            time.sleep(5)
        
        # ä½¿ç”¨çº¿ç¨‹æ± è¿›è¡Œå¹¶è¡Œå¤„ç†ï¼Œé™åˆ¶æœ€å¤§å¹¶å‘æ•°é¿å…APIé™åˆ¶
        max_workers = min(len(papers), 50)  # æœ€å¤š50ä¸ªå¹¶å‘ä»»åŠ¡
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = [executor.submit(generate_with_delay, paper) for paper in papers]
            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            for future in concurrent.futures.as_completed(futures):
                try:
                    future.result()
                except Exception as e:
                    print(f"å¤„ç†è®ºæ–‡æ—¶å‡ºé”™: {e}")

    def _generate_single_blog(self, paper: DocSet, input_format="pdf"):
        # Read and encode the PDF bytes only if input_format is pdf
        if input_format == "pdf":
            with open(paper.pdf_path, "rb") as pdf_file:
                pdf_data = pdf_file.read()

        arxiv_id = paper.doc_id
        prompt = format_blog_prompt(
            data_path=self.data_path,
            arxiv_id=arxiv_id,
            text_chunks=paper.text_chunks,
            table_chunks=paper.table_chunks,
            figure_chunks=paper.figure_chunks,
            title=paper.title,
            input_format=input_format
        )
        import time

        max_retries = 5
        for attempt in range(1, max_retries + 1):
            try:
                # Build contents list based on input_format
                contents = [prompt]
                if input_format == "pdf":
                    contents.append(
                        types.Part.from_bytes(
                            data=pdf_data,
                            mime_type='application/pdf',
                        )
                    )
                
                response = self.client.models.generate_content(
                    model=self.model_name,
                    contents=contents
                )
                break  # æˆåŠŸå°±è·³å‡ºå¾ªç¯
            except Exception as e:
                print(f"å¤„ç†è®ºæ–‡æ—¶å‡ºé”™ï¼ˆç¬¬ {attempt} æ¬¡å°è¯•ï¼‰: {e}")
                if attempt < max_retries:
                    sleep_time = 2 ** attempt  # æŒ‡æ•°é€€é¿ï¼ˆ2, 4, 8, 16...ç§’ï¼‰
                    print(f"ç­‰å¾… {sleep_time} ç§’åé‡è¯•...")
                    time.sleep(sleep_time)
                else:
                    print("å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œç»ˆæ­¢ã€‚")
                    return

        markdown_path = os.path.join(self.output_path, f"{arxiv_id}.md")
        os.makedirs(os.path.dirname(markdown_path), exist_ok=True)  # ç¡®ä¿ç›®å½•å­˜åœ¨
        with open(markdown_path, "w", encoding="utf-8") as md_file:
            md_file.write(response.text)

        print(f"âœ… Markdown file saved to {markdown_path}")
        print("ğŸ“Š Token usage:", response.usage_metadata.prompt_token_count, response.usage_metadata.candidates_token_count)

class GeminiBlogGenerator_recommend(BaseGenerator):
    """
    A class to generate blog posts using the Gemini model.
    This class uses the Google Gemini model to generate blog posts based on the provided PDF documents.
    TODO: @Qi, replace data_path and output_path with the actual DB_query and DB_write functions.
    """
    def __init__(self, model_name="gemini-2.5-flash-preview-09-2025", data_path="./output", output_path="./experiments/output", input_format="pdf"):
        api_key = "AIzaSyDbZun4zIIwgCWpmccj_wAczR4fXHVIZ0M"
        if not api_key:
            raise ValueError("GEMINI_API_KEY environment variable is not set")
        self.client = genai.Client(api_key=api_key)
        self.model_name = model_name
        self.data_path = data_path
        self.output_path = output_path
        self.input_format = input_format

    def generate_digest(self, papers: List[DocSet], input_format="pdf"):
        import concurrent.futures
        import threading
        
        def generate_with_delay(paper):
            self._generate_single_blog(paper, input_format)
            time.sleep(5)
        
        # ä½¿ç”¨çº¿ç¨‹æ± è¿›è¡Œå¹¶è¡Œå¤„ç†ï¼Œé™åˆ¶æœ€å¤§å¹¶å‘æ•°é¿å…APIé™åˆ¶
        max_workers = min(len(papers), 50)  # æœ€å¤š50ä¸ªå¹¶å‘ä»»åŠ¡
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            # ä¿æŒè¾“å…¥é¡ºåºï¼šæäº¤ä»»åŠ¡å¹¶ä¿å­˜æ˜ å°„
            paper_to_future = {i: (paper, executor.submit(generate_with_delay, paper)) 
                               for i, paper in enumerate(papers)}
            
            # æŒ‰ç…§è¾“å…¥é¡ºåºç­‰å¾…ä»»åŠ¡å®Œæˆï¼Œç¡®ä¿åšå®¢æ–‡ä»¶é¡ºåºä¸è®ºæ–‡åˆ—è¡¨ä¸€è‡´
            for i in range(len(papers)):
                paper, future = paper_to_future[i]
                try:
                    future.result()
                except Exception as e:
                    print(f"å¤„ç†è®ºæ–‡ {paper.doc_id} æ—¶å‡ºé”™: {e}")

    def _generate_single_blog(self, paper: DocSet, input_format="pdf"):
        # Debug: print paper information
        print(f"ğŸ“„ æ­£åœ¨ç”Ÿæˆåšå®¢ - è®ºæ–‡ID: {paper.doc_id}")
        print(f"ğŸ“„ è®ºæ–‡æ ‡é¢˜: {paper.title[:100]}...")
        print(f"ğŸ“„ PDFè·¯å¾„: {paper.pdf_path}")
        print(f"ğŸ“„ è¾“å…¥æ ¼å¼: {input_format}")
        
        # Read and encode the PDF bytes only if input_format is pdf
        if input_format == "pdf":
            with open(paper.pdf_path, "rb") as pdf_file:
                pdf_data = pdf_file.read()

        arxiv_id = paper.doc_id
        prompt = format_blog_prompt(
            data_path=self.data_path,
            arxiv_id=arxiv_id,
            text_chunks=str(paper.text_chunks),
            table_chunks=str(paper.table_chunks),
            figure_chunks=str(paper.figure_chunks),
            title=paper.title,
            input_format=input_format
        )

        print(prompt)
        import time

        max_retries = 5
        for attempt in range(1, max_retries + 1):
            try:
                # Build contents list based on input_format
                contents = [prompt]
                if input_format == "pdf":
                    contents.append(
                        types.Part.from_bytes(
                            data=pdf_data,
                            mime_type='application/pdf',
                        )
                    )
                
                response = self.client.models.generate_content(
                    model=self.model_name,
                    contents=contents
                )
                break  # æˆåŠŸå°±è·³å‡ºå¾ªç¯
            except Exception as e:
                print(f"å¤„ç†è®ºæ–‡æ—¶å‡ºé”™ï¼ˆç¬¬ {attempt} æ¬¡å°è¯•ï¼‰: {e}")
                if attempt < max_retries:
                    sleep_time = 2 ** attempt  # æŒ‡æ•°é€€é¿ï¼ˆ2, 4, 8, 16...ç§’ï¼‰
                    print(f"ç­‰å¾… {sleep_time} ç§’åé‡è¯•...")
                    time.sleep(sleep_time)
                else:
                    print("å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œç»ˆæ­¢ã€‚")
                    return

        markdown_path = os.path.join(self.output_path, f"{arxiv_id}.md")
        os.makedirs(os.path.dirname(markdown_path), exist_ok=True)  # ç¡®ä¿ç›®å½•å­˜åœ¨
        with open(markdown_path, "w", encoding="utf-8") as md_file:
            md_file.write(response.text)

        print(f"âœ… Markdown file saved to {markdown_path}")
        print("ğŸ“Š Token usage:", response.usage_metadata.prompt_token_count, response.usage_metadata.candidates_token_count)

class AsyncvLLMGenerator:
    def __init__(self, model_name="deepseek-ai/DeepSeek-R1-Distill-Llama-8B", api_base="http://localhost:8000/v1",data_path="./output", output_path="./experiments/output"):
        self.model_name = model_name
        self.api_base = api_base
        self.data_path = data_path
        self.output_path = output_path

    async def generate_response(self, session, prompt, system_prompt, max_tokens=2048, arxiv_id=None):
        """
        Send a single chat completion request to the vLLM server.
        """
        url = f"{self.api_base}/chat/completions"
        payload = {
            "model": self.model_name,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt},
            ],
            #"max_tokens": max_tokens,
        }
        async with session.post(url, json=payload) as resp:
            if resp.status != 200:
                text = await resp.text()
                raise Exception(f"Error {resp.status}: {text}")
            response = await resp.json()

            if arxiv_id is None:
                arxiv_id = 111
            markdown_path = os.path.join(self.output_path, f"{arxiv_id}.md")
            os.makedirs(os.path.dirname(markdown_path), exist_ok=True)  # ç¡®ä¿ç›®å½•å­˜åœ¨
            with open(markdown_path, "w", encoding="utf-8") as md_file:
                content = response["choices"][0]["message"]["content"]
                #åœ¨è¿™é‡Œå¢åŠ é€»è¾‘ï¼ŒåŒ¹é…<think>å’Œ</think>ä¹‹é—´çš„å†…å®¹ï¼Œå»æ‰å®ƒï¼Œå°†å‰©ä½™éƒ¨åˆ†ä¿å­˜åˆ°markdown_pathä¸­
                think_start = content.find("<think>")
                think_end = content.find("</think>")
                if think_start != -1 and think_end != -1:
                    think_content = content[think_start:think_end + 8]
                    content = content.replace(think_content, "")
                md_file.write(content)
            print(f"âœ… Markdown file saved to {markdown_path}")
            print("ğŸ“Š Token usage:", response["usage"]["prompt_tokens"], response["usage"]["completion_tokens"])
            # Return the generated text.
            return response["choices"][0]["message"]["content"]
        
    async def batch_generate(self, prompts, system_prompts=None, max_tokens=2048, papers: List[DocSet] = None):
        """
        Batch generate responses concurrently using asynchronous HTTP requests,
        preserving the order of prompts and updating progress.

        Args:
            prompts (list): List of input prompts.
            system_prompts (str or list, optional): A single system prompt or list of prompts.
            max_tokens (int, optional): Maximum number of tokens to generate.

        Returns:
            list: Generated responses in the same order as the input prompts.
        """
        if isinstance(system_prompts, str):
            system_prompts = [system_prompts] * len(prompts)
        elif system_prompts is None:
            system_prompts = [''] * len(prompts)
        if len(system_prompts) != len(prompts):
            raise ValueError("Length of system_prompts must match the length of prompts")

        timeout = aiohttp.ClientTimeout(total=None)  # No hardcoded timeout
        async with aiohttp.ClientSession(timeout=timeout) as session:
            tasks = []
            pbar = tqdm(total=len(prompts), desc="Processing")
            
            # Create indexed tasks to track the original order
            for i, (prompt, sys_prompt) in enumerate(zip(prompts, system_prompts)):
                task = asyncio.create_task(self.generate_response(session, prompt, sys_prompt, max_tokens, papers[i].doc_id))
                task.add_done_callback(lambda fut: pbar.update(1))
                tasks.append((i, task))
            
            # Wait for all tasks to complete
            done, _ = await asyncio.wait([t[1] for t in tasks])
            
            # Retrieve results in the original order
            responses = [None] * len(prompts)
            for i, task in tasks:
                responses[i] = task.result()

            pbar.close()
            return responses

    def generate_digest(self, papers: List[DocSet]):
        return 0