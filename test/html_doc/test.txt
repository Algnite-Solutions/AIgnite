<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2502.13957] RAG-Gym: Optimizing Reasoning and Search Agents with Process Supervision</title><meta property="og:description" content="Retrieval-augmented generation (RAG) has shown great potential for knowledge-intensive tasks, but its traditional architectures rely on static retrieval, limiting their effectiveness for complex questions that require …">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="RAG-Gym: Optimizing Reasoning and Search Agents with Process Supervision">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="RAG-Gym: Optimizing Reasoning and Search Agents with Process Supervision">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2502.13957">
<link rel="canonical" target="_blank" href="https://ar5iv.labs.arxiv.org/html/2502.13957">

<!--Generated on Wed Mar  5 18:41:33 2025 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="Machine Learning,  ICML">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_pruned_first">
<h1 class="ltx_title ltx_title_document">RAG-Gym: Optimizing Reasoning and Search Agents with Process Supervision</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Guangzhi Xiong
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Qiao Jin
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xiao Wang
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yin Fang
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Haolin Liu
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yifan Yang
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Fangyuan Chen
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zhixing Song
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Dengyu Wang
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Minjia Zhang
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zhiyong Lu
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Aidong Zhang
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p">Retrieval-augmented generation (RAG) has shown great potential for knowledge-intensive tasks, but its traditional architectures rely on static retrieval, limiting their effectiveness for complex questions that require sequential information-seeking. While agentic reasoning and search offer a more adaptive approach, most existing methods depend heavily on prompt engineering. In this work, we introduce RAG-Gym, a unified optimization framework that enhances information-seeking agents through fine-grained process supervision at each search step. We also propose ReSearch, a novel agent architecture that synergizes answer reasoning and search query generation within the RAG-Gym framework. Experiments on four challenging datasets show that RAG-Gym improves performance by up to 25.6% across various agent architectures, with ReSearch consistently outperforming existing baselines. Further analysis highlights the effectiveness of advanced LLMs as process reward judges and the transferability of trained reward models as verifiers for different LLMs. Additionally, we examine the scaling properties of training and inference in agentic RAG.
The project homepage is available at <a target="_blank" href="https://rag-gym.github.io/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://rag-gym.github.io/</a>.</p>
</div>
<div class="ltx_keywords">Machine Learning, ICML
</div>
<div id="p2" class="ltx_para">
<br class="ltx_break">
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p class="ltx_p">Large language models (LLMs) have revolutionized online information-seeking by providing direct responses to user needs. Such capabilities are often evaluated by knowledge-intensive tasks such as multi-hop question answering (QA), which require sequential reasoning over relevant knowledge <cite class="ltx_cite ltx_citemacro_citep">(Lewis et al., <a href="#bib.bib23" title="" class="ltx_ref">2020</a>; Welbl et al., <a href="#bib.bib54" title="" class="ltx_ref">2018</a>; Tang &amp; Yang, <a href="#bib.bib41" title="" class="ltx_ref">2024</a>)</cite>. Pre-trained on trillions of tokens from diverse sources, LLMs have demonstrated strong reasoning capabilities and acquired considerable general knowledge <cite class="ltx_cite ltx_citemacro_citep">(Wei et al., <a href="#bib.bib51" title="" class="ltx_ref">2022a</a>)</cite>. As such, they are often used as standalone systems that process user queries and generate responses without relying on external utilities <cite class="ltx_cite ltx_citemacro_citep">(Brown et al., <a href="#bib.bib3" title="" class="ltx_ref">2020</a>; Achiam et al., <a href="#bib.bib1" title="" class="ltx_ref">2023</a>; Touvron et al., <a href="#bib.bib42" title="" class="ltx_ref">2023a</a>, <a href="#bib.bib43" title="" class="ltx_ref">b</a>)</cite>. Despite these impressive capabilities, LLMs often struggle with questions where they may lack sufficient and up-to-date domain knowledge. This may lead to inaccurate responses or hallucinations, which can be particularly problematic in high-stakes fields <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a href="#bib.bib67" title="" class="ltx_ref">2023b</a>; Sahoo et al., <a href="#bib.bib33" title="" class="ltx_ref">2024</a>; Ji et al., <a href="#bib.bib15" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p class="ltx_p">Retrieval-augmented generation (RAG) has emerged as a promising solution to address knowledge-intensive tasks <cite class="ltx_cite ltx_citemacro_citep">(Lewis et al., <a href="#bib.bib23" title="" class="ltx_ref">2020</a>)</cite>. By grounding the LLM outputs in relevant information returned by information retrieval (IR) systems, RAG improves both accuracy and verifiability <cite class="ltx_cite ltx_citemacro_citep">(Gao et al., <a href="#bib.bib10" title="" class="ltx_ref">2023</a>)</cite>. Expanding on this paradigm, information-seeking agents such as ReAct <cite class="ltx_cite ltx_citemacro_citep">(Yao et al., <a href="#bib.bib61" title="" class="ltx_ref">2023</a>)</cite> iteratively interact with IR systems to tackle complex questions more effectively <cite class="ltx_cite ltx_citemacro_citep">(Yao et al., <a href="#bib.bib61" title="" class="ltx_ref">2023</a>; Asai et al., <a href="#bib.bib2" title="" class="ltx_ref">2024</a>; Shinn et al., <a href="#bib.bib37" title="" class="ltx_ref">2024</a>; Li et al., <a href="#bib.bib25" title="" class="ltx_ref">2025</a>)</cite>. However, most existing methods rely heavily on prompting techniques <cite class="ltx_cite ltx_citemacro_citep">(Trivedi et al., <a href="#bib.bib44" title="" class="ltx_ref">2023</a>; Asai et al., <a href="#bib.bib2" title="" class="ltx_ref">2024</a>; Jiang et al., <a href="#bib.bib16" title="" class="ltx_ref">2023</a>; Press et al., <a href="#bib.bib29" title="" class="ltx_ref">2023</a>)</cite>, which require substantial efforts for the prompt design and might not generalize well across different tasks <cite class="ltx_cite ltx_citemacro_citep">(Khot et al., <a href="#bib.bib21" title="" class="ltx_ref">2023</a>; Shao et al., <a href="#bib.bib35" title="" class="ltx_ref">2023</a>; Yu et al., <a href="#bib.bib62" title="" class="ltx_ref">2024</a>)</cite>.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2502.13957/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="436" height="252" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Overview of RAG-Gym. (a) RAG-Gym formulates the knowledge-intensive question-answering task as a nested Markov Decision Process (MDP). The process reward data is collected by randomly sampling action candidates at each time step and using an external annotator to select the best one. (b) Different process supervision methods implemented in RAG-Gym.</figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<p class="ltx_p">To improve such agents in solving knowledge-intensive and multi-hop QA tasks, we propose <span class="ltx_text ltx_font_bold">RAG-Gym</span>, a unified framework for optimizing agentic RAG through process supervision. RAG-Gym formulates the knowledge-intensive QA as a nested Markov Decision Process (MDP), where the outer MDP action quality is annotated with process rewards, which in turn guide the inner MDP token generation. By supporting diverse agent architectures and process supervision methods, RAG-Gym provides a comprehensive platform for improving information-seeking agents.
In addition to RAG-Gym, we propose <span class="ltx_text ltx_font_bold">ReSearch</span>, a novel agent architecture that unifies answer reasoning and search query generation in a single action. This design explicitly aligns the generated query with missing information in answer construction.
Through extensive experiments, we demonstrate that RAG-Gym effectively enhances agentic RAG performance on knowledge-intensive tasks, with ReSearch outperforming other baseline agents. Notably, our findings highlight the benefits of training process reward models as verifiers for agentic RAG actions and show their transferability across different LLMs. Further analysis reveals the effectiveness of advanced LLMs as process reward judges and uncovers the scaling laws of process supervision in RAG-Gym across both training and inference phases.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p class="ltx_p">Our key contributions are four-fold: (1) We introduce RAG-Gym, a unified framework for optimizing agentic RAG with process supervision. (2) We propose ReSearch, a novel agent architecture that synergizes answer reasoning and search, achieving state-of-the-art performance over existing baselines. (3) We demonstrate that using trained process reward models as verifiers significantly improves search agent performance. (4) We provide a comprehensive analysis of process supervision sources, reward model transferability, and scaling laws in agentic RAG.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>RAG-Gymnasium (RAG-Gym)</h2>

<div id="S2.p1" class="ltx_para">
<p class="ltx_p">RAG-Gym models knowledge-intensive QA as a nested Markov Decision Process (MDP), where the outer MDP governs high-level action generation through interactions with the information retrieval (IR) environment, while the inner MDP controls token generation within LLMs. This formulation is compatible with various agentic RAG architecture that leverages IR for augmented answer generation. By assigning process rewards from high-level actions to sequential token generation, RAG-Gym enables effective tuning of language agents, which can be further utilized for optimizing agent parameters or training process verifiers.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Knowledge-intensive QA as Nested MDP</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p class="ltx_p">Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ RAG-Gym: Optimizing Reasoning and Search Agents with Process Supervision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> presents the architecture of our RAG-Gym framework, which formulates a knowledge-intensive question answering task as a nested Markov decision process (MDP). With LLMs as the core engine of the reasoning and search agents, the sequential token generation is the inner MDP, where each action is to generate the next token given all existing ones. Below we explain in detail how the outer MDP is constructed to generalize various language agent designs on knowledge-intensive questions.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">State Space <math id="S2.SS1.p2.m1" class="ltx_Math" alttext="\mathcal{S}" display="inline"><semantics><mi class="ltx_font_mathcaligraphic">𝒮</mi><annotation encoding="application/x-tex">\mathcal{S}</annotation></semantics></math>.</span> At each time step <math id="S2.SS1.p2.m2" class="ltx_Math" alttext="t" display="inline"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math>, the state <math id="S2.SS1.p2.m3" class="ltx_Math" alttext="s_{t}\in\mathcal{S}" display="inline"><semantics><mrow><msub><mi>s</mi><mi>t</mi></msub><mo>∈</mo><mi class="ltx_font_mathcaligraphic">𝒮</mi></mrow><annotation encoding="application/x-tex">s_{t}\in\mathcal{S}</annotation></semantics></math> comprises the original question <math id="S2.SS1.p2.m4" class="ltx_Math" alttext="\mathcal{Q}" display="inline"><semantics><mi class="ltx_font_mathcaligraphic">𝒬</mi><annotation encoding="application/x-tex">\mathcal{Q}</annotation></semantics></math> and the information-seeking history <math id="S2.SS1.p2.m5" class="ltx_Math" alttext="\mathcal{H}_{t}" display="inline"><semantics><msub><mi class="ltx_font_mathcaligraphic">ℋ</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\mathcal{H}_{t}</annotation></semantics></math>. Formally, the state <math id="S2.SS1.p2.m6" class="ltx_Math" alttext="s_{t}" display="inline"><semantics><msub><mi>s</mi><mi>t</mi></msub><annotation encoding="application/x-tex">s_{t}</annotation></semantics></math> can be expressed as</p>
<table id="S2.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E1.m1" class="ltx_Math" alttext="s_{t}=(\mathcal{Q},\mathcal{H}_{t})" display="block"><semantics><mrow><msub><mi>s</mi><mi>t</mi></msub><mo>=</mo><mrow><mo stretchy="false">(</mo><mi class="ltx_font_mathcaligraphic">𝒬</mi><mo>,</mo><msub><mi class="ltx_font_mathcaligraphic">ℋ</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">s_{t}=(\mathcal{Q},\mathcal{H}_{t})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math id="S2.SS1.p2.m7" class="ltx_Math" alttext="\mathcal{H}_{t}=\{(q_{1},D_{1}),\cdots,(q_{t-1},D_{t-1})\}" display="inline"><semantics><mrow><msub><mi class="ltx_font_mathcaligraphic">ℋ</mi><mi>t</mi></msub><mo>=</mo><mrow><mo stretchy="false">{</mo><mrow><mo stretchy="false">(</mo><msub><mi>q</mi><mn>1</mn></msub><mo>,</mo><msub><mi>D</mi><mn>1</mn></msub><mo stretchy="false">)</mo></mrow><mo>,</mo><mi mathvariant="normal">⋯</mi><mo>,</mo><mrow><mo stretchy="false">(</mo><msub><mi>q</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>D</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></mrow><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{H}_{t}=\{(q_{1},D_{1}),\cdots,(q_{t-1},D_{t-1})\}</annotation></semantics></math> is history of information-seeking queries <math id="S2.SS1.p2.m8" class="ltx_Math" alttext="q_{1},\cdots,q_{t-1}" display="inline"><semantics><mrow><msub><mi>q</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">⋯</mi><mo>,</mo><msub><mi>q</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">q_{1},\cdots,q_{t-1}</annotation></semantics></math> and their corresponding retrieved documents <math id="S2.SS1.p2.m9" class="ltx_Math" alttext="D_{1},\cdots,D_{t-1}" display="inline"><semantics><mrow><msub><mi>D</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">⋯</mi><mo>,</mo><msub><mi>D</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">D_{1},\cdots,D_{t-1}</annotation></semantics></math> returned by the environment.
Initially, <math id="S2.SS1.p2.m10" class="ltx_Math" alttext="\mathcal{H}_{1}=\emptyset" display="inline"><semantics><mrow><msub><mi class="ltx_font_mathcaligraphic">ℋ</mi><mn>1</mn></msub><mo>=</mo><mi mathvariant="normal">∅</mi></mrow><annotation encoding="application/x-tex">\mathcal{H}_{1}=\emptyset</annotation></semantics></math> since no queries have been made.
The state space <math id="S2.SS1.p2.m11" class="ltx_Math" alttext="\mathcal{S}" display="inline"><semantics><mi class="ltx_font_mathcaligraphic">𝒮</mi><annotation encoding="application/x-tex">\mathcal{S}</annotation></semantics></math> is the set of all possible states:</p>
<table id="S2.E2" class="ltx_equationgroup ltx_eqn_table">
<tbody>
<tr id="S2.E2X" class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S2.E2X.m2" class="ltx_Math" alttext="\displaystyle\mathcal{S}=\big{\{}(\mathcal{Q},\mathcal{H})\big{|}\mathcal{Q}\in\mathcal{I},\mathcal{H}\in\{q\in\mathcal{A}_{q},D\subseteq\mathcal{D}\}\cup\{\emptyset\}\big{\}}," display="inline"><semantics><mrow><mrow><mi class="ltx_font_mathcaligraphic" mathsize="0.700em">𝒮</mi><mo mathsize="0.700em">=</mo><mrow><mo maxsize="1.200em" minsize="1.200em">{</mo><mrow><mo maxsize="0.700em" minsize="0.700em">(</mo><mi class="ltx_font_mathcaligraphic" mathsize="0.700em">𝒬</mi><mo mathsize="0.700em">,</mo><mi class="ltx_font_mathcaligraphic" mathsize="0.700em">ℋ</mi><mo maxsize="0.700em" minsize="0.700em">)</mo></mrow><mo lspace="0em" mathsize="1.200em" rspace="0em">|</mo><mrow><mrow><mi class="ltx_font_mathcaligraphic" mathsize="0.700em">𝒬</mi><mo mathsize="0.700em">∈</mo><mi class="ltx_font_mathcaligraphic" mathsize="0.700em">ℐ</mi></mrow><mo mathsize="0.700em">,</mo><mrow><mi class="ltx_font_mathcaligraphic" mathsize="0.700em">ℋ</mi><mo mathsize="0.700em">∈</mo><mrow><mrow><mo maxsize="0.700em" minsize="0.700em">{</mo><mrow><mrow><mi mathsize="0.700em">q</mi><mo mathsize="0.700em">∈</mo><msub><mi class="ltx_font_mathcaligraphic" mathsize="0.700em">𝒜</mi><mi mathsize="0.700em">q</mi></msub></mrow><mo mathsize="0.700em">,</mo><mrow><mi mathsize="0.700em">D</mi><mo mathsize="0.700em">⊆</mo><mi class="ltx_font_mathcaligraphic" mathsize="0.700em">𝒟</mi></mrow></mrow><mo maxsize="0.700em" minsize="0.700em">}</mo></mrow><mo mathsize="0.700em">∪</mo><mrow><mo maxsize="0.700em" minsize="0.700em">{</mo><mi mathsize="0.700em" mathvariant="normal">∅</mi><mo maxsize="0.700em" minsize="0.700em">}</mo></mrow></mrow></mrow></mrow><mo maxsize="1.200em" minsize="1.200em">}</mo></mrow></mrow><mo mathsize="0.700em">,</mo></mrow><annotation encoding="application/x-tex">\displaystyle\mathcal{S}=\big{\{}(\mathcal{Q},\mathcal{H})\big{|}\mathcal{Q}\in\mathcal{I},\mathcal{H}\in\{q\in\mathcal{A}_{q},D\subseteq\mathcal{D}\}\cup\{\emptyset\}\big{\}},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(2)</span></td>
</tr>
</tbody>
</table>
<p class="ltx_p">where <math id="S2.SS1.p2.m12" class="ltx_Math" alttext="\mathcal{I}" display="inline"><semantics><mi class="ltx_font_mathcaligraphic">ℐ</mi><annotation encoding="application/x-tex">\mathcal{I}</annotation></semantics></math> is the question space, <math id="S2.SS1.p2.m13" class="ltx_Math" alttext="\mathcal{A}_{q}" display="inline"><semantics><msub><mi class="ltx_font_mathcaligraphic">𝒜</mi><mi>q</mi></msub><annotation encoding="application/x-tex">\mathcal{A}_{q}</annotation></semantics></math> is the space of all possible search queries, <math id="S2.SS1.p2.m14" class="ltx_Math" alttext="\mathcal{D}" display="inline"><semantics><mi class="ltx_font_mathcaligraphic">𝒟</mi><annotation encoding="application/x-tex">\mathcal{D}</annotation></semantics></math> is the space of all documents.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Action Space <math id="S2.SS1.p3.m1" class="ltx_Math" alttext="\mathcal{A}" display="inline"><semantics><mi class="ltx_font_mathcaligraphic">𝒜</mi><annotation encoding="application/x-tex">\mathcal{A}</annotation></semantics></math>.</span> While different agents may use various designs to reason about the given state and generate token sequences, we standardize these action sequences by defining a shared abstract action space <math id="S2.SS1.p3.m2" class="ltx_Math" alttext="\mathcal{A}" display="inline"><semantics><mi class="ltx_font_mathcaligraphic">𝒜</mi><annotation encoding="application/x-tex">\mathcal{A}</annotation></semantics></math>. At each time step <math id="S2.SS1.p3.m3" class="ltx_Math" alttext="t" display="inline"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math>, the action <math id="S2.SS1.p3.m4" class="ltx_Math" alttext="a_{t}" display="inline"><semantics><msub><mi>a</mi><mi>t</mi></msub><annotation encoding="application/x-tex">a_{t}</annotation></semantics></math> can be either a search query or a predicted answer to the original question. Thus, we define the action space <math id="S2.SS1.p3.m5" class="ltx_Math" alttext="\mathcal{A}=\mathcal{A}_{q}\cup\mathcal{A}_{p}" display="inline"><semantics><mrow><mi class="ltx_font_mathcaligraphic">𝒜</mi><mo>=</mo><mrow><msub><mi class="ltx_font_mathcaligraphic">𝒜</mi><mi>q</mi></msub><mo>∪</mo><msub><mi class="ltx_font_mathcaligraphic">𝒜</mi><mi>p</mi></msub></mrow></mrow><annotation encoding="application/x-tex">\mathcal{A}=\mathcal{A}_{q}\cup\mathcal{A}_{p}</annotation></semantics></math> where <math id="S2.SS1.p3.m6" class="ltx_Math" alttext="\mathcal{A}_{q}" display="inline"><semantics><msub><mi class="ltx_font_mathcaligraphic">𝒜</mi><mi>q</mi></msub><annotation encoding="application/x-tex">\mathcal{A}_{q}</annotation></semantics></math> is the set of all possible queries, and <math id="S2.SS1.p3.m7" class="ltx_Math" alttext="\mathcal{A}_{p}" display="inline"><semantics><msub><mi class="ltx_font_mathcaligraphic">𝒜</mi><mi>p</mi></msub><annotation encoding="application/x-tex">\mathcal{A}_{p}</annotation></semantics></math> denotes the set of possible answers.</p>
</div>
<div id="S2.SS1.p4" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">IR Environment.</span> The environment of the outer MDP in RAG-Gym is driven by an IR system, which takes a search query <math id="S2.SS1.p4.m1" class="ltx_Math" alttext="q_{t}" display="inline"><semantics><msub><mi>q</mi><mi>t</mi></msub><annotation encoding="application/x-tex">q_{t}</annotation></semantics></math> as input and returns a set of relevant documents <math id="S2.SS1.p4.m2" class="ltx_Math" alttext="{D}_{t}" display="inline"><semantics><msub><mi>D</mi><mi>t</mi></msub><annotation encoding="application/x-tex">{D}_{t}</annotation></semantics></math> as output. Formally, the IR system can be represented as a mapping from <math id="S2.SS1.p4.m3" class="ltx_Math" alttext="\mathcal{A}_{q}" display="inline"><semantics><msub><mi class="ltx_font_mathcaligraphic">𝒜</mi><mi>q</mi></msub><annotation encoding="application/x-tex">\mathcal{A}_{q}</annotation></semantics></math> to <math id="S2.SS1.p4.m4" class="ltx_Math" alttext="\mathcal{P}(\mathcal{D})" display="inline"><semantics><mrow><mi class="ltx_font_mathcaligraphic">𝒫</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi class="ltx_font_mathcaligraphic">𝒟</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{P}(\mathcal{D})</annotation></semantics></math>
where <math id="S2.SS1.p4.m5" class="ltx_Math" alttext="\mathcal{P}(\mathcal{D})" display="inline"><semantics><mrow><mi class="ltx_font_mathcaligraphic">𝒫</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi class="ltx_font_mathcaligraphic">𝒟</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{P}(\mathcal{D})</annotation></semantics></math> is the power set of <math id="S2.SS1.p4.m6" class="ltx_Math" alttext="\mathcal{D}" display="inline"><semantics><mi class="ltx_font_mathcaligraphic">𝒟</mi><annotation encoding="application/x-tex">\mathcal{D}</annotation></semantics></math>. The retrieval process is determined by the underlying text retriever and specific retrieval settings (e.g., the number of returned documents). These settings are fixed to ensure stability of state transitions.</p>
</div>
<div id="S2.SS1.p5" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">MDP Workflow.</span> For a given question <math id="S2.SS1.p5.m1" class="ltx_Math" alttext="\mathcal{Q}" display="inline"><semantics><mi class="ltx_font_mathcaligraphic">𝒬</mi><annotation encoding="application/x-tex">\mathcal{Q}</annotation></semantics></math>, the MDP starts at the initial state <math id="S2.SS1.p5.m2" class="ltx_Math" alttext="s_{1}=(\mathcal{Q},\emptyset)" display="inline"><semantics><mrow><msub><mi>s</mi><mn>1</mn></msub><mo>=</mo><mrow><mo stretchy="false">(</mo><mi class="ltx_font_mathcaligraphic">𝒬</mi><mo>,</mo><mi mathvariant="normal">∅</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">s_{1}=(\mathcal{Q},\emptyset)</annotation></semantics></math>. For every step <math id="S2.SS1.p5.m3" class="ltx_Math" alttext="t" display="inline"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math>, <math id="S2.SS1.p5.m4" class="ltx_Math" alttext="a_{t}" display="inline"><semantics><msub><mi>a</mi><mi>t</mi></msub><annotation encoding="application/x-tex">a_{t}</annotation></semantics></math> is sampled from the agent’s policy <math id="S2.SS1.p5.m5" class="ltx_math_unparsed" alttext="\pi_{f(\theta)}(\cdot\,|\,s_{t})" display="inline"><semantics><mrow><msub><mi>π</mi><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow></mrow></msub><mrow><mo stretchy="false">(</mo><mo lspace="0em">⋅</mo><mo fence="false" rspace="0.337em" stretchy="false">|</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\pi_{f(\theta)}(\cdot\,|\,s_{t})</annotation></semantics></math> where <math id="S2.SS1.p5.m6" class="ltx_Math" alttext="\pi_{f(\theta)}:\mathcal{S}\rightarrow\Delta(\mathcal{A})" display="inline"><semantics><mrow><msub><mi>π</mi><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow></mrow></msub><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒮</mi><mo stretchy="false">→</mo><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi class="ltx_font_mathcaligraphic">𝒜</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\pi_{f(\theta)}:\mathcal{S}\rightarrow\Delta(\mathcal{A})</annotation></semantics></math> defines the action distribution given a state. The agent’s policy consists of <math id="S2.SS1.p5.m7" class="ltx_Math" alttext="\theta" display="inline"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math>, which denotes the base language model’s parameters, and an agent-specific function <math id="S2.SS1.p5.m8" class="ltx_Math" alttext="f" display="inline"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math>, which denotes how the base LLM policy is utilized (e.g., by applying different prompts to generate actions). If <math id="S2.SS1.p5.m9" class="ltx_Math" alttext="a_{t}\in\mathcal{A}_{q}" display="inline"><semantics><mrow><msub><mi>a</mi><mi>t</mi></msub><mo>∈</mo><msub><mi class="ltx_font_mathcaligraphic">𝒜</mi><mi>q</mi></msub></mrow><annotation encoding="application/x-tex">a_{t}\in\mathcal{A}_{q}</annotation></semantics></math>, we reinterpret it as <math id="S2.SS1.p5.m10" class="ltx_Math" alttext="q_{t}" display="inline"><semantics><msub><mi>q</mi><mi>t</mi></msub><annotation encoding="application/x-tex">q_{t}</annotation></semantics></math>, perform the query, and get document <math id="S2.SS1.p5.m11" class="ltx_Math" alttext="{D}_{t}=\text{IR}(q_{t})" display="inline"><semantics><mrow><msub><mi>D</mi><mi>t</mi></msub><mo>=</mo><mrow><mtext>IR</mtext><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>q</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">{D}_{t}=\text{IR}(q_{t})</annotation></semantics></math>. The history is then updated to <math id="S2.SS1.p5.m12" class="ltx_Math" alttext="\mathcal{H}_{t+1}" display="inline"><semantics><msub><mi class="ltx_font_mathcaligraphic">ℋ</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><annotation encoding="application/x-tex">\mathcal{H}_{t+1}</annotation></semantics></math> by appending <math id="S2.SS1.p5.m13" class="ltx_Math" alttext="(q_{t},{D}_{t})" display="inline"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>q</mi><mi>t</mi></msub><mo>,</mo><msub><mi>D</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(q_{t},{D}_{t})</annotation></semantics></math>, and the state transitions to <math id="S2.SS1.p5.m14" class="ltx_Math" alttext="s_{t+1}=(\mathcal{Q},\mathcal{H}_{t+1})" display="inline"><semantics><mrow><msub><mi>s</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mrow><mo stretchy="false">(</mo><mi class="ltx_font_mathcaligraphic">𝒬</mi><mo>,</mo><msub><mi class="ltx_font_mathcaligraphic">ℋ</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">s_{t+1}=(\mathcal{Q},\mathcal{H}_{t+1})</annotation></semantics></math>. Otherwise, if <math id="S2.SS1.p5.m15" class="ltx_Math" alttext="a_{t}\in\mathcal{A}_{p}" display="inline"><semantics><mrow><msub><mi>a</mi><mi>t</mi></msub><mo>∈</mo><msub><mi class="ltx_font_mathcaligraphic">𝒜</mi><mi>p</mi></msub></mrow><annotation encoding="application/x-tex">a_{t}\in\mathcal{A}_{p}</annotation></semantics></math>, the current episode is considered complete, and the MDP terminates.</p>
</div>
<div id="S2.SS1.p6" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Rewards.</span> For the outer MDP, the reward of an episode is determined by the correctness of the final prediction. The immediate reward for a state-action pair <math id="S2.SS1.p6.m1" class="ltx_Math" alttext="(s_{t},a_{t})" display="inline"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(s_{t},a_{t})</annotation></semantics></math> is:</p>
<table id="S2.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E3.m1" class="ltx_Math" alttext="\small\mathcal{R}(s_{t},a_{t})=\begin{cases}0,&amp;\text{if }a_{t}\in\mathcal{A}_{q}\\
\mathcal{F}(a_{t},\mathtt{g}(\mathcal{Q})),&amp;\text{if }a_{t}\in\mathcal{A}_{p}.\end{cases}" display="block"><semantics><mrow><mrow><mi class="ltx_font_mathcaligraphic" mathsize="0.900em">ℛ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="0.900em" minsize="0.900em">(</mo><msub><mi mathsize="0.900em">s</mi><mi mathsize="0.900em">t</mi></msub><mo mathsize="0.900em">,</mo><msub><mi mathsize="0.900em">a</mi><mi mathsize="0.900em">t</mi></msub><mo maxsize="0.900em" minsize="0.900em">)</mo></mrow></mrow><mo mathsize="0.900em">=</mo><mrow><mo>{</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mtr><mtd class="ltx_align_left" columnalign="left"><mrow><mn mathsize="0.900em">0</mn><mo mathsize="0.900em">,</mo></mrow></mtd><mtd class="ltx_align_left" columnalign="left"><mrow><mrow><mtext mathsize="0.900em">if </mtext><mo lspace="0em" rspace="0em">​</mo><msub><mi mathsize="0.900em">a</mi><mi mathsize="0.900em">t</mi></msub></mrow><mo mathsize="0.900em">∈</mo><msub><mi class="ltx_font_mathcaligraphic" mathsize="0.900em">𝒜</mi><mi mathsize="0.900em">q</mi></msub></mrow></mtd></mtr><mtr><mtd class="ltx_align_left" columnalign="left"><mrow><mrow><mi class="ltx_font_mathcaligraphic" mathsize="0.900em">ℱ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="0.900em" minsize="0.900em">(</mo><msub><mi mathsize="0.900em">a</mi><mi mathsize="0.900em">t</mi></msub><mo mathsize="0.900em">,</mo><mrow><mi mathsize="0.900em">𝚐</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="0.900em" minsize="0.900em">(</mo><mi class="ltx_font_mathcaligraphic" mathsize="0.900em">𝒬</mi><mo maxsize="0.900em" minsize="0.900em">)</mo></mrow></mrow><mo maxsize="0.900em" minsize="0.900em">)</mo></mrow></mrow><mo mathsize="0.900em">,</mo></mrow></mtd><mtd class="ltx_align_left" columnalign="left"><mrow><mrow><mrow><mtext mathsize="0.900em">if </mtext><mo lspace="0em" rspace="0em">​</mo><msub><mi mathsize="0.900em">a</mi><mi mathsize="0.900em">t</mi></msub></mrow><mo mathsize="0.900em">∈</mo><msub><mi class="ltx_font_mathcaligraphic" mathsize="0.900em">𝒜</mi><mi mathsize="0.900em">p</mi></msub></mrow><mo lspace="0em" mathsize="0.900em">.</mo></mrow></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex">\small\mathcal{R}(s_{t},a_{t})=\begin{cases}0,&amp;\text{if }a_{t}\in\mathcal{A}_{q}\\
\mathcal{F}(a_{t},\mathtt{g}(\mathcal{Q})),&amp;\text{if }a_{t}\in\mathcal{A}_{p}.\end{cases}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Here, <math id="S2.SS1.p6.m2" class="ltx_Math" alttext="\mathtt{g}(\mathcal{Q})" display="inline"><semantics><mrow><mi>𝚐</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi class="ltx_font_mathcaligraphic">𝒬</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathtt{g}(\mathcal{Q})</annotation></semantics></math> is the ground-truth answer for the original question <math id="S2.SS1.p6.m3" class="ltx_Math" alttext="\mathcal{Q}" display="inline"><semantics><mi class="ltx_font_mathcaligraphic">𝒬</mi><annotation encoding="application/x-tex">\mathcal{Q}</annotation></semantics></math>, and <math id="S2.SS1.p6.m4" class="ltx_Math" alttext="\mathcal{F}" display="inline"><semantics><mi class="ltx_font_mathcaligraphic">ℱ</mi><annotation encoding="application/x-tex">\mathcal{F}</annotation></semantics></math> is the evaluation metric, such as accuracy.
The objective for the agent in the outer MDP is to maximize the expected cumulative reward over a trajectory:</p>
<table id="S2.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E4.m1" class="ltx_Math" alttext="\small\text{maximize}_{\theta}\quad\mathbb{E}_{\pi_{f(\theta)}}\left[\sum_{t=1}^{T}\gamma^{t-1}\mathcal{R}(s_{t},a_{t})\right]" display="block"><semantics><mrow><msub><mtext mathsize="0.900em">maximize</mtext><mi mathsize="0.900em">θ</mi></msub><mo lspace="0.900em" rspace="0em">​</mo><msub><mi mathsize="0.900em">𝔼</mi><msub><mi mathsize="0.900em">π</mi><mrow><mi mathsize="0.900em">f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="0.900em" minsize="0.900em">(</mo><mi mathsize="0.900em">θ</mi><mo maxsize="0.900em" minsize="0.900em">)</mo></mrow></mrow></msub></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><mrow><munderover><mo lspace="0em" maxsize="0.900em" minsize="0.900em" movablelimits="false" stretchy="true">∑</mo><mrow><mi mathsize="0.900em">t</mi><mo mathsize="0.900em">=</mo><mn mathsize="0.900em">1</mn></mrow><mi mathsize="0.900em">T</mi></munderover><mrow><msup><mi mathsize="0.900em">γ</mi><mrow><mi mathsize="0.900em">t</mi><mo mathsize="0.900em">−</mo><mn mathsize="0.900em">1</mn></mrow></msup><mo lspace="0em" rspace="0em">​</mo><mi class="ltx_font_mathcaligraphic" mathsize="0.900em">ℛ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="0.900em" minsize="0.900em">(</mo><msub><mi mathsize="0.900em">s</mi><mi mathsize="0.900em">t</mi></msub><mo mathsize="0.900em">,</mo><msub><mi mathsize="0.900em">a</mi><mi mathsize="0.900em">t</mi></msub><mo maxsize="0.900em" minsize="0.900em">)</mo></mrow></mrow></mrow><mo>]</mo></mrow></mrow><annotation encoding="application/x-tex">\small\text{maximize}_{\theta}\quad\mathbb{E}_{\pi_{f(\theta)}}\left[\sum_{t=1}^{T}\gamma^{t-1}\mathcal{R}(s_{t},a_{t})\right]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math id="S2.SS1.p6.m5" class="ltx_Math" alttext="T" display="inline"><semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics></math> is the time step at termination, and <math id="S2.SS1.p6.m6" class="ltx_Math" alttext="\gamma\in[0,1]" display="inline"><semantics><mrow><mi>γ</mi><mo>∈</mo><mrow><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\gamma\in[0,1]</annotation></semantics></math> is the discount factor that penalizes longer trajectories.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Improving Search Agents with Process Supervision</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p class="ltx_p">While outcome-based rewards, such as answer correctness, provide a clear optimization objective, they offer limited insight into the quality of intermediate reasoning and search steps. In RAG-Gym, a poorly formulated query may still lead to a correct answer if the language agent compensates with internal knowledge, making it difficult to assess the effectiveness of the search query. To address this, process supervision is essential. Recent advancements in process reward modeling <cite class="ltx_cite ltx_citemacro_citep">(Lightman et al., <a href="#bib.bib26" title="" class="ltx_ref">2024</a>; Wang et al., <a href="#bib.bib48" title="" class="ltx_ref">2024a</a>)</cite> show that guiding intermediate steps enhances both performance and robustness in language agents. Since outer MDP actions in RAG-Gym are discrete, optimizing agents solely through outcome rewards is challenging. By incorporating process rewards, RAG-Gym enables more effective LLM tuning, aligning token generation with high-quality search behavior. Section <a href="#S2.SS2.SSS1" title="2.2.1 Collection of Process Reward Data ‣ 2.2 Improving Search Agents with Process Supervision ‣ 2 RAG-Gymnasium (RAG-Gym) ‣ RAG-Gym: Optimizing Reasoning and Search Agents with Process Supervision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2.1</span></a> details our process reward data collection pipeline, and Section <a href="#S2.SS2.SSS2" title="2.2.2 Tuning Agents with Process Supervision ‣ 2.2 Improving Search Agents with Process Supervision ‣ 2 RAG-Gymnasium (RAG-Gym) ‣ RAG-Gym: Optimizing Reasoning and Search Agents with Process Supervision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2.2</span></a> describes the algorithms for tuning language agents with process supervision.</p>
</div>
<section id="S2.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.1 </span>Collection of Process Reward Data</h4>

<div id="S2.SS2.SSS1.p1" class="ltx_para">
<p class="ltx_p">To evaluate intermediate reasoning and search steps in RAG-Gym, we design a process reward function that assesses queries based on three key criteria:</p>
<ul id="S2.I1" class="ltx_itemize">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p class="ltx_p">Sufficiency: A query must first be necessary. If the retrieval history already contains sufficient information, answering should be the preferred action instead of searching further.</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p class="ltx_p">Utility: Queries should also be precise, actionable, and foundational to solving the question while avoiding unnecessary details.</p>
</div>
</li>
<li id="S2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i3.p1" class="ltx_para">
<p class="ltx_p">Redundancy: Queries should introduce new, useful information rather than repeating past searches.</p>
</div>
</li>
</ul>
<p class="ltx_p">These criteria ensure that queries are efficient, targeted, and contribute meaningfully to constructing the final answer.</p>
</div>
<div id="S2.SS2.SSS1.p2" class="ltx_para">
<p class="ltx_p">The data collection pipeline begins with trajectory sampling, where the language agent generates a sequence of actions based on its current policy. At each step in the trajectory, multiple candidate actions are proposed, and the best action is selected according to predefined evaluation criteria. To streamline the annotation process and ensure consistency, we employ a ranking-based evaluation framework rather than assigning numerical scores. The selected action is then executed, and the trajectory transitions to the next state. This process is repeated iteratively until the trajectory terminates.</p>
</div>
<div id="S2.SS2.SSS1.p3" class="ltx_para">
<p class="ltx_p">To ensure quality, only trajectories that result in a correct final answer are retained, as determined by the outcome reward. This filtering guarantees that the selected actions not only align with the process reward criteria but also contribute to successful task completion. To address the challenges of slow and costly human annotation, we leverage LLMs such as GPT-4o to annotate the sampled trajectories. As demonstrated in our experiments (Table <a href="#S5.T2" title="Table 2 ‣ 5.1 Comparison of Different Reward Sources ‣ 5 Analysis and Discussion ‣ RAG-Gym: Optimizing Reasoning and Search Agents with Process Supervision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>), annotations generated by GPT-4o exhibit high reliability, closely aligning with domain expert judgments. This approach enables scalable and efficient data collection, making it feasible to gather high-quality process reward data at scale.</p>
</div>
</section>
<section id="S2.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.2 </span>Tuning Agents with Process Supervision</h4>

<div id="S2.SS2.SSS2.p1" class="ltx_para">
<p class="ltx_p">The process reward data collected serves as a key resource for improving language agents in RAG-Gym through three distinct methods: supervised fine-tuning, direct preference optimization <cite class="ltx_cite ltx_citemacro_citep">(Rafailov et al., <a href="#bib.bib30" title="" class="ltx_ref">2024</a>)</cite>, and process reward modeling. Each method leverages the data to address specific aspects of the training paradigm and task requirements, enabling robust optimization of reasoning and action generation.</p>
</div>
<div id="S2.SS2.SSS2.p2" class="ltx_para">
<p class="ltx_p">In <span class="ltx_text ltx_font_italic">supervised fine-tuning (SFT)</span>, selected actions from the process rewards are used to train the language agent. Formally, the goal of SFT is to minimize the negative log-likelihood of the selected actions given their states:</p>
<table id="S2.E5" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E5.m1" class="ltx_Math" alttext="\mathcal{L}_{\text{SFT}}(\theta)=-\mathbb{E}_{(s_{t},a_{t}^{+})\sim\mathfrak{D}}\left[\log\pi_{f(\theta)}(a_{t}^{+}|s_{t})\right]," display="block"><semantics><mrow><mrow><mrow><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mtext>SFT</mtext></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mo>−</mo><mrow><msub><mi>𝔼</mi><mrow><mrow><mo stretchy="false">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msubsup><mi>a</mi><mi>t</mi><mo>+</mo></msubsup><mo stretchy="false">)</mo></mrow><mo>∼</mo><mi>𝔇</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><mrow><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><msub><mi>π</mi><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow></mrow></msub></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>a</mi><mi>t</mi><mo>+</mo></msubsup><mo fence="false">|</mo><msub><mi>s</mi><mi>t</mi></msub></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>]</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\mathcal{L}_{\text{SFT}}(\theta)=-\mathbb{E}_{(s_{t},a_{t}^{+})\sim\mathfrak{D}}\left[\log\pi_{f(\theta)}(a_{t}^{+}|s_{t})\right],</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math id="S2.SS2.SSS2.p2.m1" class="ltx_Math" alttext="\mathfrak{D}" display="inline"><semantics><mi>𝔇</mi><annotation encoding="application/x-tex">\mathfrak{D}</annotation></semantics></math> is the dataset of process reward-labeled state-action pairs. This method provides a straightforward way to incorporate process supervision but does not explicitly account for unselected actions, potentially limiting its ability to distinguish between subtle preferences.</p>
</div>
<div id="S2.SS2.SSS2.p3" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Direct preference optimization (DPO)</span> introduces a contrastive learning framework that incorporates both selected and unselected actions. The process reward data is reformulated into preference pairs <math id="S2.SS2.SSS2.p3.m1" class="ltx_Math" alttext="(a_{t}^{+},a_{t}^{-})" display="inline"><semantics><mrow><mo stretchy="false">(</mo><msubsup><mi>a</mi><mi>t</mi><mo>+</mo></msubsup><mo>,</mo><msubsup><mi>a</mi><mi>t</mi><mo>−</mo></msubsup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(a_{t}^{+},a_{t}^{-})</annotation></semantics></math>, where <math id="S2.SS2.SSS2.p3.m2" class="ltx_Math" alttext="a_{t}^{+}" display="inline"><semantics><msubsup><mi>a</mi><mi>t</mi><mo>+</mo></msubsup><annotation encoding="application/x-tex">a_{t}^{+}</annotation></semantics></math> is the preferred action and <math id="S2.SS2.SSS2.p3.m3" class="ltx_Math" alttext="a_{t}^{-}" display="inline"><semantics><msubsup><mi>a</mi><mi>t</mi><mo>−</mo></msubsup><annotation encoding="application/x-tex">a_{t}^{-}</annotation></semantics></math> is the less-preferred alternative for <math id="S2.SS2.SSS2.p3.m4" class="ltx_Math" alttext="s_{t}" display="inline"><semantics><msub><mi>s</mi><mi>t</mi></msub><annotation encoding="application/x-tex">s_{t}</annotation></semantics></math>. The DPO objective minimizes the following loss:</p>
<table id="S2.E6" class="ltx_equationgroup ltx_eqn_table">
<tbody>
<tr id="S2.E6X" class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S2.E6X.m2" class="ltx_math_unparsed" alttext="\displaystyle\mathcal{L}_{\text{DPO}}(\theta)=-\mathbb{E}_{(s_{t},a_{t}^{+},a_{t}^{-})\sim\mathfrak{D}}\Bigg{[}\log\sigma" display="inline"><semantics><mrow><msub><mi class="ltx_font_mathcaligraphic" mathsize="0.700em">ℒ</mi><mtext mathsize="0.700em">DPO</mtext></msub><mrow><mo maxsize="0.700em" minsize="0.700em">(</mo><mi mathsize="0.700em">θ</mi><mo maxsize="0.700em" minsize="0.700em">)</mo></mrow><mo mathsize="0.700em" rspace="0em">=</mo><mo lspace="0em" mathsize="0.700em">−</mo><msub><mi mathsize="0.700em">𝔼</mi><mrow><mrow><mo maxsize="0.700em" minsize="0.700em">(</mo><msub><mi mathsize="0.700em">s</mi><mi mathsize="0.700em">t</mi></msub><mo mathsize="0.700em">,</mo><msubsup><mi mathsize="0.700em">a</mi><mi mathsize="0.700em">t</mi><mo mathsize="0.700em">+</mo></msubsup><mo mathsize="0.700em">,</mo><msubsup><mi mathsize="0.700em">a</mi><mi mathsize="0.700em">t</mi><mo mathsize="0.700em">−</mo></msubsup><mo maxsize="0.700em" minsize="0.700em">)</mo></mrow><mo mathsize="0.700em">∼</mo><mi mathsize="0.700em">𝔇</mi></mrow></msub><mrow><mo maxsize="2.600em" minsize="2.600em">[</mo><mi mathsize="0.700em">log</mi><mi mathsize="0.700em">σ</mi></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\mathcal{L}_{\text{DPO}}(\theta)=-\mathbb{E}_{(s_{t},a_{t}^{+},a_{t}^{-})\sim\mathfrak{D}}\Bigg{[}\log\sigma</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S2.E6X.m3" class="ltx_math_unparsed" alttext="\displaystyle\Big{(}\beta\log\frac{\pi_{f(\theta)}(a_{t}^{+}|s_{t})}{\pi_{\text{ref}}(a_{t}^{+}|s_{t})}" display="inline"><semantics><mrow><mo maxsize="1.600em" minsize="1.600em">(</mo><mi mathsize="0.700em">β</mi><mi mathsize="0.700em">log</mi><mstyle displaystyle="true"><mfrac><mrow><msub><mi mathsize="0.700em">π</mi><mrow><mi mathsize="0.700em">f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="0.700em" minsize="0.700em">(</mo><mi mathsize="0.700em">θ</mi><mo maxsize="0.700em" minsize="0.700em">)</mo></mrow></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="0.700em" minsize="0.700em">(</mo><mrow><msubsup><mi mathsize="0.700em">a</mi><mi mathsize="0.700em">t</mi><mo mathsize="0.700em">+</mo></msubsup><mo fence="false" mathsize="0.700em">|</mo><msub><mi mathsize="0.700em">s</mi><mi mathsize="0.700em">t</mi></msub></mrow><mo maxsize="0.700em" minsize="0.700em">)</mo></mrow></mrow><mrow><msub><mi mathsize="0.700em">π</mi><mtext mathsize="0.700em">ref</mtext></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="0.700em" minsize="0.700em">(</mo><mrow><msubsup><mi mathsize="0.700em">a</mi><mi mathsize="0.700em">t</mi><mo mathsize="0.700em">+</mo></msubsup><mo fence="false" mathsize="0.700em">|</mo><msub><mi mathsize="0.700em">s</mi><mi mathsize="0.700em">t</mi></msub></mrow><mo maxsize="0.700em" minsize="0.700em">)</mo></mrow></mrow></mfrac></mstyle></mrow><annotation encoding="application/x-tex">\displaystyle\Big{(}\beta\log\frac{\pi_{f(\theta)}(a_{t}^{+}|s_{t})}{\pi_{\text{ref}}(a_{t}^{+}|s_{t})}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="2" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(6)</span></td>
</tr>
<tr id="S2.E6Xa" class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S2.E6Xa.m2" class="ltx_math_unparsed" alttext="\displaystyle-\beta\log\frac{\pi_{f(\theta)}(a_{t}^{-}|s_{t})}{\pi_{\text{ref}}(a_{t}^{-}|s_{t})}\Big{)}\Bigg{]}," display="inline"><semantics><mrow><mrow><mo mathsize="0.700em">−</mo><mi mathsize="0.700em">β</mi><mi mathsize="0.700em">log</mi><mstyle displaystyle="true"><mfrac><mrow><msub><mi mathsize="0.700em">π</mi><mrow><mi mathsize="0.700em">f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="0.700em" minsize="0.700em">(</mo><mi mathsize="0.700em">θ</mi><mo maxsize="0.700em" minsize="0.700em">)</mo></mrow></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="0.700em" minsize="0.700em">(</mo><mrow><msubsup><mi mathsize="0.700em">a</mi><mi mathsize="0.700em">t</mi><mo mathsize="0.700em">−</mo></msubsup><mo fence="false" mathsize="0.700em">|</mo><msub><mi mathsize="0.700em">s</mi><mi mathsize="0.700em">t</mi></msub></mrow><mo maxsize="0.700em" minsize="0.700em">)</mo></mrow></mrow><mrow><msub><mi mathsize="0.700em">π</mi><mtext mathsize="0.700em">ref</mtext></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="0.700em" minsize="0.700em">(</mo><mrow><msubsup><mi mathsize="0.700em">a</mi><mi mathsize="0.700em">t</mi><mo mathsize="0.700em">−</mo></msubsup><mo fence="false" mathsize="0.700em">|</mo><msub><mi mathsize="0.700em">s</mi><mi mathsize="0.700em">t</mi></msub></mrow><mo maxsize="0.700em" minsize="0.700em">)</mo></mrow></mrow></mfrac></mstyle><mo maxsize="1.600em" minsize="1.600em">)</mo></mrow><mo maxsize="2.600em" minsize="2.600em">]</mo><mo mathsize="0.700em">,</mo></mrow><annotation encoding="application/x-tex">\displaystyle-\beta\log\frac{\pi_{f(\theta)}(a_{t}^{-}|s_{t})}{\pi_{\text{ref}}(a_{t}^{-}|s_{t})}\Big{)}\Bigg{]},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</tbody>
</table>
<p class="ltx_p">where <math id="S2.SS2.SSS2.p3.m5" class="ltx_Math" alttext="\pi_{f(\theta)}" display="inline"><semantics><msub><mi>π</mi><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow></mrow></msub><annotation encoding="application/x-tex">\pi_{f(\theta)}</annotation></semantics></math> is the policy being optimized, <math id="S2.SS2.SSS2.p3.m6" class="ltx_Math" alttext="\pi_{\text{ref}}" display="inline"><semantics><msub><mi>π</mi><mtext>ref</mtext></msub><annotation encoding="application/x-tex">\pi_{\text{ref}}</annotation></semantics></math> is the reference policy, <math id="S2.SS2.SSS2.p3.m7" class="ltx_Math" alttext="\beta" display="inline"><semantics><mi>β</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math> is a temperature parameter controlling the strength of the preference weighting, and <math id="S2.SS2.SSS2.p3.m8" class="ltx_Math" alttext="\sigma(\cdot)" display="inline"><semantics><mrow><mi>σ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\sigma(\cdot)</annotation></semantics></math> is the sigmoid function. By explicitly comparing actions, DPO captures nuanced preferences and enables the agent to learn from both positive and negative feedback.</p>
</div>
<div id="S2.SS2.SSS2.p4" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Process reward modeling (PRM)</span> takes a different approach by training a separate reward model <math id="S2.SS2.SSS2.p4.m1" class="ltx_Math" alttext="r_{\phi}(s_{t},a_{t})" display="inline"><semantics><mrow><msub><mi>r</mi><mi>ϕ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">r_{\phi}(s_{t},a_{t})</annotation></semantics></math> to predict process rewards based on the collected data. The objective is to minimize a contrastive loss that evaluates the quality of preferred actions relative to less-preferred actions:</p>
<table id="S2.E7" class="ltx_equationgroup ltx_eqn_table">
<tbody>
<tr id="S2.E7X" class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S2.E7X.m2" class="ltx_Math" alttext="\displaystyle\mathcal{L}_{\text{PRM}}(\phi)=-\mathbb{E}_{(s_{t},a_{t}^{+},a_{t}^{-})\sim\mathfrak{D}}\Big{[}\log\sigma\big{(}r_{\phi}(s_{t},a_{t}^{+})-r_{\phi}(s_{t},a_{t}^{-})\big{)}\Big{]}." display="inline"><semantics><mrow><mrow><mrow><msub><mi class="ltx_font_mathcaligraphic" mathsize="0.700em">ℒ</mi><mtext mathsize="0.700em">PRM</mtext></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="0.700em" minsize="0.700em">(</mo><mi mathsize="0.700em">ϕ</mi><mo maxsize="0.700em" minsize="0.700em">)</mo></mrow></mrow><mo mathsize="0.700em">=</mo><mrow><mo mathsize="0.700em">−</mo><mrow><msub><mi mathsize="0.700em">𝔼</mi><mrow><mrow><mo maxsize="0.700em" minsize="0.700em">(</mo><msub><mi mathsize="0.700em">s</mi><mi mathsize="0.700em">t</mi></msub><mo mathsize="0.700em">,</mo><msubsup><mi mathsize="0.700em">a</mi><mi mathsize="0.700em">t</mi><mo mathsize="0.700em">+</mo></msubsup><mo mathsize="0.700em">,</mo><msubsup><mi mathsize="0.700em">a</mi><mi mathsize="0.700em">t</mi><mo mathsize="0.700em">−</mo></msubsup><mo maxsize="0.700em" minsize="0.700em">)</mo></mrow><mo mathsize="0.700em">∼</mo><mi mathsize="0.700em">𝔇</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="1.600em" minsize="1.600em">[</mo><mrow><mrow><mi mathsize="0.700em">log</mi><mo lspace="0.167em">⁡</mo><mi mathsize="0.700em">σ</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="1.200em" minsize="1.200em">(</mo><mrow><mrow><msub><mi mathsize="0.700em">r</mi><mi mathsize="0.700em">ϕ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="0.700em" minsize="0.700em">(</mo><msub><mi mathsize="0.700em">s</mi><mi mathsize="0.700em">t</mi></msub><mo mathsize="0.700em">,</mo><msubsup><mi mathsize="0.700em">a</mi><mi mathsize="0.700em">t</mi><mo mathsize="0.700em">+</mo></msubsup><mo maxsize="0.700em" minsize="0.700em">)</mo></mrow></mrow><mo mathsize="0.700em">−</mo><mrow><msub><mi mathsize="0.700em">r</mi><mi mathsize="0.700em">ϕ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="0.700em" minsize="0.700em">(</mo><msub><mi mathsize="0.700em">s</mi><mi mathsize="0.700em">t</mi></msub><mo mathsize="0.700em">,</mo><msubsup><mi mathsize="0.700em">a</mi><mi mathsize="0.700em">t</mi><mo mathsize="0.700em">−</mo></msubsup><mo maxsize="0.700em" minsize="0.700em">)</mo></mrow></mrow></mrow><mo maxsize="1.200em" minsize="1.200em">)</mo></mrow></mrow><mo maxsize="1.600em" minsize="1.600em">]</mo></mrow></mrow></mrow></mrow><mo lspace="0em" mathsize="0.700em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle\mathcal{L}_{\text{PRM}}(\phi)=-\mathbb{E}_{(s_{t},a_{t}^{+},a_{t}^{-})\sim\mathfrak{D}}\Big{[}\log\sigma\big{(}r_{\phi}(s_{t},a_{t}^{+})-r_{\phi}(s_{t},a_{t}^{-})\big{)}\Big{]}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(7)</span></td>
</tr>
</tbody>
</table>
<p class="ltx_p">Unlike SFT and DPO, PRM does not directly tune the policy <math id="S2.SS2.SSS2.p4.m2" class="ltx_Math" alttext="\pi_{f(\theta)}" display="inline"><semantics><msub><mi>π</mi><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow></mrow></msub><annotation encoding="application/x-tex">\pi_{f(\theta)}</annotation></semantics></math> but instead trains the reward model <math id="S2.SS2.SSS2.p4.m3" class="ltx_Math" alttext="r_{\phi}" display="inline"><semantics><msub><mi>r</mi><mi>ϕ</mi></msub><annotation encoding="application/x-tex">r_{\phi}</annotation></semantics></math> parameterized by <math id="S2.SS2.SSS2.p4.m4" class="ltx_Math" alttext="\phi" display="inline"><semantics><mi>ϕ</mi><annotation encoding="application/x-tex">\phi</annotation></semantics></math> to estimate the quality of intermediate reasoning and actions. The reward model can then guide decision-making by selecting high-quality actions during inference, eliminating the need for agent fine-tuning. This makes PRM especially useful for large-scale or proprietary models, offering a flexible and scalable approach to improving reasoning and search.
Algorithm <a href="#alg1" title="Algorithm 1 ‣ Appendix D Implementation Details ‣ RAG-Gym: Optimizing Reasoning and Search Agents with Process Supervision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> details how trained process reward models are applied during inference.</p>
</div>
</section>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Reasoning and Search (ReSearch) Agent</h2>

<div id="S3.p1" class="ltx_para">
<p class="ltx_p">Existing agent architectures, such as ReAct <cite class="ltx_cite ltx_citemacro_citep">(Yao et al., <a href="#bib.bib61" title="" class="ltx_ref">2023</a>)</cite>, exhibit inherent limitations when applied to knowledge-intensive tasks. They can only reason about either the answer generation or the query generation at each step. This may lead to queries that cannot directly contribute to answer construction, as these two tasks are not necessarily aligned during reasoning. As illustrated in Appendix <a href="#A5" title="Appendix E Case Studies ‣ RAG-Gym: Optimizing Reasoning and Search Agents with Process Supervision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">E</span></a>, existing designs may generate queries that fail to retrieve the most relevant evidence, leading to suboptimal information acquisition and degrading answer quality. This limitation underscores the necessity of an architecture that explicitly aligns search queries with answer construction.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2502.13957/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="436" height="201" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Architecture of ReSearch, which synergizes <span class="ltx_text ltx_framed ltx_framed_underline">Re</span>asoning and <span class="ltx_text ltx_framed ltx_framed_underline">Search</span> by integrating history knowledge summarization, answer reasoning, and query generation to iteratively resolve missing information in constructing the final answer.</figcaption>
</figure>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 1: </span>Performance of different agents implemented with Llama-3.1-8B-Instruct. The highest scores in each column are bolded.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_tt" rowspan="2"><span class="ltx_text ltx_font_bold" style="font-size:90%;">Method</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" rowspan="2"><span class="ltx_text ltx_font_bold" style="font-size:90%;">Agent</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2"><span class="ltx_text ltx_font_bold" style="font-size:90%;">HotpotQA</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2"><span class="ltx_text ltx_font_bold" style="font-size:90%;">2Wiki</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2"><span class="ltx_text ltx_font_bold" style="font-size:90%;">Bamboogle</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt"><span class="ltx_text ltx_font_bold" style="font-size:90%;">MedQA</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2"><span class="ltx_text ltx_font_bold" style="font-size:90%;">Average</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">EM</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">F1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">EM</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">F1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">EM</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">F1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">Acc</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">EM</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">F1</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_t" rowspan="6"><span class="ltx_text" style="font-size:90%;"><span class="ltx_text ltx_font_italic"> 
<span class="ltx_tabular ltx_align_middle">
<span class="ltx_tr">
<span class="ltx_td ltx_nopad_r ltx_align_center">Zero-shot</span></span>
<span class="ltx_tr">
<span class="ltx_td ltx_nopad_r ltx_align_center">Learning</span></span>
</span> </span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">Direct</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">21.10</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">27.93</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">24.10</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">27.68</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">9.60</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">14.89</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">61.82</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">29.16</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">33.08</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">CoT</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">27.10</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">35.17</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">25.70</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">30.08</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">37.60</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">49.50</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">69.60</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">40.00</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">46.09</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">RAG</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">38.30</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">48.57</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">32.00</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">36.91</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">22.40</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">33.73</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">66.85</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">39.89</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">46.51</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">ReAct</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">30.70</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">41.09</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">28.90</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">35.03</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">32.00</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">41.35</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">62.37</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">38.49</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">44.96</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">Search-o1</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">35.30</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">47.33</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">34.00</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">41.29</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">44.80</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">52.50</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">66.14</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">45.06</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">51.82</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center" style="background-color:#D9FFFF;"><span class="ltx_text" style="font-size:90%;background-color:#D9FFFF;">ReSearch</span></td>
<td class="ltx_td ltx_align_center" style="background-color:#D9FFFF;"><span class="ltx_text" style="font-size:90%;background-color:#D9FFFF;">34.00</span></td>
<td class="ltx_td ltx_align_center" style="background-color:#D9FFFF;"><span class="ltx_text" style="font-size:90%;background-color:#D9FFFF;">43.72</span></td>
<td class="ltx_td ltx_align_center" style="background-color:#D9FFFF;"><span class="ltx_text" style="font-size:90%;background-color:#D9FFFF;">41.00</span></td>
<td class="ltx_td ltx_align_center" style="background-color:#D9FFFF;"><span class="ltx_text" style="font-size:90%;background-color:#D9FFFF;">47.48</span></td>
<td class="ltx_td ltx_align_center" style="background-color:#D9FFFF;"><span class="ltx_text" style="font-size:90%;background-color:#D9FFFF;">50.40</span></td>
<td class="ltx_td ltx_align_center" style="background-color:#D9FFFF;"><span class="ltx_text" style="font-size:90%;background-color:#D9FFFF;">57.51</span></td>
<td class="ltx_td ltx_align_center" style="background-color:#D9FFFF;"><span class="ltx_text" style="font-size:90%;background-color:#D9FFFF;">70.23</span></td>
<td class="ltx_td ltx_align_center" style="background-color:#D9FFFF;"><span class="ltx_text" style="font-size:90%;background-color:#D9FFFF;">48.91</span></td>
<td class="ltx_td ltx_align_center" style="background-color:#D9FFFF;"><span class="ltx_text" style="font-size:90%;background-color:#D9FFFF;">54.74</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_t" rowspan="6"><span class="ltx_text" style="font-size:90%;"> 
<span class="ltx_tabular ltx_align_middle">
<span class="ltx_tr">
<span class="ltx_td ltx_nopad_r ltx_align_center"><svg id="S3.T1.pic1" class="ltx_picture" height="20.19" overflow="visible" version="1.1" width="69.45"><g transform="translate(0,20.19) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#CCFFCC" fill-opacity="1.0"><path d="M 0 6.92 L 0 13.27 C 0 17.09 3.1 20.19 6.92 20.19 L 62.53 20.19 C 66.35 20.19 69.45 17.09 69.45 13.27 L 69.45 6.92 C 69.45 3.1 66.35 0 62.53 0 L 6.92 0 C 3.1 0 0 3.1 0 6.92 Z" style="stroke:none"></path></g><g fill="#CCFFCC" fill-opacity="1.0"><path d="M 0 6.92 L 0 13.27 C 0 17.09 3.1 20.19 6.92 20.19 L 62.53 20.19 C 66.35 20.19 69.45 17.09 69.45 13.27 L 69.45 6.92 C 69.45 3.1 66.35 0 62.53 0 L 6.92 0 C 3.1 0 0 3.1 0 6.92 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 5.32 7.05)"><text transform="matrix(1 0 0 -1 0 0)" color="#000000">RAG-Gym</text></g></g></svg></span></span>
<span class="ltx_tr">
<span class="ltx_td ltx_nopad_r ltx_align_center"><span class="ltx_text ltx_font_italic">Supervised</span></span></span>
<span class="ltx_tr">
<span class="ltx_td ltx_nopad_r ltx_align_center"><span class="ltx_text ltx_font_italic">Fine-tuning</span></span></span>
</span> </span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">Direct</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">22.80</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">31.67</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">28.00</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">33.17</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">20.00</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">27.21</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">63.63</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">33.61</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">38.92</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">CoT</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">26.50</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">35.60</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">27.30</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">32.10</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">42.40</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">53.89</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">69.68</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">41.47</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">47.82</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">RAG</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">41.50</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">52.26</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">38.00</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">42.74</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">28.80</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">40.76</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">67.79</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">44.02</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">50.89</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">ReAct</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">35.50</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">46.06</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">31.00</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">36.79</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">34.40</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">44.17</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">66.69</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">41.90</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">48.43</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">Search-o1</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">38.20</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">50.02</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">39.00</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">45.91</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">46.40</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">57.18</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">67.64</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">47.81</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">55.19</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center" style="background-color:#D9FFFF;"><span class="ltx_text" style="font-size:90%;background-color:#D9FFFF;">ReSearch</span></td>
<td class="ltx_td ltx_align_center" style="background-color:#D9FFFF;"><span class="ltx_text" style="font-size:90%;background-color:#D9FFFF;">37.60</span></td>
<td class="ltx_td ltx_align_center" style="background-color:#D9FFFF;"><span class="ltx_text" style="font-size:90%;background-color:#D9FFFF;">49.16</span></td>
<td class="ltx_td ltx_align_center" style="background-color:#D9FFFF;"><span class="ltx_text" style="font-size:90%;background-color:#D9FFFF;">44.00</span></td>
<td class="ltx_td ltx_align_center" style="background-color:#D9FFFF;"><span class="ltx_text" style="font-size:90%;background-color:#D9FFFF;">50.54</span></td>
<td class="ltx_td ltx_align_center" style="background-color:#D9FFFF;"><span class="ltx_text" style="font-size:90%;background-color:#D9FFFF;">44.80</span></td>
<td class="ltx_td ltx_align_center" style="background-color:#D9FFFF;"><span class="ltx_text" style="font-size:90%;background-color:#D9FFFF;">56.78</span></td>
<td class="ltx_td ltx_align_center" style="background-color:#D9FFFF;"><span class="ltx_text" style="font-size:90%;background-color:#D9FFFF;">69.52</span></td>
<td class="ltx_td ltx_align_center" style="background-color:#D9FFFF;"><span class="ltx_text" style="font-size:90%;background-color:#D9FFFF;">48.98</span></td>
<td class="ltx_td ltx_align_center" style="background-color:#D9FFFF;"><span class="ltx_text" style="font-size:90%;background-color:#D9FFFF;">56.50</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_t" rowspan="6"><span class="ltx_text" style="font-size:90%;"> 
<span class="ltx_tabular ltx_align_middle">
<span class="ltx_tr">
<span class="ltx_td ltx_nopad_r ltx_align_center"><svg id="S3.T1.pic2" class="ltx_picture" height="20.19" overflow="visible" version="1.1" width="69.45"><g transform="translate(0,20.19) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#CCFFCC" fill-opacity="1.0"><path d="M 0 6.92 L 0 13.27 C 0 17.09 3.1 20.19 6.92 20.19 L 62.53 20.19 C 66.35 20.19 69.45 17.09 69.45 13.27 L 69.45 6.92 C 69.45 3.1 66.35 0 62.53 0 L 6.92 0 C 3.1 0 0 3.1 0 6.92 Z" style="stroke:none"></path></g><g fill="#CCFFCC" fill-opacity="1.0"><path d="M 0 6.92 L 0 13.27 C 0 17.09 3.1 20.19 6.92 20.19 L 62.53 20.19 C 66.35 20.19 69.45 17.09 69.45 13.27 L 69.45 6.92 C 69.45 3.1 66.35 0 62.53 0 L 6.92 0 C 3.1 0 0 3.1 0 6.92 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 5.32 7.05)"><text transform="matrix(1 0 0 -1 0 0)" color="#000000">RAG-Gym</text></g></g></svg></span></span>
<span class="ltx_tr">
<span class="ltx_td ltx_nopad_r ltx_align_center"><span class="ltx_text ltx_font_italic">Direct</span></span></span>
<span class="ltx_tr">
<span class="ltx_td ltx_nopad_r ltx_align_center"><span class="ltx_text ltx_font_italic">Preference</span></span></span>
<span class="ltx_tr">
<span class="ltx_td ltx_nopad_r ltx_align_center"><span class="ltx_text ltx_font_italic">Optimization</span></span></span>
</span> </span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">Direct</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">20.80</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">28.79</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">25.20</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">29.45</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">12.00</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">20.67</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">62.37</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">30.09</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">35.32</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">CoT</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">26.30</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">35.06</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">28.20</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">32.84</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">40.80</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">51.67</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">71.33</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">41.66</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">47.73</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">RAG</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">38.00</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">49.38</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">37.60</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">42.88</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">28.80</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">39.57</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">67.79</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">43.05</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">49.91</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">ReAct</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">33.00</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">43.96</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">32.20</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">39.24</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">44.80</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">54.35</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">68.89</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">44.72</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">51.61</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">Search-o1</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">42.20</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">54.34</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">44.10</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">52.66</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">42.40</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">55.59</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">70.23</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">49.73</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">58.21</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center" style="background-color:#D9FFFF;"><span class="ltx_text" style="font-size:90%;background-color:#D9FFFF;">ReSearch</span></td>
<td class="ltx_td ltx_align_center" style="background-color:#D9FFFF;"><span class="ltx_text" style="font-size:90%;background-color:#D9FFFF;">42.20</span></td>
<td class="ltx_td ltx_align_center" style="background-color:#D9FFFF;"><span class="ltx_text" style="font-size:90%;background-color:#D9FFFF;">55.22</span></td>
<td class="ltx_td ltx_align_center" style="background-color:#D9FFFF;"><span class="ltx_text" style="font-size:90%;background-color:#D9FFFF;">44.30</span></td>
<td class="ltx_td ltx_align_center" style="background-color:#D9FFFF;"><span class="ltx_text" style="font-size:90%;background-color:#D9FFFF;">51.36</span></td>
<td class="ltx_td ltx_align_center" style="background-color:#D9FFFF;"><span class="ltx_text" style="font-size:90%;background-color:#D9FFFF;">48.00</span></td>
<td class="ltx_td ltx_align_center" style="background-color:#D9FFFF;"><span class="ltx_text" style="font-size:90%;background-color:#D9FFFF;">56.57</span></td>
<td class="ltx_td ltx_align_center" style="background-color:#D9FFFF;"><span class="ltx_text ltx_font_bold" style="font-size:90%;background-color:#D9FFFF;">72.11</span></td>
<td class="ltx_td ltx_align_center" style="background-color:#D9FFFF;"><span class="ltx_text" style="font-size:90%;background-color:#D9FFFF;">51.65</span></td>
<td class="ltx_td ltx_align_center" style="background-color:#D9FFFF;"><span class="ltx_text" style="font-size:90%;background-color:#D9FFFF;">58.82</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" rowspan="6"><span class="ltx_text" style="font-size:90%;"> 
<span class="ltx_tabular ltx_align_middle">
<span class="ltx_tr">
<span class="ltx_td ltx_nopad_r ltx_align_center"><svg id="S3.T1.pic3" class="ltx_picture" height="20.19" overflow="visible" version="1.1" width="69.45"><g transform="translate(0,20.19) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#CCFFCC" fill-opacity="1.0"><path d="M 0 6.92 L 0 13.27 C 0 17.09 3.1 20.19 6.92 20.19 L 62.53 20.19 C 66.35 20.19 69.45 17.09 69.45 13.27 L 69.45 6.92 C 69.45 3.1 66.35 0 62.53 0 L 6.92 0 C 3.1 0 0 3.1 0 6.92 Z" style="stroke:none"></path></g><g fill="#CCFFCC" fill-opacity="1.0"><path d="M 0 6.92 L 0 13.27 C 0 17.09 3.1 20.19 6.92 20.19 L 62.53 20.19 C 66.35 20.19 69.45 17.09 69.45 13.27 L 69.45 6.92 C 69.45 3.1 66.35 0 62.53 0 L 6.92 0 C 3.1 0 0 3.1 0 6.92 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 5.32 7.05)"><text transform="matrix(1 0 0 -1 0 0)" color="#000000">RAG-Gym</text></g></g></svg></span></span>
<span class="ltx_tr">
<span class="ltx_td ltx_nopad_r ltx_align_center"><span class="ltx_text ltx_font_italic">Process</span></span></span>
<span class="ltx_tr">
<span class="ltx_td ltx_nopad_r ltx_align_center"><span class="ltx_text ltx_font_italic">Reward</span></span></span>
<span class="ltx_tr">
<span class="ltx_td ltx_nopad_r ltx_align_center"><span class="ltx_text ltx_font_italic">Modeling</span></span></span>
</span> </span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">Direct</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">19.90</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">28.05</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">24.30</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">28.46</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">8.00</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">13.28</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">60.02</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">28.06</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">32.45</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">CoT</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">30.00</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">38.31</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">31.60</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">35.97</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">44.80</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">56.13</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">65.67</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">43.02</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">49.02</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">RAG</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold" style="font-size:90%;">44.80</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">56.26</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">40.30</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">46.45</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">41.60</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">53.55</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">66.30</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">48.25</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">55.64</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">ReAct</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">40.60</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">53.05</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">40.20</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">48.93</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">43.20</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">55.72</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">68.19</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">48.05</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">56.47</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">Search-o1</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">41.80</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">55.13</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">46.00</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">54.16</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold" style="font-size:90%;">51.20</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold" style="font-size:90%;">63.32</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">71.56</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">52.64</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">61.04</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#D9FFFF;"><span class="ltx_text" style="font-size:90%;background-color:#D9FFFF;">ReSearch</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#D9FFFF;"><span class="ltx_text" style="font-size:90%;background-color:#D9FFFF;">44.10</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#D9FFFF;"><span class="ltx_text ltx_font_bold" style="font-size:90%;background-color:#D9FFFF;">56.84</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#D9FFFF;"><span class="ltx_text ltx_font_bold" style="font-size:90%;background-color:#D9FFFF;">50.20</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#D9FFFF;"><span class="ltx_text ltx_font_bold" style="font-size:90%;background-color:#D9FFFF;">57.94</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#D9FFFF;"><span class="ltx_text ltx_font_bold" style="font-size:90%;background-color:#D9FFFF;">51.20</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#D9FFFF;"><span class="ltx_text" style="font-size:90%;background-color:#D9FFFF;">63.15</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#D9FFFF;"><span class="ltx_text" style="font-size:90%;background-color:#D9FFFF;">71.72</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#D9FFFF;"><span class="ltx_text ltx_font_bold" style="font-size:90%;background-color:#D9FFFF;">54.31</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#D9FFFF;"><span class="ltx_text ltx_font_bold" style="font-size:90%;background-color:#D9FFFF;">62.41</span></td>
</tr>
</table>
</figure>
<div id="S3.p2" class="ltx_para">
<p class="ltx_p">To address this issue, we propose the Reasoning and Search (ReSearch, as shown in Fig <a href="#S3.F2" title="Figure 2 ‣ 3 Reasoning and Search (ReSearch) Agent ‣ RAG-Gym: Optimizing Reasoning and Search Agents with Process Supervision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>) agent, which integrates reasoning and search in a unified, answer-driven framework.
Given the state <math id="S3.p2.m1" class="ltx_Math" alttext="s_{t}" display="inline"><semantics><msub><mi>s</mi><mi>t</mi></msub><annotation encoding="application/x-tex">s_{t}</annotation></semantics></math> with the original question <math id="S3.p2.m2" class="ltx_Math" alttext="\mathcal{Q}" display="inline"><semantics><mi class="ltx_font_mathcaligraphic">𝒬</mi><annotation encoding="application/x-tex">\mathcal{Q}</annotation></semantics></math> and the history <math id="S3.p2.m3" class="ltx_Math" alttext="\mathcal{H}_{t}=\{(q_{1},D_{1}),\dots,(q_{t-1},D_{t-1})\}" display="inline"><semantics><mrow><msub><mi class="ltx_font_mathcaligraphic">ℋ</mi><mi>t</mi></msub><mo>=</mo><mrow><mo stretchy="false">{</mo><mrow><mo stretchy="false">(</mo><msub><mi>q</mi><mn>1</mn></msub><mo>,</mo><msub><mi>D</mi><mn>1</mn></msub><mo stretchy="false">)</mo></mrow><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mrow><mo stretchy="false">(</mo><msub><mi>q</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>D</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></mrow><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{H}_{t}=\{(q_{1},D_{1}),\dots,(q_{t-1},D_{t-1})\}</annotation></semantics></math>,
the agent will first summarize the retrieved documents into structured responses to their corresponding queries, forming a refined knowledge representation <math id="S3.p2.m4" class="ltx_Math" alttext="\mathcal{H}_{t}^{\prime}" display="inline"><semantics><msubsup><mi class="ltx_font_mathcaligraphic">ℋ</mi><mi>t</mi><mo>′</mo></msubsup><annotation encoding="application/x-tex">\mathcal{H}_{t}^{\prime}</annotation></semantics></math>:</p>
<table id="S3.E8" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E8.m1" class="ltx_Math" alttext="\mathcal{H}_{t}^{\prime}=\{(q_{1},m_{1}),\dots,(q_{t-1},m_{t-1})\}," display="block"><semantics><mrow><mrow><msubsup><mi class="ltx_font_mathcaligraphic">ℋ</mi><mi>t</mi><mo>′</mo></msubsup><mo>=</mo><mrow><mo stretchy="false">{</mo><mrow><mo stretchy="false">(</mo><msub><mi>q</mi><mn>1</mn></msub><mo>,</mo><msub><mi>m</mi><mn>1</mn></msub><mo stretchy="false">)</mo></mrow><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mrow><mo stretchy="false">(</mo><msub><mi>q</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>m</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></mrow><mo stretchy="false">}</mo></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\mathcal{H}_{t}^{\prime}=\{(q_{1},m_{1}),\dots,(q_{t-1},m_{t-1})\},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(8)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math id="S3.p2.m5" class="ltx_Math" alttext="m_{i}" display="inline"><semantics><msub><mi>m</mi><mi>i</mi></msub><annotation encoding="application/x-tex">m_{i}</annotation></semantics></math> represents the summarized answer to <math id="S3.p2.m6" class="ltx_Math" alttext="q_{i}" display="inline"><semantics><msub><mi>q</mi><mi>i</mi></msub><annotation encoding="application/x-tex">q_{i}</annotation></semantics></math> given <math id="S3.p2.m7" class="ltx_Math" alttext="D_{i}" display="inline"><semantics><msub><mi>D</mi><mi>i</mi></msub><annotation encoding="application/x-tex">D_{i}</annotation></semantics></math>. This summarization step filters out irrelevant information and mitigates the challenge of long-context processing, allowing the agent to focus on the most relevant facts when constructing an answer.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p class="ltx_p">Using this refined knowledge <math id="S3.p3.m1" class="ltx_Math" alttext="\mathcal{H}_{t}^{\prime}" display="inline"><semantics><msubsup><mi class="ltx_font_mathcaligraphic">ℋ</mi><mi>t</mi><mo>′</mo></msubsup><annotation encoding="application/x-tex">\mathcal{H}_{t}^{\prime}</annotation></semantics></math>, the agent then engages in structured reasoning to infer a candidate answer to the question.
It will then examine the reasoning steps and judge if all claims are well grounded in the history.
If the agent determines that all claims in the answer reasoning are supported by retrieved evidence, it outputs the answer as its final action. Otherwise, it identifies unverified claims, which are statements that lack sufficient justification based on the available evidence. These unverified claims serve as the basis for generating the next search query, which is specifically designed to retrieve the missing information. The retrieved documents from this query are then added to <math id="S3.p3.m2" class="ltx_Math" alttext="\mathcal{H}_{t}" display="inline"><semantics><msub><mi class="ltx_font_mathcaligraphic">ℋ</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\mathcal{H}_{t}</annotation></semantics></math>, and the reasoning process is repeated iteratively until all claims are verified or the retrieval budget is exhausted.</p>
</div>
<div id="S3.p4" class="ltx_para">
<p class="ltx_p">A key advantage of the ReSearch agent lies in its answer-driven query formulation, which ensures that each search query is explicitly designed to search for missing information in the answer reasoning process. In contrast, conventional architectures such as ReAct rely on implicit LLM heuristics to determine when and how to issue search queries, often resulting in the retrieval of unhelpful information that cannot be effectively utilized in answer generation. By structuring query generation around ungrounded claims, our approach guarantees that retrieved evidence directly contributes to constructing the final answer, significantly improving retrieval efficiency and overall response quality. Empirical results, as presented in Section <a href="#S4" title="4 Results ‣ RAG-Gym: Optimizing Reasoning and Search Agents with Process Supervision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, demonstrate that this structured approach to query generation leads to more precise and well-supported answers compared to conventional information-seeking agents.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Experimental Settings</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p class="ltx_p">To evaluate how different agents perform on knowledge-intensive QA tasks and how they can benefit from various process supervision methods implemented in RAG-Gym, we select different datasets that are both knowledge-intensive and reasoning-intensive, covering questions from both general and medical domains. Specifically, we choose HotpotQA <cite class="ltx_cite ltx_citemacro_citep">(Yang et al., <a href="#bib.bib60" title="" class="ltx_ref">2018</a>)</cite>, 2WikiMultihopQA <cite class="ltx_cite ltx_citemacro_citep">(Ho et al., <a href="#bib.bib11" title="" class="ltx_ref">2020</a>)</cite>, and Bamboogle <cite class="ltx_cite ltx_citemacro_citep">(Press et al., <a href="#bib.bib29" title="" class="ltx_ref">2023</a>)</cite>, which are commonly used multi-hop QA datasets constructed from Wikipedia. We also include the MedQA dataset <cite class="ltx_cite ltx_citemacro_citep">(Jin et al., <a href="#bib.bib17" title="" class="ltx_ref">2021</a>)</cite>, which consists of medical exam questions requiring specialized domain knowledge and complex reasoning. Following prior work <cite class="ltx_cite ltx_citemacro_citep">(Shao et al., <a href="#bib.bib35" title="" class="ltx_ref">2023</a>; Xiong et al., <a href="#bib.bib57" title="" class="ltx_ref">2024</a>)</cite>, we use Exact Match (EM) and F1 scores as the evaluation metrics of HotpotQA, 2WikiMultihopQA, and Bamboogle. The multi-choice MedQA dataset is evaluated with accuracy (Acc) as the metric. We also computed the average EM and F1 scores across different tasks, with accuracy being considered as both EM and F1 scores in the multi-choice evaluation setting.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p class="ltx_p">We tested the RAG-Gym process supervision methods on six different agent designs, including the direct prompting (Direct), Chain-of-thought (CoT) <cite class="ltx_cite ltx_citemacro_citep">(Wei et al., <a href="#bib.bib52" title="" class="ltx_ref">2022b</a>)</cite>, RAG <cite class="ltx_cite ltx_citemacro_citep">(Lewis et al., <a href="#bib.bib23" title="" class="ltx_ref">2020</a>)</cite>, ReAct <cite class="ltx_cite ltx_citemacro_citep">(Yao et al., <a href="#bib.bib61" title="" class="ltx_ref">2023</a>)</cite>, Search-o1 <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib25" title="" class="ltx_ref">2025</a>)</cite>, and the newly proposed ReSearch. More descriptions about the baselines can be found in Appendix <a href="#A3" title="Appendix C Baseline Descriptions ‣ RAG-Gym: Optimizing Reasoning and Search Agents with Process Supervision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">C</span></a>.
We selected Llama-3.1-8B-Instruct <cite class="ltx_cite ltx_citemacro_citep">(Dubey et al., <a href="#bib.bib7" title="" class="ltx_ref">2024</a>)</cite> as the base LLM for the implementations of various reasoning and search agents, due to its context length of 128k tokens and its availability of open-source parameters. We also involved GPT-4o-mini as the representative of proprietary LLMs to examine if the trained reward models can be generalized to guide the action selection of other LLMs.
For the implementation of process supervision algorithms in RAG-Gym, we sampled 1k instances from the training sets of both HotpotQA and MedQA to generate process reward data for LLM tuning. 2WikiMultihopQA and Bamboogle were tested using LLMs trained on HotpotQA to see if the tuned agents are generalizable to other datasets with similar formats. More implementation details can be found in Appendix <a href="#A4" title="Appendix D Implementation Details ‣ RAG-Gym: Optimizing Reasoning and Search Agents with Process Supervision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">D</span></a>.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Comparison of Process Supervision Methods</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p class="ltx_p">Table <a href="#S3.T1" title="Table 1 ‣ 3 Reasoning and Search (ReSearch) Agent ‣ RAG-Gym: Optimizing Reasoning and Search Agents with Process Supervision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the performance of various agents and their tuned versions using different process supervision methods in RAG-Gym. Process supervision consistently improves performance across all agents compared to the zero-shot learning (ZSL) baseline, demonstrating its effectiveness in enhancing intermediate reasoning and query generation. Among the three process supervision algorithms, PRM achieves the best results overall, outperforming ZSL baselines by up to 25.6% (ReAct; Average F1).</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p class="ltx_p">While PRM outperforms the other methods, both DPO and SFT show significant improvements over the ZSL baseline. Interestingly, SFT slightly outperforms DPO on the Direct, CoT, and RAG agents, where the tuning focuses exclusively on the answer generation step. In contrast, DPO significantly surpasses SFT on ReAct, Search-o1, and ReSearch, where the tuning process also involves learning to generate high-quality queries by contrasting positive and negative samples. This result highlights the importance and effectiveness of leveraging contrastive learning in tasks that require the generation of search queries, especially for agents that tightly integrate reasoning and query generation.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Comparison of ReSearch and other Agents</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p class="ltx_p">The results also show that ReSearch consistently outperforms other agents, both in the ZSL setting and in settings with process supervision. Without tuning, ReSearch achieves strong zero-shot performance, demonstrating the effectiveness of explicitly aligning answer reasoning with query generation. Using process reward models, ReSearch achieves state-of-the-art performance, with an average EM score of 54.31% and an average F1 score of 62.41% across different datasets. Furthermore, ReSearch exhibits superior generalization, achieving top scores on 2WikiMultihopQA and Bamboogle without task-specific fine-tuning. These results validate the ability of ReSearch’s unified reasoning and search framework to effectively leverage process supervision for both task-specific improvements and broader generalization across knowledge-intensive QA tasks.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Reward Model Transferability</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p class="ltx_p">Figure <a href="#S4.F3" title="Figure 3 ‣ 4.4 Reward Model Transferability ‣ 4 Results ‣ RAG-Gym: Optimizing Reasoning and Search Agents with Process Supervision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> highlights the performance improvements of the ReSearch agent with GPT-4o-mini using Llama-3.1-8B-based process reward models. The action selection with reward models leads to consistent gains across all tasks, demonstrating the transferability of PRM to effectively select high-quality actions in different LLMs. This result also highlights the potential of using process reward models as a plug-and-play module to enhance the reasoning and search capabilities of proprietary LLMs, where direct fine-tuning is not feasible due to restrictions on model access.</p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2502.13957/assets/x3.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="436" height="291" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Performance improvement of GPT-4o-mini on different tasks with the reward model (PRM) trained on Llama-3.1 actions.</figcaption>
</figure>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2502.13957/assets/x4.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="436" height="95" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Performance of ReSearch agents with process reward models tuned on different numbers of training samples.</figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Analysis and Discussion</h2>

<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Comparison of Different Reward Sources</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p class="ltx_p">To evaluate the effectiveness of different process reward sources in training reward models, we conducted experiments on MedQA and compared their alignments with domain expert preferences as well as their impact on downstream accuracy. Specifically, we examined whether using LLMs like GPT-4o for process data annotation is an effective approach and how its preferences align with human annotations. Four domain experts annotated 200 MedQA questions. A reward model was trained on the remaining 800 training questions annotated with GPT-4o, and its preferences were compared to those of domain experts.</p>
</div>
<figure id="S5.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 2: </span>Agreement between domain expert preference and the estimation of rewards with different sources for MedQA.</figcaption>
<br class="ltx_break ltx_centering">
<table class="ltx_tabular ltx_centering ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_tt"><span class="ltx_text ltx_font_bold" style="font-size:90%;">Type</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt"><span class="ltx_text ltx_font_bold" style="font-size:90%;">Source</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt"><span class="ltx_text ltx_font_bold" style="font-size:90%;">Agreement (%)</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt"><span class="ltx_text ltx_font_bold" style="font-size:90%;">Accuracy (%)</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">ORM</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">Truth</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">–</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">66.77</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">PRM</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">Random</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">50.00</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">68.26</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">PRM</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">Rollout</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">71.03</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">68.34</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text" style="font-size:90%;">PRM</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text" style="font-size:90%;">GPT-4o</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text" style="font-size:90%;">85.85</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text" style="font-size:90%;">71.96</span></td>
</tr>
</table>
</figure>
<div id="S5.SS1.p2" class="ltx_para">
<p class="ltx_p">The results are shown in Table <a href="#S5.T2" title="Table 2 ‣ 5.1 Comparison of Different Reward Sources ‣ 5 Analysis and Discussion ‣ RAG-Gym: Optimizing Reasoning and Search Agents with Process Supervision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. The reward model trained with GPT-4o annotations achieved the highest agreement with human preferences (85.85%), significantly outperforming the rollout-based method (71.03%) introduced in Math-Shepherd <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a href="#bib.bib48" title="" class="ltx_ref">2024a</a>)</cite>. This demonstrates that GPT-4o annotations closely align with human reasoning and decision-making in this context. Furthermore, the model trained with GPT-4o annotations achieved the highest accuracy (71.96%), highlighting its effectiveness in knowledge-intensive tasks. In contrast, the rollout-based process reward model achieved an accuracy of 68.34%, comparable to random action selection (68.26%). We also evaluated the outcome reward model (ORM), which uses the final evaluation metric (accuracy) for trajectory assessment, and found it achieved an accuracy of only 66.77%. This result shows the limitations of ORM in multi-step reasoning and search scenarios, where process-level supervision is critical. These findings suggest that leveraging advanced LLMs like GPT-4o for reward annotation is both practical and effective.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Training Time Scaling</h3>

<figure id="S5.F5" class="ltx_figure"><img src="/html/2502.13957/assets/x5.png" id="S5.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="436" height="95" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Performance of ReSearch agents with different numbers of actions sampled for reward scoring and selection per step.</figcaption>
</figure>
<div id="S5.SS2.p1" class="ltx_para">
<p class="ltx_p">For the evaluation of training sample size and its impacts on the performance of ReSearch agents, we conducted experiments using process reward models trained on varying numbers of instances, ranging from 250 to 1000 questions. The results, presented in Figure <a href="#S4.F4" title="Figure 4 ‣ 4.4 Reward Model Transferability ‣ 4 Results ‣ RAG-Gym: Optimizing Reasoning and Search Agents with Process Supervision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, show how the agent’s performance scales with the availability of more training data across four datasets.
In general, the performance of ReSearch improves with an increasing number of training samples, but the gains tend to converge as the sample size grows. Notably, there is a sharp improvement in F1 scores on HotpotQA, 2WikiMultihopQA, and Bamboogle when comparing the ZSL baseline to process reward models trained on 250 samples, showing that even a small amount of process reward data can yield significant performance gains. However, the improvements become less pronounced on HotpotQA and 2WikiMultihopQA when increasing the training samples from 500 to 1000, indicating diminishing returns as the model approaches a saturation point in its learning from additional data. This convergence may also be attributed to the candidate actions being sampled from the untuned agent during inference, which constrains the space of actions that process reward models can evaluate.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p class="ltx_p">For MedQA, which involves complex reasoning and information-seeking tasks requiring domain-specific knowledge, a different trend is observed. With only 250 training samples, the performance slightly drops below the ZSL baseline, highlighting the challenges of capturing intricate domain-specific processes with limited training data. As the sample size increases, however, the performance gradually recovers and eventually surpasses the ZSL baseline, achieving the highest accuracy of 71.72 with 1000 samples. This underscores the importance of sufficient training data in capturing the nuanced reasoning and query-generation processes required for specialized tasks.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Inference Time Scaling</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p class="ltx_p">Since process reward models optimize action-taking by identifying high-quality actions from the generated candidates during inference, we explored how the agent performance changes with the increasing number of sampled actions at each time step. Figure <a href="#S5.F5" title="Figure 5 ‣ 5.2 Training Time Scaling ‣ 5 Analysis and Discussion ‣ RAG-Gym: Optimizing Reasoning and Search Agents with Process Supervision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> displays the results of our inference time scaling study, with ReSearch as the tested agent.
We observe a consistent trend across multiple benchmarks, where increasing the number of sampled actions generally improves performance. Specifically, for HotpotQA and Bamboogle, the F1 score continues to rise as more actions are sampled, demonstrating the benefits of expanding the candidate set to enable better action selection at each step. However, performance gains gradually diminish, indicating that the agent reaches a point where additional sampled actions contribute less to improvement. This suggests that while action sampling is beneficial, there is a limit to how much additional sampling enhances decision-making.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Related Work</h2>

<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Retrieval-Augmented Generation (RAG)</h3>

<div id="S6.SS1.p1" class="ltx_para">
<p class="ltx_p">RAG has emerged as a powerful paradigm for enhancing language models in knowledge-intensive tasks. By integrating external knowledge sources into generative models, RAG enables the production of more accurate and informative responses. This approach has seen success across various domains, such as open-domain question answering <cite class="ltx_cite ltx_citemacro_citep">(Karpukhin et al., <a href="#bib.bib20" title="" class="ltx_ref">2020</a>)</cite> and conversational agents <cite class="ltx_cite ltx_citemacro_citep">(Shuster et al., <a href="#bib.bib38" title="" class="ltx_ref">2021</a>)</cite>. Researchers have aimed to improve RAG by optimizing retrieval components <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a href="#bib.bib65" title="" class="ltx_ref">2023a</a>; Nguyen et al., <a href="#bib.bib28" title="" class="ltx_ref">2024</a>; Xu et al., <a href="#bib.bib59" title="" class="ltx_ref">2024b</a>)</cite> or enhancing the generative model’s ability to leverage retrieved content <cite class="ltx_cite ltx_citemacro_citep">(Izacard et al., <a href="#bib.bib14" title="" class="ltx_ref">2023</a>; Zhang et al., <a href="#bib.bib66" title="" class="ltx_ref">2024b</a>; Fang et al., <a href="#bib.bib8" title="" class="ltx_ref">2024</a>; Wei et al., <a href="#bib.bib53" title="" class="ltx_ref">2024</a>; Jin et al., <a href="#bib.bib19" title="" class="ltx_ref">2024</a>; Wang et al., <a href="#bib.bib50" title="" class="ltx_ref">2024c</a>; Zhang et al., <a href="#bib.bib64" title="" class="ltx_ref">2025</a>)</cite>. Applications of RAG extend to specialized fields, like biomedical question answering <cite class="ltx_cite ltx_citemacro_citep">(Xiong et al., <a href="#bib.bib57" title="" class="ltx_ref">2024</a>; Xu et al., <a href="#bib.bib58" title="" class="ltx_ref">2024a</a>; Kim et al., <a href="#bib.bib22" title="" class="ltx_ref">2024</a>)</cite>, showcasing its versatility. Despite these advancements, vanilla RAG architectures typically rely on a single round of retrieval, limiting their effectiveness in scenarios requiring iterative reasoning and complex information integration.</p>
</div>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Multi-hop Question Answering (QA)</h3>

<div id="S6.SS2.p1" class="ltx_para">
<p class="ltx_p">Multi-hop QA tasks require systems to perform reasoning across multiple pieces of information, often from disparate sources, to derive a correct answer <cite class="ltx_cite ltx_citemacro_citep">(Yang et al., <a href="#bib.bib60" title="" class="ltx_ref">2018</a>; Ho et al., <a href="#bib.bib11" title="" class="ltx_ref">2020</a>)</cite>. These tasks are inherently challenging for traditional RAG architectures due to their single retrieval pass, which restricts the ability to gather all necessary information for complex questions. Early attempts to address multi-hop QA in RAG systems include agent designs such as ReAct <cite class="ltx_cite ltx_citemacro_citep">(Yao et al., <a href="#bib.bib61" title="" class="ltx_ref">2023</a>)</cite>, which integrate reasoning processes into retrieval steps <cite class="ltx_cite ltx_citemacro_citep">(Trivedi et al., <a href="#bib.bib44" title="" class="ltx_ref">2023</a>; Asai et al., <a href="#bib.bib2" title="" class="ltx_ref">2024</a>; Jiang et al., <a href="#bib.bib16" title="" class="ltx_ref">2023</a>; Press et al., <a href="#bib.bib29" title="" class="ltx_ref">2023</a>; Ram et al., <a href="#bib.bib31" title="" class="ltx_ref">2023</a>; Wang et al., <a href="#bib.bib49" title="" class="ltx_ref">2024b</a>; Khot et al., <a href="#bib.bib21" title="" class="ltx_ref">2023</a>; Shao et al., <a href="#bib.bib35" title="" class="ltx_ref">2023</a>; Wang et al., <a href="#bib.bib47" title="" class="ltx_ref">2023</a>; Shi et al., <a href="#bib.bib36" title="" class="ltx_ref">2024</a>; Yu et al., <a href="#bib.bib62" title="" class="ltx_ref">2024</a>)</cite>. Nevertheless, these strategies commonly rely on finely tuned heuristics or handcrafted prompts, lacking robustness when facing diverse question formulations.</p>
</div>
</section>
<section id="S6.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3 </span>Process Reward Modeling (PRM)</h3>

<div id="S6.SS3.p1" class="ltx_para">
<p class="ltx_p">PRM provides a structured framework for guiding LLM reasoning by incorporating intermediate feedback during the reasoning process <cite class="ltx_cite ltx_citemacro_citep">(Lightman et al., <a href="#bib.bib26" title="" class="ltx_ref">2024</a>; Ma et al., <a href="#bib.bib27" title="" class="ltx_ref">2023</a>)</cite>. This approach has been shown to enhance the reasoning capabilities and accuracy of models in complex tasks <cite class="ltx_cite ltx_citemacro_citep">(Setlur et al., <a href="#bib.bib34" title="" class="ltx_ref">2024</a>; Wu et al., <a href="#bib.bib55" title="" class="ltx_ref">2024</a>)</cite>. Researchers have explored various methods to implement process supervision, including constructing intermediate supervision signals through rollouts, human feedback, or LLM feedback <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a href="#bib.bib63" title="" class="ltx_ref">2024a</a>; Cobbe et al., <a href="#bib.bib4" title="" class="ltx_ref">2021</a>; Wang et al., <a href="#bib.bib48" title="" class="ltx_ref">2024a</a>; Sun et al., <a href="#bib.bib39" title="" class="ltx_ref">2024</a>)</cite>. By shifting the focus from outcome-based rewards to reasoning trajectories, this paradigm fosters more nuanced understanding and improved problem-solving abilities in language models.</p>
</div>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>

<div id="S7.p1" class="ltx_para">
<p class="ltx_p">We presented RAG-Gym, a framework for optimizing reasoning and search agents through process supervision, and introduced ReSearch, an agent architecture that unifies answer reasoning with search query generation. Experiments show that RAG-Gym improves search agents on knowledge-intensive tasks, with ReSearch consistently outperforming baselines.
We also demonstrated the effectiveness of using LLMs as process reward judges, the transferability of trained reward models on different LLMs, and the scaling patterns of ReSearch during training and inference.</p>
</div>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Acknowledgment</h2>

<div id="S8.p1" class="ltx_para">
<p class="ltx_p">This research was supported by the Division of Intramural Research (DIR) of the National Library of Medicine (NLM), National Institutes of Health (NIH), and other grants from NIH and National Science Foundation (NSF).</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Impact Statement</h2>

<div id="Sx1.p1" class="ltx_para">
<p class="ltx_p">This paper presents RAG-Gym, a framework designed to advance the field of language modeling by optimizing reasoning and search agents through process supervision. Our work contributes to improving retrieval-augmented generation (RAG) by refining how language models seek, retrieve, and reason about information. The techniques proposed in this paper enhance the capabilities of language agents to handle knowledge-intensive tasks, making them more effective across various domains.</p>
</div>
<div id="Sx1.p2" class="ltx_para">
<p class="ltx_p">There are potential societal implications of this work, including its applications in improving AI-driven decision-making systems in areas such as research, education, and healthcare. However, our research primarily focuses on the technical advancement of process supervision in search and reasoning agents, and we do not foresee any immediate ethical concerns requiring further discussion.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Achiam et al. (2023)</span>
<span class="ltx_bibblock">
Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al.

</span>
<span class="ltx_bibblock">Gpt-4 technical report.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.08774</em>, 2023.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Asai et al. (2024)</span>
<span class="ltx_bibblock">
Asai, A., Wu, Z., Wang, Y., Sil, A., and Hajishirzi, H.

</span>
<span class="ltx_bibblock">Self-rag: Learning to retrieve, generate, and critique through self-reflection.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024</em>. OpenReview.net, 2024.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://openreview.net/forum?id=hSyW5go0v8" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openreview.net/forum?id=hSyW5go0v8</a>.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et al. (2020)</span>
<span class="ltx_bibblock">
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 33:1877–1901, 2020.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cobbe et al. (2021)</span>
<span class="ltx_bibblock">
Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et al.

</span>
<span class="ltx_bibblock">Training verifiers to solve math word problems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2110.14168</em>, 2021.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cormack et al. (2009)</span>
<span class="ltx_bibblock">
Cormack, G. V., Clarke, C. L., and Buettcher, S.

</span>
<span class="ltx_bibblock">Reciprocal rank fusion outperforms condorcet and individual rank learning methods.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval</em>, pp.  758–759, 2009.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dong et al. (2024)</span>
<span class="ltx_bibblock">
Dong, G., Zhang, C., Deng, M., Zhu, Y., Dou, Z., and Wen, J.-R.

</span>
<span class="ltx_bibblock">Progressive multimodal reasoning via active retrieval.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2412.14835</em>, 2024.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dubey et al. (2024)</span>
<span class="ltx_bibblock">
Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al.

</span>
<span class="ltx_bibblock">The llama 3 herd of models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2407.21783</em>, 2024.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fang et al. (2024)</span>
<span class="ltx_bibblock">
Fang, F., Bai, Y., Ni, S., Yang, M., Chen, X., and Xu, R.

</span>
<span class="ltx_bibblock">Enhancing noise robustness of retrieval-augmented language models with adaptive adversarial training.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2405.20978</em>, 2024.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. (2024)</span>
<span class="ltx_bibblock">
Gao, J., Li, L., Li, W., Fu, Y., and Dai, B.

</span>
<span class="ltx_bibblock">Smartrag: Jointly learn rag-related tasks from the environment feedback.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2410.18141</em>, 2024.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. (2023)</span>
<span class="ltx_bibblock">
Gao, Y., Xiong, Y., Gao, X., Jia, K., Pan, J., Bi, Y., Dai, Y., Sun, J., and Wang, H.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for large language models: A survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2312.10997</em>, 2023.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ho et al. (2020)</span>
<span class="ltx_bibblock">
Ho, X., Nguyen, A.-K. D., Sugawara, S., and Aizawa, A.

</span>
<span class="ltx_bibblock">Constructing a multi-hop qa dataset for comprehensive evaluation of reasoning steps.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Proceedings of the 28th International Conference on Computational Linguistics</em>, pp.  6609–6625, 2020.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hsu et al. (2024)</span>
<span class="ltx_bibblock">
Hsu, S., Khattab, O., Finn, C., and Sharma, A.

</span>
<span class="ltx_bibblock">Grounding by trying: Llms with reinforcement learning-enhanced retrieval.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2410.23214</em>, 2024.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. (2021)</span>
<span class="ltx_bibblock">
Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W.

</span>
<span class="ltx_bibblock">Lora: Low-rank adaptation of large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2106.09685</em>, 2021.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Izacard et al. (2023)</span>
<span class="ltx_bibblock">
Izacard, G., Lewis, P., Lomeli, M., Hosseini, L., Petroni, F., Schick, T., Dwivedi-Yu, J., Joulin, A., Riedel, S., and Grave, E.

</span>
<span class="ltx_bibblock">Atlas: Few-shot learning with retrieval augmented language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Journal of Machine Learning Research</em>, 24(251):1–43, 2023.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ji et al. (2023)</span>
<span class="ltx_bibblock">
Ji, Z., Lee, N., Frieske, R., Yu, T., Su, D., Xu, Y., Ishii, E., Bang, Y. J., Madotto, A., and Fung, P.

</span>
<span class="ltx_bibblock">Survey of hallucination in natural language generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">ACM Computing Surveys</em>, 55(12):1–38, 2023.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. (2023)</span>
<span class="ltx_bibblock">
Jiang, Z., Xu, F. F., Gao, L., Sun, Z., Liu, Q., Dwivedi-Yu, J., Yang, Y., Callan, J., and Neubig, G.

</span>
<span class="ltx_bibblock">Active retrieval augmented generation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>, pp.  7969–7992, 2023.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jin et al. (2021)</span>
<span class="ltx_bibblock">
Jin, D., Pan, E., Oufattole, N., Weng, W.-H., Fang, H., and Szolovits, P.

</span>
<span class="ltx_bibblock">What disease does this patient have? a large-scale open domain question answering dataset from medical exams.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Applied Sciences</em>, 11(14):6421, 2021.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jin et al. (2023)</span>
<span class="ltx_bibblock">
Jin, Q., Kim, W., Chen, Q., Comeau, D. C., Yeganova, L., Wilbur, W. J., and Lu, Z.

</span>
<span class="ltx_bibblock">Medcpt: Contrastive pre-trained transformers with large-scale pubmed search logs for zero-shot biomedical information retrieval.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Bioinformatics</em>, 39(11):btad651, 2023.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jin et al. (2024)</span>
<span class="ltx_bibblock">
Jin, Z., Yuan, H., Men, T., Cao, P., Chen, Y., Liu, K., and Zhao, J.

</span>
<span class="ltx_bibblock">Rag-rewardbench: Benchmarking reward models in retrieval augmented generation for preference alignment.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2412.13746</em>, 2024.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karpukhin et al. (2020)</span>
<span class="ltx_bibblock">
Karpukhin, V., Oguz, B., Min, S., Lewis, P., Wu, L., Edunov, S., Chen, D., and Yih, W.-t.

</span>
<span class="ltx_bibblock">Dense passage retrieval for open-domain question answering.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, pp.  6769–6781, 2020.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khot et al. (2023)</span>
<span class="ltx_bibblock">
Khot, T., Trivedi, H., Finlayson, M., Fu, Y., Richardson, K., Clark, P., and Sabharwal, A.

</span>
<span class="ltx_bibblock">Decomposed prompting: A modular approach for solving complex tasks.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023</em>. OpenReview.net, 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://openreview.net/forum?id=_nGgzQjzaRy" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openreview.net/forum?id=_nGgzQjzaRy</a>.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al. (2024)</span>
<span class="ltx_bibblock">
Kim, Y., Park, C., Jeong, H., Chan, Y. S., Xu, X., McDuff, D., Lee, H., Ghassemi, M., Breazeal, C., and Park, H. W.

</span>
<span class="ltx_bibblock">Mdagents: An adaptive collaboration of llms for medical decision-making.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">The Thirty-eighth Annual Conference on Neural Information Processing Systems</em>, 2024.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et al. (2020)</span>
<span class="ltx_bibblock">
Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Küttler, H., Lewis, M., Yih, W.-t., Rocktäschel, T., et al.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for knowledge-intensive nlp tasks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 33:9459–9474, 2020.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2024)</span>
<span class="ltx_bibblock">
Li, X., Xu, W., Zhao, R., Jiao, F., Joty, S., and Bing, L.

</span>
<span class="ltx_bibblock">Can we further elicit reasoning in llms? critic-guided planning with retrieval-augmentation for solving challenging tasks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2410.01428</em>, 2024.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2025)</span>
<span class="ltx_bibblock">
Li, X., Dong, G., Jin, J., Zhang, Y., Zhou, Y., Zhu, Y., Zhang, P., and Dou, Z.

</span>
<span class="ltx_bibblock">Search-o1: Agentic search-enhanced large reasoning models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2501.05366</em>, 2025.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lightman et al. (2024)</span>
<span class="ltx_bibblock">
Lightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker, B., Lee, T., Leike, J., Schulman, J., Sutskever, I., and Cobbe, K.

</span>
<span class="ltx_bibblock">Let’s verify step by step.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024</em>. OpenReview.net, 2024.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://openreview.net/forum?id=v8L0pN6EOi" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openreview.net/forum?id=v8L0pN6EOi</a>.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al. (2023)</span>
<span class="ltx_bibblock">
Ma, Q., Zhou, H., Liu, T., Yuan, J., Liu, P., You, Y., and Yang, H.

</span>
<span class="ltx_bibblock">Let’s reward step by step: Step-level reward model as the navigators for reasoning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.10080</em>, 2023.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen et al. (2024)</span>
<span class="ltx_bibblock">
Nguyen, T., Chin, P., and Tai, Y.-W.

</span>
<span class="ltx_bibblock">Reward-rag: Enhancing rag with reward driven supervision.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2410.03780</em>, 2024.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Press et al. (2023)</span>
<span class="ltx_bibblock">
Press, O., Zhang, M., Min, S., Schmidt, L., Smith, N. A., and Lewis, M.

</span>
<span class="ltx_bibblock">Measuring and narrowing the compositionality gap in language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics: EMNLP 2023</em>, pp.  5687–5711, 2023.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rafailov et al. (2024)</span>
<span class="ltx_bibblock">
Rafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Ermon, S., and Finn, C.

</span>
<span class="ltx_bibblock">Direct preference optimization: Your language model is secretly a reward model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 36, 2024.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ram et al. (2023)</span>
<span class="ltx_bibblock">
Ram, O., Levine, Y., Dalmedigos, I., Muhlgay, D., Shashua, A., Leyton-Brown, K., and Shoham, Y.

</span>
<span class="ltx_bibblock">In-context retrieval-augmented language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>, 11:1316–1331, 2023.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Robertson et al. (2009)</span>
<span class="ltx_bibblock">
Robertson, S., Zaragoza, H., et al.

</span>
<span class="ltx_bibblock">The probabilistic relevance framework: Bm25 and beyond.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Foundations and Trends® in Information Retrieval</em>, 3(4):333–389, 2009.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sahoo et al. (2024)</span>
<span class="ltx_bibblock">
Sahoo, S. S., Plasek, J. M., Xu, H., Uzuner, Ö., Cohen, T., Yetisgen, M., Liu, H., Meystre, S., and Wang, Y.

</span>
<span class="ltx_bibblock">Large language models for biomedicine: foundations, opportunities, challenges, and best practices.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Journal of the American Medical Informatics Association</em>, pp.  ocae074, 2024.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Setlur et al. (2024)</span>
<span class="ltx_bibblock">
Setlur, A., Nagpal, C., Fisch, A., Geng, X., Eisenstein, J., Agarwal, R., Agarwal, A., Berant, J., and Kumar, A.

</span>
<span class="ltx_bibblock">Rewarding progress: Scaling automated process verifiers for llm reasoning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2410.08146</em>, 2024.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shao et al. (2023)</span>
<span class="ltx_bibblock">
Shao, Z., Gong, Y., Shen, Y., Huang, M., Duan, N., and Chen, W.

</span>
<span class="ltx_bibblock">Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics: EMNLP 2023</em>, pp.  9248–9274, 2023.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et al. (2024)</span>
<span class="ltx_bibblock">
Shi, Z., Zhang, S., Sun, W., Gao, S., Ren, P., Chen, Z., and Ren, Z.

</span>
<span class="ltx_bibblock">Generate-then-ground in retrieval-augmented generation for multi-hop question answering.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pp.  7339–7353, 2024.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shinn et al. (2024)</span>
<span class="ltx_bibblock">
Shinn, N., Cassano, F., Gopinath, A., Narasimhan, K., and Yao, S.

</span>
<span class="ltx_bibblock">Reflexion: Language agents with verbal reinforcement learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 36, 2024.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shuster et al. (2021)</span>
<span class="ltx_bibblock">
Shuster, K., Poff, S., Chen, M., Kiela, D., and Weston, J.

</span>
<span class="ltx_bibblock">Retrieval augmentation reduces hallucination in conversation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics: EMNLP 2021</em>, pp.  3784–3803, 2021.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al. (2024)</span>
<span class="ltx_bibblock">
Sun, S., Liu, R., Lyu, J., Yang, J.-W., Zhang, L., and Li, X.

</span>
<span class="ltx_bibblock">A large language model-driven reward design framework via dynamic feedback for reinforcement learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2410.14660</em>, 2024.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al. (2025)</span>
<span class="ltx_bibblock">
Sun, Z., Wang, Q., Yu, W., Zang, X., Zheng, K., Xu, J., Zhang, X., Yang, S., and Li, H.

</span>
<span class="ltx_bibblock">Rearter: Retrieval-augmented reasoning with trustworthy process rewarding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2501.07861</em>, 2025.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang &amp; Yang (2024)</span>
<span class="ltx_bibblock">
Tang, Y. and Yang, Y.

</span>
<span class="ltx_bibblock">Multihop-rag: Benchmarking retrieval-augmented generation for multi-hop queries.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2401.15391</em>, 2024.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. (2023a)</span>
<span class="ltx_bibblock">
Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., et al.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2302.13971</em>, 2023a.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. (2023b)</span>
<span class="ltx_bibblock">
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2307.09288</em>, 2023b.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Trivedi et al. (2023)</span>
<span class="ltx_bibblock">
Trivedi, H., Balasubramanian, N., Khot, T., and Sabharwal, A.

</span>
<span class="ltx_bibblock">Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pp.  10014–10037, 2023.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani (2017)</span>
<span class="ltx_bibblock">
Vaswani, A.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 2017.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">von Werra et al. (2020)</span>
<span class="ltx_bibblock">
von Werra, L., Belkada, Y., Tunstall, L., Beeching, E., Thrush, T., Lambert, N., Huang, S., Rasul, K., and Gallouédec, Q.

</span>
<span class="ltx_bibblock">Trl: Transformer reinforcement learning.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/huggingface/trl" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/huggingface/trl</a>, 2020.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2023)</span>
<span class="ltx_bibblock">
Wang, K., Duan, F., Wang, S., Li, P., Xian, Y., Yin, C., Rong, W., and Xiong, Z.

</span>
<span class="ltx_bibblock">Knowledge-driven cot: Exploring faithful reasoning in llms for knowledge-intensive question answering.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2308.13259</em>, 2023.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2024a)</span>
<span class="ltx_bibblock">
Wang, P., Li, L., Shao, Z., Xu, R., Dai, D., Li, Y., Chen, D., Wu, Y., and Sui, Z.

</span>
<span class="ltx_bibblock">Math-shepherd: Verify and reinforce llms step-by-step without human annotations.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pp.  9426–9439, 2024a.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2024b)</span>
<span class="ltx_bibblock">
Wang, Z., Liu, A., Lin, H., Li, J., Ma, X., and Liang, Y.

</span>
<span class="ltx_bibblock">Rat: Retrieval augmented thoughts elicit context-aware reasoning in long-horizon generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2403.05313</em>, 2024b.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2024c)</span>
<span class="ltx_bibblock">
Wang, Z., Wang, Z., Le, L., Zheng, H. S., Mishra, S., Perot, V., Zhang, Y., Mattapalli, A., Taly, A., Shang, J., et al.

</span>
<span class="ltx_bibblock">Speculative rag: Enhancing retrieval augmented generation through drafting.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2407.08223</em>, 2024c.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. (2022a)</span>
<span class="ltx_bibblock">
Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Metzler, D., et al.

</span>
<span class="ltx_bibblock">Emergent abilities of large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Transactions on Machine Learning Research</em>, 2022a.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. (2022b)</span>
<span class="ltx_bibblock">
Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al.

</span>
<span class="ltx_bibblock">Chain-of-thought prompting elicits reasoning in large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 35:24824–24837, 2022b.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. (2024)</span>
<span class="ltx_bibblock">
Wei, Z., Chen, W.-L., and Meng, Y.

</span>
<span class="ltx_bibblock">InstructRAG: Instructing retrieval-augmented generation via self-synthesized rationales.

</span>
<span class="ltx_bibblock">2024.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Welbl et al. (2018)</span>
<span class="ltx_bibblock">
Welbl, J., Stenetorp, P., and Riedel, S.

</span>
<span class="ltx_bibblock">Constructing datasets for multi-hop reading comprehension across documents.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>, 6:287–302, 2018.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2024)</span>
<span class="ltx_bibblock">
Wu, Y., Sun, Z., Li, S., Welleck, S., and Yang, Y.

</span>
<span class="ltx_bibblock">Inference scaling laws: An empirical analysis of compute-optimal inference for problem-solving with language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2408.00724</em>, 2024.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiao et al. (2023)</span>
<span class="ltx_bibblock">
Xiao, S., Liu, Z., Zhang, P., and Muennighoff, N.

</span>
<span class="ltx_bibblock">C-pack: Packaged resources to advance general chinese embedding, 2023.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiong et al. (2024)</span>
<span class="ltx_bibblock">
Xiong, G., Jin, Q., Lu, Z., and Zhang, A.

</span>
<span class="ltx_bibblock">Benchmarking retrieval-augmented generation for medicine.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2402.13178</em>, 2024.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. (2024a)</span>
<span class="ltx_bibblock">
Xu, R., Liu, H., Nag, S., Dai, Z., Xie, Y., Tang, X., Luo, C., Li, Y., Ho, J. C., Yang, C., et al.

</span>
<span class="ltx_bibblock">Simrag: Self-improving retrieval-augmented generation for adapting large language models to specialized domains.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2410.17952</em>, 2024a.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. (2024b)</span>
<span class="ltx_bibblock">
Xu, R., Shi, W., Yu, Y., Zhuang, Y., Zhu, Y., Wang, M. D., Ho, J. C., Zhang, C., and Yang, C.

</span>
<span class="ltx_bibblock">BMRetriever: Tuning large language models as better biomedical text retrievers.

</span>
<span class="ltx_bibblock">In Al-Onaizan, Y., Bansal, M., and Chen, Y.-N. (eds.), <em class="ltx_emph ltx_font_italic">Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing</em>, pp.  22234–22254, Miami, Florida, USA, November 2024b. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2024.emnlp-main.1241</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://aclanthology.org/2024.emnlp-main.1241/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/2024.emnlp-main.1241/</a>.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2018)</span>
<span class="ltx_bibblock">
Yang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W., Salakhutdinov, R., and Manning, C. D.

</span>
<span class="ltx_bibblock">HotpotQA: A dataset for diverse, explainable multi-hop question answering.

</span>
<span class="ltx_bibblock">In Riloff, E., Chiang, D., Hockenmaier, J., and Tsujii, J. (eds.), <em class="ltx_emph ltx_font_italic">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</em>, pp.  2369–2380, Brussels, Belgium, October-November 2018. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/D18-1259</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://aclanthology.org/D18-1259" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/D18-1259</a>.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yao et al. (2023)</span>
<span class="ltx_bibblock">
Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., and Cao, Y.

</span>
<span class="ltx_bibblock">React: Synergizing reasoning and acting in language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">International Conference on Learning Representations (ICLR)</em>, 2023.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. (2024)</span>
<span class="ltx_bibblock">
Yu, T., Zhang, S., and Feng, Y.

</span>
<span class="ltx_bibblock">Auto-rag: Autonomous retrieval-augmented generation for large language models.

</span>
<span class="ltx_bibblock">2024.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2411.19443" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2411.19443</a>.

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2024a)</span>
<span class="ltx_bibblock">
Zhang, D., Zhoubian, S., Hu, Z., Yue, Y., Dong, Y., and Tang, J.

</span>
<span class="ltx_bibblock">Rest-mcts*: Llm self-training via process reward guided tree search.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2406.03816</em>, 2024a.

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2025)</span>
<span class="ltx_bibblock">
Zhang, H., Song, J., Zhu, J., Wu, Y., Zhang, T., and Niu, C.

</span>
<span class="ltx_bibblock">Rag-reward: Optimizing rag with reward modeling and rlhf.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2501.13264</em>, 2025.

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2023a)</span>
<span class="ltx_bibblock">
Zhang, P., Xiao, S., Liu, Z., Dou, Z., and Nie, J.-Y.

</span>
<span class="ltx_bibblock">Retrieve anything to augment large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.07554</em>, 2023a.

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2024b)</span>
<span class="ltx_bibblock">
Zhang, T., Patil, S. G., Jain, N., Shen, S., Zaharia, M., Stoica, I., and Gonzalez, J. E.

</span>
<span class="ltx_bibblock">Raft: Adapting language model to domain specific rag.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2403.10131</em>, 2024b.

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2023b)</span>
<span class="ltx_bibblock">
Zhang, Y., Li, Y., Cui, L., Cai, D., Liu, L., Fu, T., Huang, X., Zhao, E., Zhang, Y., Chen, Y., et al.

</span>
<span class="ltx_bibblock">Siren’s song in the ai ocean: a survey on hallucination in large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.01219</em>, 2023b.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Comparison with Concurrent Work</h2>

<div id="A1.p1" class="ltx_para">
<p class="ltx_p">Table <a href="#A1.T3" title="Table 3 ‣ Appendix A Comparison with Concurrent Work ‣ RAG-Gym: Optimizing Reasoning and Search Agents with Process Supervision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> provides a detailed comparison of RAG-Gym with concurrent methods. SmartRAG <cite class="ltx_cite ltx_citemacro_citep">(Gao et al., <a href="#bib.bib9" title="" class="ltx_ref">2024</a>)</cite> shares a similar MDP formulation but relies solely on reinforcement learning-based tuning without process-level supervision. LeReT <cite class="ltx_cite ltx_citemacro_citep">(Hsu et al., <a href="#bib.bib12" title="" class="ltx_ref">2024</a>)</cite>, AR-MCTS <cite class="ltx_cite ltx_citemacro_citep">(Dong et al., <a href="#bib.bib6" title="" class="ltx_ref">2024</a>)</cite>, and CR-Planner <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib24" title="" class="ltx_ref">2024</a>)</cite> incorporate process supervision but focus exclusively on either agent tuning or verifier training, without a unified framework for both. Additionally, these methods, along with the recent ReARTeR <cite class="ltx_cite ltx_citemacro_citep">(Sun et al., <a href="#bib.bib40" title="" class="ltx_ref">2025</a>)</cite>, are tailored to specific agent architectures, limiting their generalizability. In contrast, RAG-Gym is agent-agnostic and supports both process reward modeling and direct agent tuning, enabling robust optimization of information-seeking agents. Furthermore, we systematically explore different process reward designs and highlight the effectiveness of advanced LLMs as scalable process reward judges, a key aspect largely unexplored in prior work.</p>
</div>
<figure id="A1.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Comparison of RAG-Gym with concurrent work.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt">Method</td>
<td class="ltx_td ltx_align_center ltx_border_tt">Process Supervision</td>
<td class="ltx_td ltx_align_center ltx_border_tt">Agent Tuning</td>
<td class="ltx_td ltx_align_center ltx_border_tt">Verifier Training</td>
<td class="ltx_td ltx_align_center ltx_border_tt">Agent-Agnostic</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t">SmartRAG <cite class="ltx_cite ltx_citemacro_citep">(Gao et al., <a href="#bib.bib9" title="" class="ltx_ref">2024</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="color:#FF0000;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="color:#00FF00;">✔</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="color:#FF0000;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="color:#FF0000;">✗</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_r">LeRet <cite class="ltx_cite ltx_citemacro_citep">(Hsu et al., <a href="#bib.bib12" title="" class="ltx_ref">2024</a>)</cite>
</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="color:#00FF00;">✔</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="color:#00FF00;">✔</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="color:#FF0000;">✗</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="color:#FF0000;">✗</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_r">AR-MCTS <cite class="ltx_cite ltx_citemacro_citep">(Dong et al., <a href="#bib.bib6" title="" class="ltx_ref">2024</a>)</cite>
</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="color:#00FF00;">✔</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="color:#FF0000;">✗</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="color:#00FF00;">✔</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="color:#FF0000;">✗</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_r">CR-Planner <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib24" title="" class="ltx_ref">2024</a>)</cite>
</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="color:#00FF00;">✔</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="color:#FF0000;">✗</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="color:#00FF00;">✔</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="color:#FF0000;">✗</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_r">ReARTeR <cite class="ltx_cite ltx_citemacro_citep">(Sun et al., <a href="#bib.bib40" title="" class="ltx_ref">2025</a>)</cite>
</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="color:#00FF00;">✔</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="color:#00FF00;">✔</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="color:#00FF00;">✔</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="color:#FF0000;">✗</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r">RAG-Gym (ours)</td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text" style="color:#00FF00;">✔</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text" style="color:#00FF00;">✔</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text" style="color:#00FF00;">✔</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text" style="color:#00FF00;">✔</span></td>
</tr>
</table>
</figure>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Dataset Descriptions</h2>

<div id="A2.p1" class="ltx_para">
<p class="ltx_p">In this section, we provide detailed descriptions of the datasets used in our experiments, including HotpotQA <cite class="ltx_cite ltx_citemacro_citep">(Yang et al., <a href="#bib.bib60" title="" class="ltx_ref">2018</a>)</cite>, 2WikiMultihopQA <cite class="ltx_cite ltx_citemacro_citep">(Ho et al., <a href="#bib.bib11" title="" class="ltx_ref">2020</a>)</cite>, Bamboogle <cite class="ltx_cite ltx_citemacro_citep">(Press et al., <a href="#bib.bib29" title="" class="ltx_ref">2023</a>)</cite>, and MedQA <cite class="ltx_cite ltx_citemacro_citep">(Jin et al., <a href="#bib.bib17" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
<section id="A2.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">HotpotQA.</h5>

<div id="A2.SS0.SSS0.Px1.p1" class="ltx_para">
<p class="ltx_p">HotpotQA is a large-scale, multi-hop question-answering dataset that requires reasoning across multiple documents. It consists of questions that explicitly demand retrieving and synthesizing information from different sources. The dataset provides both distractor and supporting documents, allowing evaluation of models’ ability to filter relevant information effectively. As the answers to the test questions in HotpotQA are not publicly available we took a subsample from its validation set (7,405 instances) as previous research did <cite class="ltx_cite ltx_citemacro_citep">(Yao et al., <a href="#bib.bib61" title="" class="ltx_ref">2023</a>; Li et al., <a href="#bib.bib25" title="" class="ltx_ref">2025</a>)</cite>. The last 1,000 validation questions were selected for the agent evaluation on HotpotQA. The first 1,000 questions were used as the training data for process supervision.</p>
</div>
</section>
<section id="A2.SS0.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">2WikiMultihopQA.</h5>

<div id="A2.SS0.SSS0.Px2.p1" class="ltx_para">
<p class="ltx_p">2WikiMultihopQA is another multi-hop question-answering dataset constructed from Wikipedia. 2WikiMultihopQA focuses on high-quality reasoning paths by selecting supporting documents more systematically. The dataset contains questions that require reasoning across different Wikipedia pages, ensuring a diverse range of factual and inferential challenges. The last 1000 questions in the development set (12,576 question in total) were used for agent evaluation.</p>
</div>
</section>
<section id="A2.SS0.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Bamboogle.</h5>

<div id="A2.SS0.SSS0.Px3.p1" class="ltx_para">
<p class="ltx_p">Bamboogle is a manually constructed dataset designed to evaluate compositional reasoning and adversarial robustness. It consists of 2-hop questions written by researchers, where both supporting facts exist in Wikipedia but are structured to be challenging for retrieval-based systems. Unlike automatically generated datasets like 2WikiMultihopQA and Musique, Bamboogle questions do not follow fixed templates, increasing their variability. We used the whole test set with 125 questions for the evaluation of agents on Bamboogle.</p>
</div>
</section>
<section id="A2.SS0.SSS0.Px4" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">MedQA.</h5>

<div id="A2.SS0.SSS0.Px4.p1" class="ltx_para">
<p class="ltx_p">MedQA is a medical question-answering dataset sourced from professional medical exams such as the USMLE (United States Medical Licensing Examination). It requires domain-specific knowledge and reasoning to answer multiple-choice medical questions. We focused on the English split of MedQA with 1,273 USMLE-style test questions. A subset of 1,000 questions was sampled from the training set (10,178 questions) for the optimization of various agents.</p>
</div>
</section>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Baseline Descriptions</h2>

<div id="A3.p1" class="ltx_para">
<p class="ltx_p">Here are the detailed descriptions of various baseline agents that we implemented in the experiments.</p>
<ul id="A3.I1" class="ltx_itemize">
<li id="A3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A3.I1.i1.p1" class="ltx_para">
<p class="ltx_p">Direct: The agent directly outputs the predicted answer without reasoning at the first iteration.</p>
</div>
</li>
<li id="A3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A3.I1.i2.p1" class="ltx_para">
<p class="ltx_p">CoT <cite class="ltx_cite ltx_citemacro_citep">(Wei et al., <a href="#bib.bib52" title="" class="ltx_ref">2022b</a>)</cite>: The agent outputs both the reasoning and the predicted answer at the first iteration.</p>
</div>
</li>
<li id="A3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A3.I1.i3.p1" class="ltx_para">
<p class="ltx_p">RAG <cite class="ltx_cite ltx_citemacro_citep">(Lewis et al., <a href="#bib.bib23" title="" class="ltx_ref">2020</a>)</cite>: The agent outputs the original question as the search query at the first iteration. It reasons about the updated state and generates a predicted answer at the second iteration.</p>
</div>
</li>
<li id="A3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A3.I1.i4.p1" class="ltx_para">
<p class="ltx_p">ReAct <cite class="ltx_cite ltx_citemacro_citep">(Yao et al., <a href="#bib.bib61" title="" class="ltx_ref">2023</a>)</cite>: The agent reasons about the given state and generates either a search action or an answer action.</p>
</div>
</li>
<li id="A3.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A3.I1.i5.p1" class="ltx_para">
<p class="ltx_p">Search-o1 <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib25" title="" class="ltx_ref">2025</a>)</cite>: Before the state reasoning, the agent summarizes the retrieved documents in the information-seeking history as a direct answer to the corresponding search query. The agent uses query-answer pairs instead of query-documents pairs for the construction of LLM input as our ReSearch does. It can be considered a special version of ReAct with RAG instead of information retrieval (IR) as the tool.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="A4" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Implementation Details</h2>

<div id="A4.p1" class="ltx_para">
<p class="ltx_p">For the implementation of the IR environment, we select Wikipedia as the supporting corpus for the retrieval of relevant information for questions from HotpotQA, 2WikiMultihopQA, and Bamboogle. For the environment of solving MedQA questions, we use a combination of medical textbooks and StatPearls which were pre-processed by <cite class="ltx_cite ltx_citemacro_citet">Xiong et al. (<a href="#bib.bib57" title="" class="ltx_ref">2024</a>)</cite>. For all tasks, we used both lexical and semantic retrievers whose results were merged with Reciprocal Rank Fusion <cite class="ltx_cite ltx_citemacro_citep">(Cormack et al., <a href="#bib.bib5" title="" class="ltx_ref">2009</a>)</cite>. BM25 <cite class="ltx_cite ltx_citemacro_citep">(Robertson et al., <a href="#bib.bib32" title="" class="ltx_ref">2009</a>)</cite> and BGE-Base <cite class="ltx_cite ltx_citemacro_citep">(Xiao et al., <a href="#bib.bib56" title="" class="ltx_ref">2023</a>)</cite> were used for HotpotQA, 2WikiMultihopQA, and Bamboogle, while in MedQA, we selected BM25 and MedCPT <cite class="ltx_cite ltx_citemacro_citep">(Jin et al., <a href="#bib.bib18" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
<div id="A4.p2" class="ltx_para">
<p class="ltx_p">For all LLM tuning in our paper, we employed the LoRA fine-tuning <cite class="ltx_cite ltx_citemacro_citep">(Hu et al., <a href="#bib.bib13" title="" class="ltx_ref">2021</a>)</cite> with <math id="A4.p2.m1" class="ltx_Math" alttext="r=256" display="inline"><semantics><mrow><mi>r</mi><mo>=</mo><mn>256</mn></mrow><annotation encoding="application/x-tex">r=256</annotation></semantics></math> and <math id="A4.p2.m2" class="ltx_Math" alttext="alpha=512" display="inline"><semantics><mrow><mrow><mi>a</mi><mo lspace="0em" rspace="0em">​</mo><mi>l</mi><mo lspace="0em" rspace="0em">​</mo><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mi>a</mi></mrow><mo>=</mo><mn>512</mn></mrow><annotation encoding="application/x-tex">alpha=512</annotation></semantics></math> on all attention components in the transformers architecture <cite class="ltx_cite ltx_citemacro_citep">(Vaswani, <a href="#bib.bib45" title="" class="ltx_ref">2017</a>)</cite>. The process supervision methods are implemented using the TRL package <cite class="ltx_cite ltx_citemacro_citep">(von Werra et al., <a href="#bib.bib46" title="" class="ltx_ref">2020</a>)</cite>. We used the instruction-tuned version of Llama-3.1-8B as the base model for implementing various agents, which also served as the base model for process reward modeling (PRM). For the tuning of Search-o1 and ReSearch agents, only the LLM for action reasoning is trained while the one for history knowledge summarization remains unturned.</p>
</div>
<div id="A4.p3" class="ltx_para">
<p class="ltx_p">All results of zero-short learning (ZSL), supervised fine-tuning (SFT), and direct preference optimization (DPO) are generated with a temperature of 0.0. For the evaluation of PRM, we employed a temperature of 1.0 with 10 different actions sampled for each step in the information-seeking trajectory. Algorithm <a href="#alg1" title="Algorithm 1 ‣ Appendix D Implementation Details ‣ RAG-Gym: Optimizing Reasoning and Search Agents with Process Supervision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> presents our algorithm of using the trained process reward model to guide the action selection during inference.</p>
</div>
<figure id="alg1" class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span class="ltx_text ltx_font_bold">Algorithm 1</span> </span> PRM-Guided Inference with Best-of-N Selection</figcaption>
<ol id="A4.I1" class="ltx_enumerate">
<li id="A4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="A4.I1.i1.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Input:</span> Original question <math id="A4.I1.i1.p1.m1" class="ltx_Math" alttext="Q" display="inline"><semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics></math>, agent policy <math id="A4.I1.i1.p1.m2" class="ltx_Math" alttext="\pi_{f(\theta)}" display="inline"><semantics><msub><mi>π</mi><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow></mrow></msub><annotation encoding="application/x-tex">\pi_{f(\theta)}</annotation></semantics></math>, process reward model <math id="A4.I1.i1.p1.m3" class="ltx_Math" alttext="r_{\phi}" display="inline"><semantics><msub><mi>r</mi><mi>ϕ</mi></msub><annotation encoding="application/x-tex">r_{\phi}</annotation></semantics></math>, number of candidate actions <math id="A4.I1.i1.p1.m4" class="ltx_Math" alttext="N" display="inline"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math>, maximum steps <math id="A4.I1.i1.p1.m5" class="ltx_Math" alttext="T" display="inline"><semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics></math>, information retrieval function IR.</p>
</div>
</li>
<li id="A4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="A4.I1.i2.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Initialize</span> state <math id="A4.I1.i2.p1.m1" class="ltx_Math" alttext="S\leftarrow(Q,H_{1}=\emptyset)" display="inline"><semantics><mrow><mi>S</mi><mo stretchy="false">←</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><mi>Q</mi><mo>,</mo><msub><mi>H</mi><mn>1</mn></msub></mrow><mo>=</mo><mi mathvariant="normal">∅</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">S\leftarrow(Q,H_{1}=\emptyset)</annotation></semantics></math>.</p>
</div>
</li>
<li id="A4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="A4.I1.i3.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">For</span> <math id="A4.I1.i3.p1.m1" class="ltx_Math" alttext="t=1" display="inline"><semantics><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">t=1</annotation></semantics></math> to <math id="A4.I1.i3.p1.m2" class="ltx_Math" alttext="T" display="inline"><semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics></math>:</p>
<ol id="A4.I1.i3.I1" class="ltx_enumerate">
<li id="A4.I1.i3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(a)</span> 
<div id="A4.I1.i3.I1.i1.p1" class="ltx_para">
<p class="ltx_p">Generate <math id="A4.I1.i3.I1.i1.p1.m1" class="ltx_Math" alttext="N" display="inline"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> candidate actions: <math id="A4.I1.i3.I1.i1.p1.m2" class="ltx_math_unparsed" alttext="a_{q},\cdots,a_{N}\sim\pi_{f(\theta)}(\cdot|S)" display="inline"><semantics><mrow><msub><mi>a</mi><mi>q</mi></msub><mo>,</mo><mi mathvariant="normal">⋯</mi><mo>,</mo><msub><mi>a</mi><mi>N</mi></msub><mo>∼</mo><msub><mi>π</mi><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow></mrow></msub><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo fence="false" rspace="0.167em" stretchy="false">|</mo><mi>S</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">a_{q},\cdots,a_{N}\sim\pi_{f(\theta)}(\cdot|S)</annotation></semantics></math>.</p>
</div>
</li>
<li id="A4.I1.i3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(b)</span> 
<div id="A4.I1.i3.I1.i2.p1" class="ltx_para">
<p class="ltx_p">Compute process rewards and select the best action: <math id="A4.I1.i3.I1.i2.p1.m1" class="ltx_Math" alttext="a^{*}\leftarrow\arg\max_{a\in\{a_{1},\cdots,a_{N}\}}r_{\phi}(S,a)" display="inline"><semantics><mrow><msup><mi>a</mi><mo>∗</mo></msup><mo stretchy="false">←</mo><mrow><mrow><mi>arg</mi><mo lspace="0.167em">⁡</mo><mrow><msub><mi>max</mi><mrow><mi>a</mi><mo>∈</mo><mrow><mo stretchy="false">{</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">⋯</mi><mo>,</mo><msub><mi>a</mi><mi>N</mi></msub><mo stretchy="false">}</mo></mrow></mrow></msub><mo lspace="0.167em">⁡</mo><msub><mi>r</mi><mi>ϕ</mi></msub></mrow></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>S</mi><mo>,</mo><mi>a</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">a^{*}\leftarrow\arg\max_{a\in\{a_{1},\cdots,a_{N}\}}r_{\phi}(S,a)</annotation></semantics></math>.</p>
</div>
</li>
<li id="A4.I1.i3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(c)</span> 
<div id="A4.I1.i3.I1.i3.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">If</span> <math id="A4.I1.i3.I1.i3.p1.m1" class="ltx_Math" alttext="a^{*}" display="inline"><semantics><msup><mi>a</mi><mo>∗</mo></msup><annotation encoding="application/x-tex">a^{*}</annotation></semantics></math> is a search query:</p>
<ol id="A4.I1.i3.I1.i3.I1" class="ltx_enumerate">
<li id="A4.I1.i3.I1.i3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">i.</span> 
<div id="A4.I1.i3.I1.i3.I1.i1.p1" class="ltx_para">
<p class="ltx_p">Retrieve documents: <math id="A4.I1.i3.I1.i3.I1.i1.p1.m1" class="ltx_Math" alttext="D\leftarrow\text{IR}(a^{*})" display="inline"><semantics><mrow><mi>D</mi><mo stretchy="false">←</mo><mrow><mtext>IR</mtext><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>a</mi><mo>∗</mo></msup><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">D\leftarrow\text{IR}(a^{*})</annotation></semantics></math>.</p>
</div>
</li>
<li id="A4.I1.i3.I1.i3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">ii.</span> 
<div id="A4.I1.i3.I1.i3.I1.i2.p1" class="ltx_para">
<p class="ltx_p">Update state: <math id="A4.I1.i3.I1.i3.I1.i2.p1.m1" class="ltx_Math" alttext="S\leftarrow(Q,H_{t+1}=H_{t}\cup\{(a^{*},D)\})" display="inline"><semantics><mrow><mi>S</mi><mo stretchy="false">←</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><mi>Q</mi><mo>,</mo><msub><mi>H</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><mo>=</mo><mrow><msub><mi>H</mi><mi>t</mi></msub><mo>∪</mo><mrow><mo stretchy="false">{</mo><mrow><mo stretchy="false">(</mo><msup><mi>a</mi><mo>∗</mo></msup><mo>,</mo><mi>D</mi><mo stretchy="false">)</mo></mrow><mo stretchy="false">}</mo></mrow></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">S\leftarrow(Q,H_{t+1}=H_{t}\cup\{(a^{*},D)\})</annotation></semantics></math>.</p>
</div>
</li>
</ol>
</div>
</li>
<li id="A4.I1.i3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(d)</span> 
<div id="A4.I1.i3.I1.i4.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">If</span> <math id="A4.I1.i3.I1.i4.p1.m1" class="ltx_Math" alttext="a^{*}" display="inline"><semantics><msup><mi>a</mi><mo>∗</mo></msup><annotation encoding="application/x-tex">a^{*}</annotation></semantics></math> is a final answer:</p>
<ol id="A4.I1.i3.I1.i4.I1" class="ltx_enumerate">
<li id="A4.I1.i3.I1.i4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">i.</span> 
<div id="A4.I1.i3.I1.i4.I1.i1.p1" class="ltx_para">
<p class="ltx_p">Return <math id="A4.I1.i3.I1.i4.I1.i1.p1.m1" class="ltx_Math" alttext="a^{*}" display="inline"><semantics><msup><mi>a</mi><mo>∗</mo></msup><annotation encoding="application/x-tex">a^{*}</annotation></semantics></math> and terminate the process.</p>
</div>
</li>
</ol>
</div>
</li>
</ol>
</div>
</li>
<li id="A4.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="A4.I1.i4.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">End For</span></p>
</div>
</li>
</ol>
</figure>
</section>
<section id="A5" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Case Studies</h2>

<section id="A5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">E.1 </span>Comparison of Agent Architectures on Bamboogle</h3>

<div id="A5.SS1.p1" class="ltx_para">
<p class="ltx_p">We analyze the reasoning and search behaviors of RAG, ReAct, Search-o1, and ReSearch using an example from the Bamboogle dataset. As shown in Figure <a href="#A5.F6" title="Figure 6 ‣ E.1 Comparison of Agent Architectures on Bamboogle ‣ Appendix E Case Studies ‣ RAG-Gym: Optimizing Reasoning and Search Agents with Process Supervision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, given the question “What was the father of the last surviving Canadian father of Confederation?”, the three agents show distinct behaviors when generating the first action.</p>
</div>
<div id="A5.SS1.p2" class="ltx_para">
<p class="ltx_p">The RAG agent directly passes the question as a search query without decomposition, relying entirely on retrieval to infer the answer. This often leads to ineffective searches that fail to retrieve necessary intermediate facts. ReAct and Search-o1 improve upon this by engaging in stepwise query reasoning, first identifying the need to determine the last surviving Canadian father of Confederation before issuing a search query. However, the generated query, “List of Canadian fathers of Confederation”, retrieves broad information rather than directly resolving the missing knowledge.</p>
</div>
<div id="A5.SS1.p3" class="ltx_para">
<p class="ltx_p">In contrast, ReSearch explicitly integrates answer reasoning with search. It first constructs a potential answer, identifying an unverified claim that William Lyon Mackenzie King is among the last surviving Canadian fathers of Confederation. Recognizing the missing evidence, it formulates a targeted query, “Who is the last surviving Canadian father of Confederation?”, to resolve the uncertainty. This approach ensures that retrieval is aligned with answer construction, minimizing unnecessary queries and improving information efficiency. The case study illustrates how ReSearch effectively refines the search process by linking query generation to specific knowledge gaps.</p>
</div>
<figure id="A5.F6" class="ltx_figure"><img src="" id="A5.F6.g1" class="ltx_graphics ltx_centering ltx_missing ltx_missing_image" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Comparison of different agent architectures in handling a multi-hop question from Bamboogle. ReSearch explicitly aligns reasoning with query generation, leading to more targeted retrieval and improved answer quality.</figcaption>
</figure>
</section>
<section id="A5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">E.2 </span>PRM-Guided Action Selection in MedQA</h3>

<figure id="A5.T4" class="ltx_table">
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 4: </span>An example of how PRM helps identify high-quality actions that lead to correct answers. The unverified claims in the presented example are <span class="ltx_text" style="background-color:#FFCCCC;">highlighted</span>.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:469.5pt;"><span class="ltx_text ltx_font_bold" style="font-size:90%;"> 
<span class="ltx_tabular ltx_align_middle">
<span class="ltx_tr">
<span class="ltx_td ltx_nopad_r ltx_align_center">MedQA Question</span></span>
</span></span></span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:469.5pt;"><span class="ltx_text ltx_font_bold" style="font-size:90%;">Question</span><span class="ltx_text" style="font-size:90%;">: A 60-year-old woman presents to a physician for worsening shortness of breath and increasing abdominal distention over the last 3 months. She says that the shortness of breath is worse on exertion and improves with rest. While she could previously walk to the nearby store for her groceries, she now has to drive because she gets ”winded” on the way. The patient was diagnosed with diabetes 5 years ago and is compliant with her medications. The medical history is otherwise unremarkable. The physical examination reveals gross ascites and visibly engorged periumbilical veins. Bilateral pitting edema is noted around the ankles. The finger-prick blood glucose level is 100 mg/dL. What is the mechanism of action of the anti-diabetic medication this patient is most likely taking? </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">A.</span><span class="ltx_text" style="font-size:90%;"> Binding to the alpha subunit of the insulin receptor, </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">B.</span><span class="ltx_text" style="font-size:90%;"> Closure of ATP-sensitive K-channels in the pancreatic beta-cell, </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">C.</span><span class="ltx_text" style="font-size:90%;"> Increased gene expression of GLUT-4, </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">D.</span><span class="ltx_text" style="font-size:90%;"> Glucagon-like peptide-1 receptor agonist</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:469.5pt;"><span class="ltx_text ltx_font_bold" style="font-size:90%;">Answer</span><span class="ltx_text" style="font-size:90%;"> (invisible): C</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" style="background-color:#CCFFCC;">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span class="ltx_inline-block ltx_align_top" style="background-color:#CCFFCC;">
<span class="ltx_p" style="width:469.5pt;"><span class="ltx_text ltx_font_bold" style="font-size:90%;"> 
<span class="ltx_tabular ltx_align_middle">
<span class="ltx_tr">
<span class="ltx_td ltx_nopad_r ltx_align_center">Action 1</span></span>
</span></span></span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:469.5pt;"><span class="ltx_text ltx_font_bold" style="font-size:90%;">[Candidate 1 – Reasoning]</span><span class="ltx_text" style="font-size:90%;">
…</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:469.5pt;"><span class="ltx_text ltx_font_bold" style="font-size:90%;">[Candidate 1 – Action]</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:469.5pt;"><span class="ltx_text ltx_font_italic" style="font-size:90%;">Query</span><span class="ltx_text" style="font-size:90%;">: What is the primary concern with the patient’s condition, specifically whether heart failure is confirmed based on symptoms and physical examination findings? </span><span class="ltx_ERROR undefined">\faThumbsDown</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:469.5pt;"><span class="ltx_text ltx_font_bold" style="font-size:90%;">[Candidate 2 – Reasoning]</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:469.5pt;"><span class="ltx_text" style="font-size:90%;">1. The patient presents with worsening shortness of breath, increasing abdominal distention, and gross ascites. These symptoms, along with visibly engorged periumbilical veins and bilateral pitting edema, are indicative of fluid overload and congestive heart failure.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:469.5pt;"><span class="ltx_text" style="font-size:90%;">2. The patient’s history of diabetes, which is compliant with medication, suggests that her anti-diabetic medication is likely a type of oral hypoglycemic agent rather than insulin.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:469.5pt;"><span class="ltx_text" style="font-size:90%;">3. Considering the patient’s age (60 years) and symptoms, it is unlikely that she is taking a glucagon-like peptide-1 (GLP-1) receptor agonist, which is typically used in younger patients with type 2 diabetes and has a different side-effect profile.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:469.5pt;"><span class="ltx_text" style="font-size:90%;">4. With the remaining options, ’Binding to the alpha subunit of the insulin receptor’ and ’Closure of ATP-sensitive K-channels in the pancreatic beta-cell’, we can narrow down the possibilities. ’Binding to the alpha subunit of the insulin receptor’ is more characteristic of insulin receptor agonists or insulin itself, which is unlikely given the patient’s age and the oral nature of her medication. Therefore, </span><span class="ltx_text" style="font-size:90%;background-color:#FFCCCC;">’Closure of ATP-sensitive K-channels in the pancreatic beta-cell’ is the most plausible mechanism of action</span><span class="ltx_text" style="font-size:90%;">.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:469.5pt;"><span class="ltx_text" style="font-size:90%;">5. This mechanism is consistent with the </span><span class="ltx_text" style="font-size:90%;background-color:#FFCCCC;">sulfonylureas class of medications</span><span class="ltx_text" style="font-size:90%;">, which are commonly used in patients with type 2 diabetes.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:469.5pt;"><span class="ltx_text ltx_font_bold" style="font-size:90%;">[Candidate 2 – Action]</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:469.5pt;"><span class="ltx_text ltx_font_italic" style="font-size:90%;">Query</span><span class="ltx_text" style="font-size:90%;">: What are common classes of oral anti-diabetic medications? </span><span class="ltx_ERROR undefined">\faThumbsUp</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" style="background-color:#CCE5FF;">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span class="ltx_inline-block ltx_align_top" style="background-color:#CCE5FF;">
<span class="ltx_p" style="width:469.5pt;"><span class="ltx_text ltx_font_bold" style="font-size:90%;"> 
<span class="ltx_tabular ltx_align_middle">
<span class="ltx_tr">
<span class="ltx_td ltx_nopad_r ltx_align_center">Environment Feedback 1</span></span>
</span></span></span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:469.5pt;"><span class="ltx_text ltx_font_bold" style="font-size:90%;">[+ Documents]</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:469.5pt;"><span class="ltx_text" style="font-size:90%;">Common classes of oral anti-diabetic medications include: 1. Sulfonylureas, 2. Dipeptidyl Peptidase IV (DPP IV) inhibitors (gliptins), 3. Thiazolidinediones, 4. Meglitinides, 5. Sodium-glucose co-transporter 2 (SGLT 2) inhibitors, 6. Alpha-glucosidase inhibitors, 7. Biguanides (metformin).</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" style="background-color:#CCFFCC;">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span class="ltx_inline-block ltx_align_top" style="background-color:#CCFFCC;">
<span class="ltx_p" style="width:469.5pt;"><span class="ltx_text ltx_font_bold" style="font-size:90%;"> 
<span class="ltx_tabular ltx_align_middle">
<span class="ltx_tr">
<span class="ltx_td ltx_nopad_r ltx_align_center">Action 2</span></span>
</span></span></span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:469.5pt;"><span class="ltx_text ltx_font_bold" style="font-size:90%;">[Candidate 1 – Reasoning]</span><span class="ltx_text" style="font-size:90%;">
…</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:469.5pt;"><span class="ltx_text ltx_font_bold" style="font-size:90%;">[Candidate 1 – Action]</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:469.5pt;"><span class="ltx_text ltx_font_italic" style="font-size:90%;">Query</span><span class="ltx_text" style="font-size:90%;">: Is metformin commonly prescribed for type 2 diabetes and known to cause lactic acidosis as a side effect? </span><span class="ltx_ERROR undefined">\faThumbsDown</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:469.5pt;"><span class="ltx_text ltx_font_bold" style="font-size:90%;">[Candidate 2 – Reasoning]</span><span class="ltx_text" style="font-size:90%;">
…</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:469.5pt;"><span class="ltx_text ltx_font_bold" style="font-size:90%;">[Candidate 2 – Action]</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:469.5pt;"><span class="ltx_text ltx_font_italic" style="font-size:90%;">Query</span><span class="ltx_text" style="font-size:90%;">: What is the mechanism of action of Biguanides (metformin)? </span><span class="ltx_ERROR undefined">\faThumbsUp</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" style="background-color:#CCE5FF;">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span class="ltx_inline-block ltx_align_top" style="background-color:#CCE5FF;">
<span class="ltx_p" style="width:469.5pt;"><span class="ltx_text ltx_font_bold" style="font-size:90%;"> 
<span class="ltx_tabular ltx_align_middle">
<span class="ltx_tr">
<span class="ltx_td ltx_nopad_r ltx_align_center">Environment Feedback 2</span></span>
</span></span></span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:469.5pt;"><span class="ltx_text ltx_font_bold" style="font-size:90%;">[+ Documents]</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:469.5pt;"><span class="ltx_text" style="font-size:90%;">The mechanism of action of Biguanides (metformin) is to activate the enzyme AMP-activated protein kinase (AMPK) and reduce hepatic glucose production.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" style="background-color:#CCFFCC;">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span class="ltx_inline-block ltx_align_top" style="background-color:#CCFFCC;">
<span class="ltx_p" style="width:469.5pt;"><span class="ltx_text ltx_font_bold" style="font-size:90%;"> 
<span class="ltx_tabular ltx_align_middle">
<span class="ltx_tr">
<span class="ltx_td ltx_nopad_r ltx_align_center">Action 3</span></span>
</span></span></span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:469.5pt;"><span class="ltx_text ltx_font_bold" style="font-size:90%;">[Candidate 1 – Reasoning]</span><span class="ltx_text" style="font-size:90%;">
…
</span><span class="ltx_text ltx_font_bold" style="font-size:90%;">[Candidate 1 – Action]</span><span class="ltx_text" style="font-size:90%;">
</span><span class="ltx_text ltx_font_italic" style="font-size:90%;">Answer</span><span class="ltx_text" style="font-size:90%;">: B </span><span class="ltx_ERROR undefined">\faThumbsDown</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:469.5pt;"><span class="ltx_text ltx_font_bold" style="font-size:90%;">[Candidate 2 – Reasoning]</span><span class="ltx_text" style="font-size:90%;">
…
</span><span class="ltx_text ltx_font_bold" style="font-size:90%;">[Candidate 2 – Action]</span><span class="ltx_text" style="font-size:90%;">
</span><span class="ltx_text ltx_font_italic" style="font-size:90%;">Answer</span><span class="ltx_text" style="font-size:90%;">: C </span><span class="ltx_ERROR undefined">\faThumbsUp</span></span>
</span>
</td>
</tr>
</table>
</figure>
<div id="A5.SS2.p1" class="ltx_para">
<p class="ltx_p">To illustrate how the Process Reward Modeling (PRM) improves decision-making, we examine a case from the MedQA dataset (Table <a href="#A5.T4" title="Table 4 ‣ E.2 PRM-Guided Action Selection in MedQA ‣ Appendix E Case Studies ‣ RAG-Gym: Optimizing Reasoning and Search Agents with Process Supervision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>). The model is tasked with identifying the mechanism of action of the most likely anti-diabetic medication for a 60-year-old patient presenting with symptoms indicative of fluid overload.</p>
</div>
<div id="A5.SS2.p2" class="ltx_para">
<p class="ltx_p">For the generation of the first action, the agent initially generates a broad and less actionable query about heart failure, which, while relevant, does not directly contribute to identifying the correct medication. Meanwhile, it also provides another candidate action with the query “What are common classes of oral anti-diabetic medications?”, which leads to retrieving a structured list of relevant drug categories. The process reward model estimates the quality of these two candidates and identifies the second candidate as the better action.</p>
</div>
<div id="A5.SS2.p3" class="ltx_para">
<p class="ltx_p">As the reasoning progresses, the process reward model continues to refine action selection, identifying the best queries to resolve missing information efficiently. The rewarded queries ultimately guide the agent toward correctly inferring that the medication most likely falls under the sulfonylureas class, which acts by closing ATP-sensitive K-channels in pancreatic beta cells.
This case demonstrates how process reward models enhance decision quality by selecting queries that effectively bridge knowledge gaps, leading to more precise reasoning and higher answer accuracy.</p>
</div>
</section>
</section>
<section id="A6" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix F </span>Prompt Templates</h2>

<div id="A6.p1" class="ltx_para">
<p class="ltx_p">We provide structured prompt templates for history knowledge summarization and action generation in our proposed ReSearch agent. The template in Figure <a href="#A6.F7" title="Figure 7 ‣ Appendix F Prompt Templates ‣ RAG-Gym: Optimizing Reasoning and Search Agents with Process Supervision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> ensures that retrieved documents are summarized concisely and factually for follow-up queries. Figure <a href="#A6.F8" title="Figure 8 ‣ Appendix F Prompt Templates ‣ RAG-Gym: Optimizing Reasoning and Search Agents with Process Supervision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> shows the prompt template used by ReSearch to align the answer construction with query formulation. The prompt of using GPT-4o for process reward data annotation is presented in Figure <a href="#A6.F9" title="Figure 9 ‣ Appendix F Prompt Templates ‣ RAG-Gym: Optimizing Reasoning and Search Agents with Process Supervision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>.</p>
</div>
<figure id="A6.F7" class="ltx_figure"><svg id="A6.F7.pic1" class="ltx_picture" height="224.01" overflow="visible" version="1.1" width="656.13"><g transform="translate(0,224.01) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#000000" fill-opacity="1.0"><path d="M 0 5.91 L 0 205.96 C 0 209.22 2.64 211.87 5.91 211.87 L 650.23 211.87 C 653.49 211.87 656.13 209.22 656.13 205.96 L 656.13 5.91 C 656.13 2.64 653.49 0 650.23 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#FFFFFF" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 205.96 C 1.97 208.13 3.73 209.9 5.91 209.9 L 650.23 209.9 C 652.4 209.9 654.16 208.13 654.16 205.96 L 654.16 5.91 C 654.16 3.73 652.4 1.97 650.23 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 15 201.86)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#FFFFFF" fill-opacity="1.0"><path d="M 0 2.95 L 0 19.19 C 0 20.82 1.32 22.14 2.95 22.14 L 519.57 22.14 C 521.2 22.14 522.52 20.82 522.52 19.19 L 522.52 2.95 C 522.52 1.32 521.2 0 519.57 0 L 2.95 0 C 1.32 0 0 1.32 0 2.95 Z" style="stroke:none"></path></g><g fill="#000000" fill-opacity="1.0"><path d="M 0 2.95 L 0 19.19 C 0 20.82 1.32 22.14 2.95 22.14 L 519.57 22.14 C 521.2 22.14 522.52 20.82 522.52 19.19 L 522.52 2.95 C 522.52 1.32 521.2 0 519.57 0 L 2.95 0 C 1.32 0 0 1.32 0 2.95 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 11.81 7.61)"><text transform="matrix(1 0 0 -1 0 0)" color="#000000">Prompt template for history knowledge summarization in Search-o1 and ReSearch</text></g></g></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignObject width="612.83" height="178.34" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:442.9pt;">
<span class="ltx_p">You are a helpful assistant tasked with answering a follow-up query using the relevant documents provided.</span>
<span class="ltx_p">### Relevant Documents</span>
<span class="ltx_p"><code class="ltx_verbatim ltx_font_typewriter">{{documents}}</code></span>
<span class="ltx_p">### Context</span>
<span class="ltx_p">Original question: <code class="ltx_verbatim ltx_font_typewriter">{{question}}</code></span>
<span class="ltx_p">### Follow-up Query</span>
<span class="ltx_p"><code class="ltx_verbatim ltx_font_typewriter">{{query}}</code></span>
<span class="ltx_p">Answer the follow-up query succinctly, using only the information from the documents. When the documents do not provide sufficient information, explicitly point this out instead of making up facts. Do not include unrelated or excessive details in the response.</span>
</span></foreignObject></g></g></svg>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Template used for history knowledge summarization in Search-o1 and ReSearch.</figcaption>
</figure>
<figure id="A6.F8" class="ltx_figure"><svg id="A6.F8.pic1" class="ltx_picture" height="420.57" overflow="visible" version="1.1" width="656.13"><g transform="translate(0,420.57) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#000000" fill-opacity="1.0"><path d="M 0 5.91 L 0 402.52 C 0 405.78 2.64 408.43 5.91 408.43 L 650.23 408.43 C 653.49 408.43 656.13 405.78 656.13 402.52 L 656.13 5.91 C 656.13 2.64 653.49 0 650.23 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#FFFFFF" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 402.52 C 1.97 404.7 3.73 406.46 5.91 406.46 L 650.23 406.46 C 652.4 406.46 654.16 404.7 654.16 402.52 L 654.16 5.91 C 654.16 3.73 652.4 1.97 650.23 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 15 398.43)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#FFFFFF" fill-opacity="1.0"><path d="M 0 2.95 L 0 19.19 C 0 20.82 1.32 22.14 2.95 22.14 L 418.41 22.14 C 420.04 22.14 421.36 20.82 421.36 19.19 L 421.36 2.95 C 421.36 1.32 420.04 0 418.41 0 L 2.95 0 C 1.32 0 0 1.32 0 2.95 Z" style="stroke:none"></path></g><g fill="#000000" fill-opacity="1.0"><path d="M 0 2.95 L 0 19.19 C 0 20.82 1.32 22.14 2.95 22.14 L 418.41 22.14 C 420.04 22.14 421.36 20.82 421.36 19.19 L 421.36 2.95 C 421.36 1.32 420.04 0 418.41 0 L 2.95 0 C 1.32 0 0 1.32 0 2.95 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 11.81 7.61)"><text transform="matrix(1 0 0 -1 0 0)" color="#000000">Prompt template for generating actions using the ReSearch agent</text></g></g></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignObject width="612.83" height="374.91" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:442.9pt;">
<span class="ltx_p">You are a helpful assistant. Your task is to answer a given question following user instructions.’</span>
<span class="ltx_p">### Information-seeking History</span>
<span class="ltx_p"><code class="ltx_verbatim ltx_font_typewriter">{{history}}</code></span>
<span class="ltx_p">### Original Question</span>
<span class="ltx_p"><code class="ltx_verbatim ltx_font_typewriter">{{question}}</code></span>
<span class="ltx_p">Your output must include three sections:</span>
<span class="ltx_p">1. **### Step-by-step Reasoning**:</span>
<span class="ltx_p">- Think step-by-step and then answer the question.</span>
<span class="ltx_p">2. **### Unverified Claim Identification**:</span>
<span class="ltx_p">- Identify if there are claims in the reasoning chain that are not grounded in the user-provided information-seeking history and need to be verified.</span>
<span class="ltx_p">- Summarize the first piece of missing information as an atomic query that will be searched in an external knowledge base. If there is no unverified claim, clearly state that no further query is needed.</span>
<span class="ltx_p">3. **### Structured Output**:</span>
<span class="ltx_p">- Present your predicted answer and generated query (if applicable) in the following JSON format:</span>
<span class="ltx_p">“‘json</span>
<span class="ltx_p">{</span>
<span class="ltx_p">“predicted_answer”: “Provide a single letter (for multiple-choice questions), digit, word, or short phrase here.”,</span>
<span class="ltx_p">“generated_query”: “Provide an entity, question, or statement to be searched in an external knowledge base. Output \“None\” if no query is generated.”,</span>
<span class="ltx_p">}</span>
<span class="ltx_p">“‘</span>
</span></foreignObject></g></g></svg>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Template used to generate actions for the ReSearch agent.</figcaption>
</figure>
<figure id="A6.F9" class="ltx_figure"><svg id="A6.F9.pic1" class="ltx_picture" height="655.72" overflow="visible" version="1.1" width="656.13"><g transform="translate(0,655.72) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#000000" fill-opacity="1.0"><path d="M 0 5.91 L 0 637.67 C 0 640.94 2.64 643.58 5.91 643.58 L 650.23 643.58 C 653.49 643.58 656.13 640.94 656.13 637.67 L 656.13 5.91 C 656.13 2.64 653.49 0 650.23 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#FFFFFF" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 637.67 C 1.97 639.85 3.73 641.61 5.91 641.61 L 650.23 641.61 C 652.4 641.61 654.16 639.85 654.16 637.67 L 654.16 5.91 C 654.16 3.73 652.4 1.97 650.23 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 15 633.58)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#FFFFFF" fill-opacity="1.0"><path d="M 0 2.95 L 0 19.19 C 0 20.82 1.32 22.14 2.95 22.14 L 391.1 22.14 C 392.73 22.14 394.05 20.82 394.05 19.19 L 394.05 2.95 C 394.05 1.32 392.73 0 391.1 0 L 2.95 0 C 1.32 0 0 1.32 0 2.95 Z" style="stroke:none"></path></g><g fill="#000000" fill-opacity="1.0"><path d="M 0 2.95 L 0 19.19 C 0 20.82 1.32 22.14 2.95 22.14 L 391.1 22.14 C 392.73 22.14 394.05 20.82 394.05 19.19 L 394.05 2.95 C 394.05 1.32 392.73 0 391.1 0 L 2.95 0 C 1.32 0 0 1.32 0 2.95 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 11.81 7.61)"><text transform="matrix(1 0 0 -1 0 0)" color="#000000">Prompt template for ranking candidate actions with GPT-4o</text></g></g></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignObject width="612.83" height="610.06" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:442.9pt;">
<span class="ltx_p">You are a decision-evaluation assistant. Your task is to rank the proposed actions from the most appropriate to the least appropriate as the next step in a sequential decision-making process aimed at solving a given question.</span>
<span class="ltx_p">### Original Question:</span>
<span class="ltx_p"><code class="ltx_verbatim ltx_font_typewriter">{{question}}</code></span>
<span class="ltx_p">### Information-Seeking History:</span>
<span class="ltx_p"><code class="ltx_verbatim ltx_font_typewriter">{{curr_history}}</code></span>
<span class="ltx_p">### Proposed Next Actions:</span>
<span class="ltx_p"><code class="ltx_verbatim ltx_font_typewriter">{{actions_text}}</code></span>
<span class="ltx_p">### Important Assumption</span>
<span class="ltx_p">The agent has no prior knowledge about the subject matter. It must rely solely on the information-seeking history provided to evaluate and answer the original question. Assumptions not explicitly supported by the history must not influence the ranking of proposed actions.</span>
<span class="ltx_p">### Evaluation Criteria for Appropriateness</span>
<span class="ltx_p">1. **Sufficiency Check**:</span>
<span class="ltx_p">- Determine whether the available information is sufficient to directly answer the original question. If not, the proposed action to “Answer” is inappropriate.</span>
<span class="ltx_p">- Prioritize queries that gather specific, missing information essential to solving the question.</span>
<span class="ltx_p">- If the history already contains all necessary information, then “Answer” is the most appropriate action, and the correct answer should be ranked highest.</span>
<span class="ltx_p">2. **Utility Check**:</span>
<span class="ltx_p">- Queries must be precise, actionable, and directly relevant to solving the question.</span>
<span class="ltx_p">- Prioritize foundational queries that establish critical context or general knowledge necessary for more specific follow-ups.</span>
<span class="ltx_p">- Rank overly narrow or prematurely specific queries lower if they presume knowledge not yet available.</span>
<span class="ltx_p">- Avoid irrelevant queries that do not contribute to solving the original question.</span>
<span class="ltx_p">3. **Redundancy Check**:</span>
<span class="ltx_p">- Queries that duplicate information already covered in the history or repeat previous queries should be ranked lower.</span>
<span class="ltx_p">- Proposed actions must add new value to the decision-making process by seeking new or clarifying missing information.</span>
<span class="ltx_p">### Expected Output Format</span>
<span class="ltx_p">- Output the indices of the ranked actions in JSON format: “‘json{“ranked_indices”: [list of indices]}”’.</span>
<span class="ltx_p">- Rank actions from most appropriate to least appropriate based on the evaluation criteria above.</span>
<span class="ltx_p">- Do not provide additional explanations or reasoning.”’</span>
</span></foreignObject></g></g></svg>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Template used by GPT-4o to rank action candidates given the state.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2502.13956" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/land_of_honey_and_milk" rel="nofollow" aria-hidden="true" tabindex="-1"></a>
    <a href="/log/2502.13957" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2502.13957">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2502.13957" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2502.13958" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Mar  5 18:41:33 2025 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
